[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Python Math & ML Course",
    "section": "",
    "text": "A\n\nhttps://www.youtube.com/watch?v=vNePhmCMnbU\nhttps://www.youtube.com/watch?v=P3Y8OWkiUts",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>A</span>"
    ]
  },
  {
    "objectID": "math/Resources/Practicals/pr_01_Vectors/pr01_vectors.html",
    "href": "math/Resources/Practicals/pr_01_Vectors/pr01_vectors.html",
    "title": "2  Practice 1: Vectors",
    "section": "",
    "text": "2.1 Exercise: Finding a Perpendicular Vector\nContext:\nIn linear algebra, two vectors are perpendicular (or orthogonal) if their dot product is zero. In this exercise, you will find a vector in \\(\\mathbb{R}^2\\) that is perpendicular to a given vector.\nGiven:\nLet \\(\\mathbf{v} = [2, 3]\\).\nTasks:",
    "crumbs": [
      "Math | Practicals",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Practice 1: Vectors</span>"
    ]
  },
  {
    "objectID": "math/Resources/Practicals/pr_01_Vectors/pr01_vectors.html#exercise-finding-a-perpendicular-vector",
    "href": "math/Resources/Practicals/pr_01_Vectors/pr01_vectors.html#exercise-finding-a-perpendicular-vector",
    "title": "2  Practice 1: Vectors",
    "section": "",
    "text": "Find a Perpendicular Vector:\n\nFind a non-zero vector \\(\\mathbf{w} = [x, y]\\) such that \\(\\mathbf{v}\\) and \\(\\mathbf{w}\\) are perpendicular.\n\nVerification:\n\nShow that your chosen vector \\(\\mathbf{w}\\) indeed satisfies the condition \\(\\mathbf{v} \\cdot \\mathbf{w} = 0\\).\n\nUnit Perpendicular Vector:\n\nFind a unit vector in the direction of \\(\\mathbf{w}\\) by computing \\(\\frac{\\mathbf{w}}{\\|\\mathbf{w}\\|}\\), where \\(\\|\\mathbf{w}\\|\\) is the Euclidean norm of \\(\\mathbf{w}\\).\n\nBonus Discussion:\n\nExplain why there are infinitely many vectors perpendicular to \\(\\mathbf{v}\\) and describe the general form of all such vectors.",
    "crumbs": [
      "Math | Practicals",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Practice 1: Vectors</span>"
    ]
  },
  {
    "objectID": "math/Resources/Practicals/pr_01_Vectors/pr01_vectors.html#exercise-finding-the-closest-word-with-2d-embeddings",
    "href": "math/Resources/Practicals/pr_01_Vectors/pr01_vectors.html#exercise-finding-the-closest-word-with-2d-embeddings",
    "title": "2  Practice 1: Vectors",
    "section": "2.2 Exercise: Finding the Closest Word with 2D Embeddings",
    "text": "2.2 Exercise: Finding the Closest Word with 2D Embeddings\nContext:\nIn NLP, words can be represented as vectors. Here, each word is represented by a 2-dimensional vector. By comparing these vectors using Euclidean distance and cosine similarity, you can determine which word is “closer” in meaning.\nGiven Word Embeddings:\n\ncheese: [1, 2]\nmushroom: [3, 1]\ntasty: [2, 2]\n\nTasks:\n\nEuclidean Distance:\n\na. Compute the Euclidean distance between tasty and cheese.\n\nb. Compute the Euclidean distance between tasty and mushroom.\n\nc. Which word is closer to tasty based on the Euclidean distance?\n\nCosine Similarity:\n\\[\n\\cos(\\theta)=\\frac{\\mathbf{u}\\cdot\\mathbf{v}}{\\|\\mathbf{u}\\|\\|\\mathbf{v}\\|}\n\\]\n\na. Compute the cosine similarity between tasty and cheese using the formula above.\nb. Compute the cosine similarity between tasty and mushroom.\n\nc. Based on cosine similarity, which word is closer to tasty?\n\nDiscussion:\n\nCompare the outcomes from the Euclidean distance and cosine similarity calculations.\n\nDiscuss why one metric might be preferred over the other in different NLP applications.\n\n\n\n\n\n\n\n\nNote\n\n\n\nCool video by 3blue1brown discussing word vectors (embeddings)",
    "crumbs": [
      "Math | Practicals",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Practice 1: Vectors</span>"
    ]
  },
  {
    "objectID": "math/Resources/Practicals/pr_01_Vectors/pr01_vectors.html#exercise-linear-transformation-matrix-power",
    "href": "math/Resources/Practicals/pr_01_Vectors/pr01_vectors.html#exercise-linear-transformation-matrix-power",
    "title": "2  Practice 1: Vectors",
    "section": "2.3 Exercise: Linear transformation matrix power",
    "text": "2.3 Exercise: Linear transformation matrix power\nTasks: 1. Matrix Power:\n- Compute the matrix power of the following matrix \\(A\\) to the power of \\(n\\): \\[\nA = \\begin{pmatrix}\n    2 & 0 \\\\\n    0 & -1\n\\end{pmatrix}\n\\] - What does the result represent in terms of linear transformations?",
    "crumbs": [
      "Math | Practicals",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Practice 1: Vectors</span>"
    ]
  },
  {
    "objectID": "math/Resources/Practicals/pr_01_Vectors/pr01_vectors.html#exercise-subspace",
    "href": "math/Resources/Practicals/pr_01_Vectors/pr01_vectors.html#exercise-subspace",
    "title": "2  Practice 1: Vectors",
    "section": "2.4 Exercise: Subspace",
    "text": "2.4 Exercise: Subspace\nTasks:\n\n\n\nsubspace_exercise",
    "crumbs": [
      "Math | Practicals",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Practice 1: Vectors</span>"
    ]
  },
  {
    "objectID": "math/Resources/Practicals/pr_01_Vectors/pr01_vectors.html#exercise-vector-space",
    "href": "math/Resources/Practicals/pr_01_Vectors/pr01_vectors.html#exercise-vector-space",
    "title": "2  Practice 1: Vectors",
    "section": "2.5 Exercise: Vector Space",
    "text": "2.5 Exercise: Vector Space\n\n\n\nPoole_vec_space\n\n\n\n\n\nvec_space_exercise",
    "crumbs": [
      "Math | Practicals",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Practice 1: Vectors</span>"
    ]
  },
  {
    "objectID": "math/Resources/Practicals/pr_02_Matrices/pr02_matrices.html",
    "href": "math/Resources/Practicals/pr_02_Matrices/pr02_matrices.html",
    "title": "3  Practice 2: Matrices",
    "section": "",
    "text": "4 Determinants\n\nProve that \\(\\text{det}(B^{-1}AB) = \\text{det}(A)\\) if \\(B\\) is invertible.\nSuppose \\(Q\\) is a \\(3 \\times 3\\) real matrix such that \\(Q^T Q = I\\). What values can \\(\\text{det}(Q)\\) take?.\n\n\n\n5 Linear dependence, basis, rank\nWe have the following dataset of cheeses\n\n\n\n\n\n\n\n\n\n\n\nCheese\nPrice in USD\nPrice in AMD\nYear Made\nYears Waited\nYear Eaten\n\n\n\n\n\\(v_1\\)\n300\n150,000\n2013\n5\n2018\n\n\n\\(v_2\\)\n450\n225,000\n2010\n9\n2019\n\n\n\\(v_3\\)\n350\n175,000\n2014\n3\n2017\n\n\n\\(v_4\\)\n400\n200,000\n2013\n5\n2018\n\n\n\\(v_5\\)\n280\n140,000\n2011\n6\n2017\n\n\n\nTasks\n\n\nDetermine the maximum possible dimension of the subspace spanned by \\(v_1, \\dots, v_5\\).\n\n\nIdentify a maximal linearly independent subset (a basis) of these vectors.\n\n\nWhich features would you use in a Machine Learning and why? If not all, why?\n\n\n\n\n6 \\(n\\)-th power of a matrix (eigendecomposition)\nCompute \\(n\\)-th power of the matrix \\[\nA = \\begin{pmatrix}\n    5 & 8 \\\\\n    2 & 5\n\\end{pmatrix}\n\\]\n\n    It appears you don't have a PDF plugin for this browser.\n    No biggie... you can click here to\n    download the PDF file.\n\n\n\n7 Recap\n\nVector\nNorm\nDot product\nCosine similarity\nMatrix geometrical interpretation\nLinear dependence/independence\nBasis\nRank\nInverse\nDeterminant\nEigenvector, eigenvalue\n\nBbzb if time allows\n\n\nsymmetric matrix eigenvectors orthogonal\n\ncondition number\n\nNotes: https://miro.com/app/board/uXjVIJAiBEM=/?share_link_id=18444345934",
    "crumbs": [
      "Math | Practicals",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Practice 2: Matrices</span>"
    ]
  }
]