[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Python Math & ML Course",
    "section": "",
    "text": "0.1 üìã‘¥’°’Ω’®’∂’©’°÷Å’´ ’Ø’°’º’∏÷Ç÷Å’æ’°’Æ÷Ñ’®\nüì¢ ’ç’Ø’Ω’∏÷Ç’¥ ’•’∂÷Ñ Python-’´ ÷á ’¥’•÷Ñ’•’∂’µ’°’Ø’°’∂ ’∏÷Ç’Ω’∏÷Ç÷Å’¥’°’∂ ’°’∂’æ’≥’°÷Ä ÷Ö’∂’¨’°’µ’∂ ’§’°’Ω’®’∂’©’°÷Å÷â",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Python & Machine Learning Course</span>"
    ]
  },
  {
    "objectID": "index.html#’§’°’Ω’®’∂’©’°÷Å’´-’Ø’°’º’∏÷Ç÷Å’æ’°’Æ÷Ñ’®",
    "href": "index.html#’§’°’Ω’®’∂’©’°÷Å’´-’Ø’°’º’∏÷Ç÷Å’æ’°’Æ÷Ñ’®",
    "title": "Python Math & ML Course",
    "section": "",
    "text": "Python ’¨’•’¶’æ’´ ’∞’´’¥’∏÷Ç’∂÷Ñ’∂’•÷Ä (2.5 - 3 ’°’¥’´’Ω)\nPython ’£÷Ä’°’§’°÷Ä’°’∂’∂’•÷Ä (1 - 1.5 ’°’¥’´’Ω)\n’Ñ’•÷Ñ’•’∂’°’µ’°’Ø’°’∂ ’∏÷Ç’Ω’∏÷Ç÷Å’¥’°’∂ ’∞’°’¥’°÷Ä ’°’∂’∞÷Ä’°’™’•’∑’ø ’¥’°’©’•’¥’°’ø’´’Ø’°’∂ ’£’´’ø’•’¨’´÷Ñ’∂’•÷Ä (1.5 - 2 ’°’¥’´’Ω)\n’Ñ’•÷Ñ’•’∂’°’µ’°’Ø’°’∂ ’∏÷Ç’Ω’∏÷Ç÷Å’∏÷Ç’¥ (3 ’°’¥’´’Ω)",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Python & Machine Learning Course</span>"
    ]
  },
  {
    "objectID": "index.html#‚Ñπ-’Ø’°÷Ä÷á’∏÷Ä-’ø’•’≤’•’Ø’∏÷Ç’©’µ’∏÷Ç’∂’∂’•÷Ä",
    "href": "index.html#‚Ñπ-’Ø’°÷Ä÷á’∏÷Ä-’ø’•’≤’•’Ø’∏÷Ç’©’µ’∏÷Ç’∂’∂’•÷Ä",
    "title": "Python Math & ML Course",
    "section": "0.2 ‚ÑπÔ∏è ‘ø’°÷Ä÷á’∏÷Ä ’ø’•’≤’•’Ø’∏÷Ç’©’µ’∏÷Ç’∂’∂’•÷Ä",
    "text": "0.2 ‚ÑπÔ∏è ‘ø’°÷Ä÷á’∏÷Ä ’ø’•’≤’•’Ø’∏÷Ç’©’µ’∏÷Ç’∂’∂’•÷Ä\n\n‘µ’©’• ’∞’•’ø’°÷Ñ÷Ä÷Ñ÷Ä’æ’°’Æ ’π’•÷Ñ ’¥’•÷Ñ’•’∂’µ’°’Ø’°’∂ ’∏÷Ç’Ω’∏÷Ç÷Å’¥’°’¥’¢ ’Ø’°÷Ä’∏’≤ ’•÷Ñ ’∞’°’≥’°’≠’•’¨ ’¥’´’°’µ’∂ Python-’´ ’§’°’Ω’•÷Ä’´’∂\n‘≤’∏’¨’∏÷Ä ’§’°’Ω’•÷Ä’´ ’ø’•’Ω’°’£÷Ä’∏÷Ç’©’µ’∏÷Ç’∂’∂’•÷Ä’® ÷á ’∂’µ’∏÷Ç’©’•÷Ä’® ’¢’°÷Å ’∞’°’Ω’°’∂’•’¨’´ ’ø’•’≤’°’§÷Ä’æ’•’¨’∏÷Ç ’•’∂ YouTube-’∏÷Ç’¥ ÷á ’¥’•÷Ä ’Ω’ø’•’≤’Æ’°’Æ ’Ø’°’µ÷Ñ’∏÷Ç’¥\n’Ä’°’∂’±’∂’°÷Ä’°÷Ä’æ’•’¨’∏÷Ç ’•’∂ ’¢’°’¶’¥’°’©’´’æ ’ø’∂’°’µ’´’∂ ’°’º’°’ª’°’§÷Ä’°’∂÷Ñ’∂’•÷Ä (’Ø÷Ä’´’ø’´’Ø’°’Ø’°’∂ ’ß ’¨’´’∂’•’¨’∏÷Ç ’¶’£’°’¨’´ ’™’°’¥’°’∂’°’Ø ’∞’°’ø’Ø’°÷Å’∂’•’¨ ’§’°’Ω’®’∂’©’°÷Å’´’∂)\n’á’°’¢’°’©’°’Ø’°’∂ 2-3 ’§’°’Ω, 1.5-2 ’™’°’¥ ’ø÷á’∏’≤’∏÷Ç’©’µ’°’¥’¢\n‘±’∑’≠’°’ø’°’∂÷Ñ’°’µ’´’∂ ÷Ö÷Ä’•÷Ä’´’∂ ’§’°’Ω’•÷Ä’® ’Ø’Ω’Ø’Ω’æ’•’∂ ’¥’∏’ø 20:00, ’´’Ω’Ø ’∑’°’¢’°’©/’Ø’´÷Ä’°’Ø’´’ù 13:00-’´’∂\n‘¥’°’Ω’®’∂’©’°÷Å’´ ’°’æ’°÷Ä’ø’´’∂ ’Ω’•÷Ä’ø’´÷Ü’´’Ø’°’ø ’π’´ ’ø÷Ä’æ’•’¨’∏÷Ç\n‚ÄºÔ∏è‘¥’´’¥’•’¨’∏÷Ç ’æ’•÷Ä’ª’∂’°’™’°’¥’Ø’•’ø’®` ’¥’°÷Ä’ø’´ 23 23:59",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Python & Machine Learning Course</span>"
    ]
  },
  {
    "objectID": "index.html#’∞’≤’∏÷Ç’¥’∂’•÷Ä",
    "href": "index.html#’∞’≤’∏÷Ç’¥’∂’•÷Ä",
    "title": "Python Math & ML Course",
    "section": "0.3 üîó’Ä’≤’∏÷Ç’¥’∂’•÷Ä",
    "text": "0.3 üîó’Ä’≤’∏÷Ç’¥’∂’•÷Ä\n\n‘¥’´’¥’•’¨’∏÷Ç ’∞’≤’∏÷Ç’¥’®’ù https://forms.gle/jpc66NNS7asHSnjz5\n‘¥’°’Ω’®’∂’©’°÷Å’´ ’∂’°’≠’∂’°’Ø’°’∂ ’∫’¨’°’∂’®’ù https://bit.ly/metric_academy",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Python & Machine Learning Course</span>"
    ]
  },
  {
    "objectID": "index.html#print-comments-÷É’∏÷É’∏’≠’°’Ø’°’∂’∂’•÷Ä-’©’æ’•÷Ä-÷á-’©’æ’°’¢’°’∂’°’Ø’°’∂-’£’∏÷Ä’Æ’∏’≤’∏÷Ç’©’µ’∏÷Ç’∂’∂’•÷Ä-input",
    "href": "index.html#print-comments-÷É’∏÷É’∏’≠’°’Ø’°’∂’∂’•÷Ä-’©’æ’•÷Ä-÷á-’©’æ’°’¢’°’∂’°’Ø’°’∂-’£’∏÷Ä’Æ’∏’≤’∏÷Ç’©’µ’∏÷Ç’∂’∂’•÷Ä-input",
    "title": "Python Math & ML Course",
    "section": "2.1 1 - Print, comments, ÷É’∏÷É’∏’≠’°’Ø’°’∂’∂’•÷Ä, ’©’æ’•÷Ä ÷á ’©’æ’°’¢’°’∂’°’Ø’°’∂ ’£’∏÷Ä’Æ’∏’≤’∏÷Ç’©’µ’∏÷Ç’∂’∂’•÷Ä, input",
    "text": "2.1 1 - Print, comments, ÷É’∏÷É’∏’≠’°’Ø’°’∂’∂’•÷Ä, ’©’æ’•÷Ä ÷á ’©’æ’°’¢’°’∂’°’Ø’°’∂ ’£’∏÷Ä’Æ’∏’≤’∏÷Ç’©’µ’∏÷Ç’∂’∂’•÷Ä, input\n’ç’¨’°’µ’§’•÷Ä’ù\nhttps://docs.google.com/presentation/d/1o8K17Jj7k-oL6ZsghdoJ1ddBedmuyeOq699v_8iThM4/edit?usp=sharing\n‘ø’∏’§’®’ù\nhttps://colab.research.google.com/drive/10y8khdVcXQpSQvVGYlQXCMCSIvEP2ETZ?usp=sharing\n’è’∂’°’µ’´’∂’ù\n‘±’º’°’ª’´’∂ ’•÷Ä’Ø’∏÷Ç ’¢’°’™’´’∂’∂’•÷Ä’® Profound-’´÷Å",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Python & Machine Learning Course</span>"
    ]
  },
  {
    "objectID": "index.html#conditions-’∫’°’µ’¥’°’∂’∂’•÷Ä",
    "href": "index.html#conditions-’∫’°’µ’¥’°’∂’∂’•÷Ä",
    "title": "Python Math & ML Course",
    "section": "2.2 2 - Conditions / ’ä’°’µ’¥’°’∂’∂’•÷Ä",
    "text": "2.2 2 - Conditions / ’ä’°’µ’¥’°’∂’∂’•÷Ä\n\n‘ø’∏’§’®¬†https://colab.research.google.com/drive/1g_GoHhH_0Im01luVhnmtp_DHV9jo6BLV?usp=sharing\n’è’∂’°’µ’´’∂ - Profound-’´÷Å ’¢’°’™’´’∂’∂’•÷Ä 5, 7, ’•’©’• ’°’¶’°’ø ’™’°’¥’°’∂’°’Ø ’∏÷Ç’∂’•’∂’°÷Ñ ’∂’°÷á 8-’® ’Ø’°÷Ä’°÷Ñ ’Ω’Ø’Ω’•÷Ñ",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Python & Machine Learning Course</span>"
    ]
  },
  {
    "objectID": "index.html#string-list-range-functions-on-floatslists",
    "href": "index.html#string-list-range-functions-on-floatslists",
    "title": "Python Math & ML Course",
    "section": "2.3 3 - String, list, range, functions on floats/lists",
    "text": "2.3 3 - String, list, range, functions on floats/lists\n‘ø’∏’§’®’ù\nhttps://colab.research.google.com/drive/1QFqpUVWwWrYf07L0TeOv2v-SYlIh4oqH?usp=sharing\n’è’∂’°’µ’´’∂’ù\nProfound-’´÷Å ’¢’°’™’´’∂’∂’•÷Ä 9, 11, 13",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Python & Machine Learning Course</span>"
    ]
  },
  {
    "objectID": "index.html#loops-÷Å’´’Ø’¨’•÷Ä",
    "href": "index.html#loops-÷Å’´’Ø’¨’•÷Ä",
    "title": "Python Math & ML Course",
    "section": "2.4 4 - Loops / ’ë’´’Ø’¨’•÷Ä",
    "text": "2.4 4 - Loops / ’ë’´’Ø’¨’•÷Ä\n‘ø’∏’§’®’ù\nhttps://colab.research.google.com/drive/1IFhd5Wj9KTK2XeE7WM5JDJSRXe65YcYB?usp=sharing\n’è’∂’°’µ’´’∂’ù\nProfound-’´÷Å ’¢’°’™’´’∂’∂’•÷Ä 15, 17, 18",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Python & Machine Learning Course</span>"
    ]
  },
  {
    "objectID": "index.html#liststring-methods-ternary-operators-list-comprehensions",
    "href": "index.html#liststring-methods-ternary-operators-list-comprehensions",
    "title": "Python Math & ML Course",
    "section": "2.5 5 - List/String Methods + Ternary Operators, List Comprehensions",
    "text": "2.5 5 - List/String Methods + Ternary Operators, List Comprehensions\n‘ø’∏’§’®`\n\nList/String Methods -¬†https://colab.research.google.com/drive/1nSTtAe0Yi6XHMh4pbBWzPF_JHflYviGY?usp=sharing\nTernary Operators, List Comprehensions -¬†https://colab.research.google.com/drive/17VmIpUJReBr79HX6e3Hi65sVvSAcocol?usp=sharing\n\n’è’∂’°’µ’´’∂’ù\n\n19 (’è’∏’≤’•÷Ä’´ ÷á ’¶’°’∂’æ’°’Æ’∂’•÷Ä’´ ’∞’•’ø ’°’∑’≠’°’ø’°’∂÷Ñ) - ’¨÷Ä’´’æ\n20 (’Ü’•÷Ä’§÷Ä’æ’°’Æ ÷Å’´’Ø’¨’•÷Ä) - ’¨÷Ä’°÷Å’∏÷Ç÷Å’´’π\n22 (List comprehension) - ’¥’´’∂’π÷á 7 (’∂’•÷Ä’°’º’µ’°’¨), ’¥’∂’°÷Å’°’Æ’® ’¨÷Ä’°÷Å’∏÷Ç÷Å’´’π",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Python & Machine Learning Course</span>"
    ]
  },
  {
    "objectID": "index.html#tuple-set-dictionary",
    "href": "index.html#tuple-set-dictionary",
    "title": "Python Math & ML Course",
    "section": "2.6 6 - Tuple, Set, Dictionary",
    "text": "2.6 6 - Tuple, Set, Dictionary\n‘ø’∏’§’®’ù https://colab.research.google.com/drive/1l9BU3nYRM19hhcNAEd54xyWj1YyhHCOT?usp=sharing\n’è’∂’°’µ’´’∂’ù\n\n24 (Tuple-’∂’•÷Ä ÷á Set-’•÷Ä) - ’¨÷Ä’´’æ\n25 (Dict) - ’¨÷Ä’´’æ, ’¢’°’µ÷Å ’Ñ’∏÷Ä’¶’•’´ ’Ø’∏’§’´ ’≠’∂’§’´÷Ä’® ’Ø’°÷Ä’∏’≤ ’•÷Ñ ’¢’°÷Å ’©’∏’≤’•’¨",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Python & Machine Learning Course</span>"
    ]
  },
  {
    "objectID": "index.html#functions-’£’∏÷Ç÷Å’•-’•÷Ä’Ø’∏÷Ç-’∑’°’¢’°’©-’∞’°’ø’Ø’°÷Å’∂’•’∂÷Ñ",
    "href": "index.html#functions-’£’∏÷Ç÷Å’•-’•÷Ä’Ø’∏÷Ç-’∑’°’¢’°’©-’∞’°’ø’Ø’°÷Å’∂’•’∂÷Ñ",
    "title": "Python Math & ML Course",
    "section": "2.7 7 - Functions (’£’∏÷Ç÷Å’• ’•÷Ä’Ø’∏÷Ç ’∑’°’¢’°’© ’∞’°’ø’Ø’°÷Å’∂’•’∂÷Ñ)",
    "text": "2.7 7 - Functions (’£’∏÷Ç÷Å’• ’•÷Ä’Ø’∏÷Ç ’∑’°’¢’°’© ’∞’°’ø’Ø’°÷Å’∂’•’∂÷Ñ)\n‘ø’∏’§’•÷Ä\n\nFunctions -¬†https://colab.research.google.com/drive/18yWk80QIy3W5qHYyh4bJdJIdZc7Ai-D5?usp=sharing\nFunctions 2 -¬†https://colab.research.google.com/drive/1Zty6fUmxC4084vf22tALDU4SE_pjYhwG?usp=sharing\n\n’è’∂’°’µ’´’∂’ù\n\n26 (’ñ’∏÷Ç’∂’Ø÷Å’´’°’∂’•÷Ä) - ’¨÷Ä’´’æ\n27 (’ñ’∏÷Ç’∂’Ø÷Å’´’°’∂’•÷Ä 2) - ’¨÷Ä’´’æ\n28 (Lambda) - ’¨÷Ä’´’æ",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Python & Machine Learning Course</span>"
    ]
  },
  {
    "objectID": "index.html#terminal-working-with-multiple-files-file-io-packages-os-random-time-tqdm",
    "href": "index.html#terminal-working-with-multiple-files-file-io-packages-os-random-time-tqdm",
    "title": "Python Math & ML Course",
    "section": "2.8 8 - Terminal, Working with multiple files, file I/O, Packages (os, random, time, tqdm)",
    "text": "2.8 8 - Terminal, Working with multiple files, file I/O, Packages (os, random, time, tqdm)\n‘ø’∏’§’®’ù\nhttps://colab.research.google.com/drive/1XyY3N1UZhfahbGmF5QzBKgHWqvqtTZgV?usp=sharing\nGit / GitHub`\n’Ä’°’µ’•÷Ä’•’∂ ’æ’´’§’•’∏’∂’•÷Ä’´ ’∑’°÷Ä÷Ñ -\nhttps://www.youtube.com/playlist?list=PLQLz3vJxwofh8KSaJ7FoA5Bw2Zdvyo4S_\nVSCode / PyCharm, Python, Anaconda\n\nPython, VSCode, Anaconda -¬†https://hayktarkhanyan.notion.site/Install-Python-VS-Code-and-Anaconda-b1cc550b63a04af298f3c784e5c8e49e?pvs=4\nPyCharm -¬†https://armen-melkonyan.notion.site/install-Pycharm-Windows-2516aeae013344d7a288a7ee01377a06\n\nTerminal-’´ ’∞÷Ä’°’¥’°’∂’∂’•÷Ä Mac/Linux-’´ ’∞’°’¥’°’º\nhttps://www.geeksforgeeks.org/basic-linux-commands/\n’è’∂’°’µ’´’∂’ù ’±’•’º’´ ’∞’•’ø ’°’∂’•’¨’∏÷Ç ’ø’∂’°’µ’´’∂\n\n29 ****’ñ’°’µ’¨’•÷Ä - ’¨÷Ä’´’æ (’Ø’°’¥ ’£’∏’∂’• 2, 3, 6, 8, 10)\n‘º’∏÷Ç’Æ’∏÷Ç’¥’∂’•÷Ä’´ ’æ’´’§’•’∏’∂’•÷Ä - https://www.youtube.com/playlist?list=PLpwQpnE0hLO5JbMe0W44DSWEKn-Dbl7o7\n\n’Ä’´’¥’∂’°’Ø’°’∂ ’ø’∂’°’µ’´’∂’ù\n\nVSCode ’Ø’°’¥ PyCharm ÷Ñ’°’∑’•’¨\nPython ÷Ñ’°’∑’•’¨\nAnaconda ÷Ñ’°’∑’•’¨\n’à÷Ç ’ß’Ω ’∂’•÷Ä÷Ñ’´ ’∫÷Ä’∏’•’Ø’ø’®’ù\n\nFile Organizer",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Python & Machine Learning Course</span>"
    ]
  },
  {
    "objectID": "index.html#git-github-venvs-anaconda-pep8-clean-codearchitecture",
    "href": "index.html#git-github-venvs-anaconda-pep8-clean-codearchitecture",
    "title": "Python Math & ML Course",
    "section": "2.9 8.5 - Git / GitHub, Venvs, Anaconda + PEP8, clean code/architecture",
    "text": "2.9 8.5 - Git / GitHub, Venvs, Anaconda + PEP8, clean code/architecture\n\nAlso - Decorators (In case not covered earlier)",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Python & Machine Learning Course</span>"
    ]
  },
  {
    "objectID": "index.html#exception-handling",
    "href": "index.html#exception-handling",
    "title": "Python Math & ML Course",
    "section": "2.10 9 - Exception Handling",
    "text": "2.10 9 - Exception Handling\n‘ø’∏’§’®’ù https://colab.research.google.com/drive/16zfO5KSFfYI6f2PgVLflzZhAz2cwXIqI?usp=sharing\n’è’∂’°’µ’´’∂’ù\nPassword generator/validator",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Python & Machine Learning Course</span>"
    ]
  },
  {
    "objectID": "index.html#classes-inheritance",
    "href": "index.html#classes-inheritance",
    "title": "Python Math & ML Course",
    "section": "2.11 10 - Classes, Inheritance",
    "text": "2.11 10 - Classes, Inheritance\n‘ø’∏’§’®’ù\nhttps://colab.research.google.com/drive/1CsNjiiI2ya7zMNa9FuSPNsg40wtf5eBW?usp=sharing\n’è’∂’°’µ’´’∂’ù\n\n2.11.1 üçø IMDB Data (classes)\n’é’•÷Ä÷Å÷Ä’•÷Ñ ’°’µ’Ω ’§’°’ø’°’∂’ù\nimdb_top_999.json\n‘ø’°÷Ä’§’°’¨’∏÷Ç ’∞’°’¥’°÷Ä ’Ø’°÷Ä’∏’≤ ’•÷Ñ ’°’∑’≠’°’ø’°÷Å’∂’•’¨ ’°’µ’Ω ’Ø’∏’§’®’ù\nimport json\nPATH = \"imdb_top_999.json\"\n\nwith open(PATH, \"r\") as f:\n    data = json.load(f)\n\nTO_DELETE = [\"No_of_Votes\", \"Overview\", \"Meta_score\"]\n\nfor kino in data.keys():\n    for k in TO_DELETE:\n        del data[kino][k]\n\n    data[kino][\"Name\"] = kino\n’è’∫’•÷Ñ ’ø’•’Ω’•÷Ñ ’©’• ’´’∂’π ’° data-’∂ ’∏÷Ç ’Ω’ø’•’≤’Æ’•÷Ñ ’∞’•’ø÷á’µ’°’¨ ’Ø’¨’°’Ω’∂’•÷Ä’®’ù\n\n\n2.11.2 Movie\n\n‘ø’∂’Ø’°÷Ä’°’£÷Ä’´ ’´’∂’π-’∏÷Ä ’Ø’∏’∂’Ø÷Ä’•’ø ’Ø’´’∂’∏÷â\n‘ø’Ω’ø’°’∂’° ’∏÷Ä’∫’•’Ω ’°÷Ä’£’∏÷Ç’¥’•’∂’ø data-’∏÷Ç’¥ ’•’≤’°’Æ ’¢’∏’¨’∏÷Ä ’¢’°’∂’°’¨’´’∂’•÷Ä’® (Name, Released_Year, Runtime, ‚Ä¶)\n‘ø’∏÷Ç’∂’•’∂’° ’¥’•’©’∏’§ ’§’•÷Ä’°’Ω’°’∂’∂’•÷Ä’´ ÷Å’∏÷Ç÷Å’°’Ø’® ’Ω’ø’°’∂’°’¨’∏÷Ç ’∞’°’¥’°÷Ä\n‘ø’∏÷Ç’∂’•’∂’° ’¥’•’©’∏’§ ’∏÷Ä’® ’Ø’æ’•÷Ä’°’§’°÷Ä’±’∂’´ ’©’• ÷Ñ’°’∂’´ ’™’°’¥ ’° ’ø÷á’∏÷Ç’¥ ’Ø’´’∂’∏’∂\n‘ø’∏÷Ç’∂’•’∂’° ’¥’•’©’∏’§ ’∏÷Ä’® ’ø’•÷Ñ’Ω’ø’∏’æ ’Ø’∂’Ø’°÷Ä’°’£÷Ä’´ ’Ø’°÷Ä’≥ ’∂’Ø’°÷Ä’°’£’´÷Ä’® (‘ø’´’∂’∏’µ’´ ’º’•’™’´’Ω’∏÷Ä’∂ ’ß ‚Ä§‚Ä§‚Ä§, ’∞÷Ä’°’∫’°÷Ä’°’Ø’æ’•’¨ ’ß ‚Ä§‚Ä§‚Ä§ ’©’æ’°’Ø’°’∂’´’∂, ’£’¨’≠’°’æ’°÷Ä ’§’•÷Ä’°’Ω’°’∂’∂’•÷Ä’∂ ’•’∂ ‚Ä§‚Ä§‚Ä§)\n\n\n\n2.11.3 Actor\n\n‘ø’∂’Ø’°÷Ä’°’£÷Ä’´ ’´’∂’π-’∏÷Ä ’Ø’∏’∂’Ø÷Ä’•’ø ’§’•÷Ä’°’Ω’°’∂’´\n’à÷Ä’∫’•’Ω ’°÷Ä’£’∏÷Ç’¥’•’∂’ø ’Ø’Ω’ø’°’∂’° ’§’•÷Ä’°’Ω’°’∂’´ ’°’∂’∏÷Ç’∂’®\n‘ø’£’ø’∂’´ ’¢’∏’¨’∏÷Ä ÷Ü’´’¨’¥’•÷Ä’® ’∏÷Ä’∏÷Ç’¥ ’≠’°’≤’°÷Å’•’¨ ’ß ’§’•÷Ä’°’Ω’°’∂’®\n‘ø’∏÷Ç’∂’•’∂’° ’¥’•’©’∏’§ ’°’µ’§ ÷Ü’´’¨’¥’•÷Ä’´\n\n’î’°’∂’°’Ø’®\n’Ñ’´’ª’´’∂ ’º’•’µ’©’´’∂’£’®\n‘±’¥’•’∂’°’ø’°÷Ä’°’Æ’æ’°’Æ ’™’°’∂÷Ä’® ’Ω’ø’°’∂’°’¨’∏÷Ç ’∞’°’¥’°÷Ä\n\n‘ø’∏÷Ç’∂’•’∂’° ’¥’•’©’∏’§’∂’•÷Ä ’∏÷Ä ’Ø’æ’•÷Ä’°’§’°÷Ä’±’∂’•’∂\n\n’å’•’™’´’Ω’µ’∏÷Ä’´ ’°’∂’∏÷Ç’∂’® ’∏÷Ç’¥ ÷Ü’´’¨’¥’•÷Ä’∏÷Ç’¥ ’°’¥’•’∂’°’∞’°’≥’°’≠’∂ ’° ’∂’Ø’°÷Ä’æ’•’¨\n‘¥’•÷Ä’°’Ω’°’∂’´ ’°’∂’∏÷Ç’∂’® ’∏÷Ç’¥ ’∞’•’ø ’°’¥’•’∂’°’∞’°’≥’≠ ’° ’°’∑’≠’°’ø’•’¨\n‘ø’°÷Ä’≥ ’∂’Ø’°÷Ä’°’£’´÷Ä ’§’•÷Ä’°’Ω’°’∂’´ ’¥’°’Ω’´’∂ (‘±’∂’∏÷Ç’∂’∂ ’ß’ù ‚Ä§‚Ä§‚Ä§, ’≠’°’≤’°÷Å’•’¨ ’ß ‚Ä§‚Ä§ ’∞’°’ø ÷Ü’´’¨’¥’∏÷Ç’¥, ’ñ’´’¨’¥’•÷Ä’´ ’¥’´’ª’´’∂ ’º’•’µ’ø’´’∂’£’∂ ’ß ‚Ä§‚Ä§‚Ä§ ÷á ’°’µ’¨’∂)\n\n\n’Ü’¥’°’∂ ’æ’•÷Ä’¨’∏÷Ç’Æ’∏÷Ç’©’µ’∏÷Ç’∂ ’Ø’°÷Ä’•’¨’´ ’° ’°’∂’•’¨ ’∂’°÷á ’®’Ω’ø ’Ø’´’∂’∏’µ’´ ’™’°’∂÷Ä’´, ’Ω’ø’°’∂’°’¨ ÷Ñ’°’∂’´ ÷Ü’´’¨’¥ ’Ø’° ’´÷Ñ’Ω ’™’°’∂÷Ä’´, ’¥’´’ª’´’∂ ’º’•’µ’ø’´’∂’£, ’°’¥’•’∂’°’∑’°’ø ’ß’§ ’™’°’∂÷Ä’∏’æ ’Ø’´’∂’∏’∂’•÷Ä’∏÷Ç’¥ ’≠’°’≤’°÷Å’∏’≤ ’§’•÷Ä’°’Ω’°’∂, ’ø’•’∂÷Å ’Ø’´’∂’∏’∂’•÷Ä ’∂’Ø’°÷Ä’∏’æ ’º’•’™’´’Ω’µ’∏’º ÷á ’°’µ’¨’∂",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Python & Machine Learning Course</span>"
    ]
  },
  {
    "objectID": "index.html#more-on-classes-polymorphism-encapsulation-abstraction-generators-iterators",
    "href": "index.html#more-on-classes-polymorphism-encapsulation-abstraction-generators-iterators",
    "title": "Python Math & ML Course",
    "section": "2.12 11 - More on classes + Polymorphism, Encapsulation, Abstraction + Generators / Iterators",
    "text": "2.12 11 - More on classes + Polymorphism, Encapsulation, Abstraction + Generators / Iterators",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Python & Machine Learning Course</span>"
    ]
  },
  {
    "objectID": "index.html#logging-unittest-argparser",
    "href": "index.html#logging-unittest-argparser",
    "title": "Python Math & ML Course",
    "section": "3.1 12 - Logging, Unittest, Argparser",
    "text": "3.1 12 - Logging, Unittest, Argparser",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Python & Machine Learning Course</span>"
    ]
  },
  {
    "objectID": "index.html#numpy",
    "href": "index.html#numpy",
    "title": "Python Math & ML Course",
    "section": "3.2 13 - NumPy",
    "text": "3.2 13 - NumPy",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Python & Machine Learning Course</span>"
    ]
  },
  {
    "objectID": "index.html#pandas",
    "href": "index.html#pandas",
    "title": "Python Math & ML Course",
    "section": "3.3 14-15 - Pandas",
    "text": "3.3 14-15 - Pandas",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Python & Machine Learning Course</span>"
    ]
  },
  {
    "objectID": "index.html#data-visualization-’£’∏÷Ç÷Å’•-2-’∑’°’¢’°’©-’∞’°’ø’Ø’°÷Å’∂’•’∂÷Ñ",
    "href": "index.html#data-visualization-’£’∏÷Ç÷Å’•-2-’∑’°’¢’°’©-’∞’°’ø’Ø’°÷Å’∂’•’∂÷Ñ",
    "title": "Python Math & ML Course",
    "section": "3.4 16 - Data Visualization (’£’∏÷Ç÷Å’• 2 ’∑’°’¢’°’© ’∞’°’ø’Ø’°÷Å’∂’•’∂÷Ñ)",
    "text": "3.4 16 - Data Visualization (’£’∏÷Ç÷Å’• 2 ’∑’°’¢’°’© ’∞’°’ø’Ø’°÷Å’∂’•’∂÷Ñ)",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Python & Machine Learning Course</span>"
    ]
  },
  {
    "objectID": "index.html#some-other-packages-streamlit-dask-sweetviz-numba",
    "href": "index.html#some-other-packages-streamlit-dask-sweetviz-numba",
    "title": "Python Math & ML Course",
    "section": "3.5 17 - Some other packages (Streamlit, Dask, Sweetviz, Numba, ‚Ä¶)",
    "text": "3.5 17 - Some other packages (Streamlit, Dask, Sweetviz, Numba, ‚Ä¶)",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Python & Machine Learning Course</span>"
    ]
  },
  {
    "objectID": "index.html#scraping",
    "href": "index.html#scraping",
    "title": "Python Math & ML Course",
    "section": "3.6 18 - Scraping",
    "text": "3.6 18 - Scraping",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Python & Machine Learning Course</span>"
    ]
  },
  {
    "objectID": "index.html#flask-fastapi",
    "href": "index.html#flask-fastapi",
    "title": "Python Math & ML Course",
    "section": "3.7 19 - Flask / FastAPI",
    "text": "3.7 19 - Flask / FastAPI",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Python & Machine Learning Course</span>"
    ]
  },
  {
    "objectID": "index.html#linear-algebra",
    "href": "index.html#linear-algebra",
    "title": "Python Math & ML Course",
    "section": "4.1 üßÆ 20-22.5 Linear Algebra",
    "text": "4.1 üßÆ 20-22.5 Linear Algebra\n\nVectors, vector operations, dot product, norm\nVector spaces and subspaces\nMatrices, matrix operations\nGeometric interpretation of matrices\nRow echelon form\nDeterminant in 2x2 and 3x3 cases, trace\nDeterminant in general case\nSystems of linear equations\nGauss-Jordan elimination\nInverse matrix\nLinear independence\nBasis, rank, dimension\nEigenvalues and eigenvectors\n(Positive/negative definite matrices\nDecompositions",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Python & Machine Learning Course</span>"
    ]
  },
  {
    "objectID": "index.html#calculus",
    "href": "index.html#calculus",
    "title": "Python Math & ML Course",
    "section": "4.2 üìà 22.5 - 24 Calculus",
    "text": "4.2 üìà 22.5 - 24 Calculus\n\nLimit of sequence and function\nDerivative\nExtrema of a function\nTaylor polynomials\nIndefinite integral, definite integral\nPartial derivative\nGradient, directional gradient\nMore topics",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Python & Machine Learning Course</span>"
    ]
  },
  {
    "objectID": "index.html#optimization",
    "href": "index.html#optimization",
    "title": "Python Math & ML Course",
    "section": "4.3 ‚õ∞Ô∏è 25 - 27 Optimization",
    "text": "4.3 ‚õ∞Ô∏è 25 - 27 Optimization\n\nQuadratic forms and Sylvester‚Äôs criterion\nGradient Descent\nMomentum\nAdaGrad / RMSProp / ADAM\nSecond order methods\nConstrained optimiziation\nEvolutionary algorithms\nBayesian optimization\nMulticriteria optimization",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Python & Machine Learning Course</span>"
    ]
  },
  {
    "objectID": "index.html#probability-theory",
    "href": "index.html#probability-theory",
    "title": "Python Math & ML Course",
    "section": "4.4 üé≤ 28 - 29 Probability Theory",
    "text": "4.4 üé≤ 28 - 29 Probability Theory\n\nSample space, events, probability\nIndependence\nConditional probability, total probability\nBayes rule\nGeometric probability\nRandom variable\nPMF, CDF, PDF\nExpected value, variance\nCovariance and correlation\nDistributions\nLaws of large numbers\nCentral limit theorem",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Python & Machine Learning Course</span>"
    ]
  },
  {
    "objectID": "index.html#statistics",
    "href": "index.html#statistics",
    "title": "Python Math & ML Course",
    "section": "4.5 üìä30 - 31 Statistics",
    "text": "4.5 üìä30 - 31 Statistics\n\nPoint estimation: Mean, median, mode\nEstimator properties\nMAP / MLE\nConfidence intervals and hypothesis testing\nP-values, type I and type II errors",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Python & Machine Learning Course</span>"
    ]
  },
  {
    "objectID": "index.html#linear-regression",
    "href": "index.html#linear-regression",
    "title": "Python Math & ML Course",
    "section": "5.1 32 Linear Regression",
    "text": "5.1 32 Linear Regression\n\nAssumptions\nLoss\nGradient based optimization\nNormal Equation\nInterpretation of Coefficients",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Python & Machine Learning Course</span>"
    ]
  },
  {
    "objectID": "index.html#main-concepts",
    "href": "index.html#main-concepts",
    "title": "Python Math & ML Course",
    "section": "5.2 33 - 34 Main Concepts",
    "text": "5.2 33 - 34 Main Concepts\n\nEncoding categoricals\nFeature scaling\nTrain Val Test split (data leakage issue)\n(Stratified) Cross validation\nRegression evaluation metrics",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Python & Machine Learning Course</span>"
    ]
  },
  {
    "objectID": "index.html#more-regression-main-concepts-2",
    "href": "index.html#more-regression-main-concepts-2",
    "title": "Python Math & ML Course",
    "section": "5.3 35 - 36 More Regression + Main Concepts 2",
    "text": "5.3 35 - 36 More Regression + Main Concepts 2\n\nPolynomial Regression\nUnder / Overfitting\nRegularization\n\nRidge\nLasso\n\nHyperparameter Search\nFeature Engineering\nOutliers\nThreshold tuning",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Python & Machine Learning Course</span>"
    ]
  },
  {
    "objectID": "index.html#logistic-regression",
    "href": "index.html#logistic-regression",
    "title": "Python Math & ML Course",
    "section": "5.4 37 Logistic Regression",
    "text": "5.4 37 Logistic Regression\n\nLogistic regression\nLog odds\nClassification evaluation metrics",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Python & Machine Learning Course</span>"
    ]
  },
  {
    "objectID": "index.html#trees",
    "href": "index.html#trees",
    "title": "Python Math & ML Course",
    "section": "5.5 38 Trees",
    "text": "5.5 38 Trees\n\nDecision tree\nBagging\nBoosting\nNotable models (i.e.¬†LightGBM)",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Python & Machine Learning Course</span>"
    ]
  },
  {
    "objectID": "index.html#model-interpretation-and-feature-selection",
    "href": "index.html#model-interpretation-and-feature-selection",
    "title": "Python Math & ML Course",
    "section": "5.6 39 Model interpretation and Feature selection",
    "text": "5.6 39 Model interpretation and Feature selection",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Python & Machine Learning Course</span>"
    ]
  },
  {
    "objectID": "index.html#unsupervised-learning",
    "href": "index.html#unsupervised-learning",
    "title": "Python Math & ML Course",
    "section": "5.7 40 Unsupervised Learning",
    "text": "5.7 40 Unsupervised Learning\n\nKMeans\nDBSCAN\nHierarchical\nClustering evaluation metrics",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Python & Machine Learning Course</span>"
    ]
  },
  {
    "objectID": "index.html#neural-networks",
    "href": "index.html#neural-networks",
    "title": "Python Math & ML Course",
    "section": "5.8 41 - 42 Neural Networks",
    "text": "5.8 41 - 42 Neural Networks",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Python & Machine Learning Course</span>"
    ]
  },
  {
    "objectID": "index.html#intro-to-computer-vision",
    "href": "index.html#intro-to-computer-vision",
    "title": "Python Math & ML Course",
    "section": "5.9 43 - 44 Intro to Computer Vision",
    "text": "5.9 43 - 44 Intro to Computer Vision",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Python & Machine Learning Course</span>"
    ]
  },
  {
    "objectID": "index.html#intro-to-natural-language-processing",
    "href": "index.html#intro-to-natural-language-processing",
    "title": "Python Math & ML Course",
    "section": "5.10 44 - 45 Intro to Natural Language Processing",
    "text": "5.10 44 - 45 Intro to Natural Language Processing",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Python & Machine Learning Course</span>"
    ]
  },
  {
    "objectID": "index.html#intro-to-gen-ai",
    "href": "index.html#intro-to-gen-ai",
    "title": "Python Math & ML Course",
    "section": "5.11 46 - 47 Intro to Gen AI",
    "text": "5.11 46 - 47 Intro to Gen AI",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Python & Machine Learning Course</span>"
    ]
  },
  {
    "objectID": "index.html#’£’∏÷Ç÷Å’•-’∂’°÷á-knn-svm-information-theory-gaussian-process",
    "href": "index.html#’£’∏÷Ç÷Å’•-’∂’°÷á-knn-svm-information-theory-gaussian-process",
    "title": "Python Math & ML Course",
    "section": "5.12 ‘≥’∏÷Ç÷Å’• ’∂’°÷á KNN, SVM, Information Theory, Gaussian Process",
    "text": "5.12 ‘≥’∏÷Ç÷Å’• ’∂’°÷á KNN, SVM, Information Theory, Gaussian Process",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Python & Machine Learning Course</span>"
    ]
  },
  {
    "objectID": "index.html#final-project",
    "href": "index.html#final-project",
    "title": "Python Math & ML Course",
    "section": "5.13 Final Project",
    "text": "5.13 Final Project",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Python & Machine Learning Course</span>"
    ]
  },
  {
    "objectID": "math/pr/pr1.html",
    "href": "math/pr/pr1.html",
    "title": "Python Math & ML Course",
    "section": "",
    "text": "&lt;!DOCTYPE html&gt;\n\n\n\n\n\n\npr1\n\n\n\n\n\n\n\nPractice Problems 1\n\n\n\n\n\nEvaluate the expression:\n\n\n\n\n\\(3\\cdot (\\va + 2\\vb)\\), where \\(\\va= \\begin{bmatrix} 4 \\\\\n5\\end{bmatrix}\\), \\(\\vb= \\begin{bmatrix}\n2 \\\\ -3 \\end{bmatrix}\\),\n\n\n\n\n\\(5\\va - 10\\vb\\), where \\(\\va= \\begin{bmatrix} 17 \\\\ 24\n\\end{bmatrix}\\), \\(\\vb= \\begin{bmatrix} 5\n\\\\ 12 \\end{bmatrix}\\),\n\n\n\n\n\\(3\\vu^T-(\\vv+2\\vw)^T\\), where \\(\\vu= \\begin{bmatrix}4\\\\1\\\\-3 \\end{bmatrix},\n\\vv= \\begin{bmatrix}5\\\\0\\\\2 \\end{bmatrix}, \\vw= \\begin{bmatrix}1\\\\3\\\\-2\n\\end{bmatrix}\\).\n\n\n\n\n\n\nMichael mowed lawns on weekends to help pay his college tuition bills. He charged his customers according to the size of their lawns at a rate of \\(0.5 per square foot and kept a record of the areas of their\nlawns in an ordered list: &lt;span class=\"math display\"&gt;\\)\\(\\va = (200, 300,\n50, 50, 100, 100, 200, 500, 1000, 100) .\\)\\(&lt;/span&gt; He also listed the\nnumber of times he mowed each lawn in a given year. For the year 2023\nthat ordered list was &lt;span class=\"math display\"&gt;\\)\\(\\vb = (20, 1, 2, 4,\n1, 5, 2, 1, 10, 6) .\\)\\(&lt;/span&gt;&lt;/p&gt;\n&lt;p&gt;a) Pretend that &lt;span class=\"math inline\"&gt;\\)\\(&lt;/span&gt; and &lt;span\nclass=\"math inline\"&gt;\\)\\(&lt;/span&gt; are vectors and compute &lt;span\nclass=\"math inline\"&gt;\\)\\(&lt;/span&gt;.&lt;br /&gt;\nb) What quantity does the dot product &lt;span\nclass=\"math inline\"&gt;\\)\\(&lt;/span&gt; measure?&lt;br /&gt;\nc) How much did Michael earn from mowing lawns in 2023? Write an\nexpression for this amount in terms of the vectors &lt;span\nclass=\"math inline\"&gt;\\)\\(&lt;/span&gt; and &lt;span\nclass=\"math inline\"&gt;\\)\\(&lt;/span&gt;.&lt;/p&gt;\n&lt;/div&gt;\n&lt;div class=\"problem\"&gt;\n&lt;p&gt;Check if the following set is a vector space:&lt;/p&gt;\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;&lt;span class=\"math inline\"&gt;\\)A = \\(&lt;/span&gt;, with the usual\noperations &lt;span class=\"math inline\"&gt;+&lt;/span&gt; and &lt;span\nclass=\"math inline\"&gt;‚ãÖ&lt;/span&gt;,&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;&lt;span class=\"math inline\"&gt;\\)B = {\n\\[\\begin{bmatrix} 0 \\\\ 0 \\\\\na\\end{bmatrix}\\]\na}\\(&lt;/span&gt;\nwith the usual operations &lt;span class=\"math inline\"&gt;+&lt;/span&gt; and &lt;span\nclass=\"math inline\"&gt;‚ãÖ&lt;/span&gt;,&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;&lt;span class=\"math inline\"&gt;\\)C = ^2 = {\n\\[\\begin{bmatrix} a\n\\\\ b \\end{bmatrix}\\]\na, b}\\(&lt;/span&gt;, with the usual operation &lt;span\nclass=\"math inline\"&gt;‚ãÖ&lt;/span&gt; and the addition defined as: &lt;span\nclass=\"math display\"&gt;\\)\\(\\begin{bmatrix} x_1 \\\\ x_2\n\\end{bmatrix}  +  \\begin{bmatrix} y_1 \\\\ y_2 \\end{bmatrix}  =\n\\begin{bmatrix} x_1+y_1 \\\\ x_2+y_2+1 \\end{bmatrix},\\)\\(&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;The set of all polynomials of degree &lt;span\nclass=\"math inline\"&gt;‚ÄÑ‚â§‚ÄÑ2&lt;/span&gt;, with the usual operations &lt;span\nclass=\"math inline\"&gt;+&lt;/span&gt; and &lt;span\nclass=\"math inline\"&gt;‚ãÖ&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;\n&lt;div class=\"problem\"&gt;\n&lt;p&gt;Calculate the Manhattan (L1) and Euclidean (L2) norms of the\nfollowing vectors:&lt;/p&gt;\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;&lt;span class=\"math inline\"&gt;\\)=\n\\[\\begin{bmatrix} 4\\\\-5\\\\7\n\\end{bmatrix}\\]\n\\(&lt;/span&gt;,&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;&lt;span class=\"math inline\"&gt;\\)+\\(&lt;/span&gt;, where &lt;span\nclass=\"math inline\"&gt;\\)=\n\\[\\begin{bmatrix}12\\\\5\\\\0\\end{bmatrix}\\]\n, =\n\\[\\begin{bmatrix} -1\\\\2\\\\-2\\end{bmatrix}\\]\n$,\n\n\n\n\n\\(13\\vc\\), where \\(\\vc=\n\\begin{bmatrix}3\\\\4\\end{bmatrix}\\),\n\n\n\n\n\\(-\\vd\\), where \\(\\vd=\n\\begin{bmatrix}1\\\\1\\\\2\\end{bmatrix}\\).\n\n\n\n\n\n\nFind the angles between the following vectors:\n\n\n\n\n\\(\\va= \\begin{bmatrix}\n1\\\\3\\\\2\\end{bmatrix}\\) and \\(\\vb=\n\\begin{bmatrix} 4\\\\-4\\\\4\\end{bmatrix}\\),\n\n\n\n\n\\(\\va= \\begin{bmatrix}\n3\\\\0\\end{bmatrix}\\) and \\(\\vb=\n\\begin{bmatrix} 3\\\\3\\end{bmatrix}\\).\n\n\n\n\n\n\nEvaluate the expression:\n\n\n\n\nAB, where \\(A=\\begin{bmatrix}\n        3&amp;2\\\\1&amp;4   \\end{bmatrix}\\), \\(B=\\begin{bmatrix}\n        5&amp;-1&amp;2\\\\0&amp;2&amp;3   \\end{bmatrix}\\),\n\n\n\n\nB2‚ÄÑ=‚ÄÑBB, where \\(B=\\begin{bmatrix}\n        4&amp;-3&amp;2\\\\3&amp;-2&amp;0\\\\1&amp;1&amp;3   \\end{bmatrix}\\),\n\n\n\n\n(A‚ÄÖ‚àí‚ÄÖB)C, where \\(A=\\begin{bmatrix}\n        2&amp;5&amp;4\\\\-3&amp;-2&amp;4\\\\5&amp;9&amp;2   \\end{bmatrix}\\), \\(B=\\begin{bmatrix}\n        2&amp;1&amp;5\\\\-5&amp;2&amp;2\\\\1&amp;6&amp;-1   \\end{bmatrix}\\), \\(C=\\begin{bmatrix}\n        4&amp;-1\\\\1&amp;2\\\\3&amp;3   \\end{bmatrix}\\).",
    "crumbs": [
      "Math",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>pr1.html</span>"
    ]
  },
  {
    "objectID": "python/Python_01_Intro.html",
    "href": "python/Python_01_Intro.html",
    "title": "3¬† ‘±÷Ä’ø’°’Æ’∏÷Ç’¥ (Print(’ø’∫’•’¨))",
    "section": "",
    "text": "3.0.1 sep\nprint(\"‘≤’°÷Ä÷á÷á÷á÷á÷á÷á÷á, ’•’•’•’•’•÷Ä’Ø’´÷Ä\")\n(—Å) Shushan Petrosyan, Inga Anush Arshakyanner, Armen Martirosyan, Dodo Babayan\nPython-’∏÷Ç’¥ ’ø’æ’µ’°’¨’∂’•÷Ä’® ’°÷Ä’ø’°’Æ’•’¨’∏÷Ç ’∞’°’¥’°÷Ä ’∫’°÷Ä’¶’°’∫’•’Ω ’£÷Ä’∏÷Ç’¥ ’•’∂÷Ñ print (’ø’∫’•’¨), ’°’∫’° ’¢’°÷Å ÷É’°’Ø’°’£’Æ’•÷Ä’´ ’¥’•’ª ’©’• ’´’∂’π ’•’∂÷Ñ ’∏÷Ç’¶’∏÷Ç’¥ ’°÷Ä’ø’°’Æ’•’¨\n’∂’∏÷Ç’µ’∂ ’±÷á ’Ø’°÷Ä’∏’≤ ’•’∂÷Ñ ’∂’°÷á ’ø’•÷Ñ’Ω’ø ’°÷Ä’ø’°’Æ’•’¨ (’ø’∫’•’¨)\n‘ø’°÷Ä’∏’≤ ’•’∂÷Ñ ’∂’°÷á ’¥’´’°’∂’£’°’¥’´÷Å ’¥’´ ÷Ñ’°’∂’´ ’¢’°’∂ ’ø’∫’•’¨\n‘ø’°÷Ä’∏’≤ ’•’∂÷Ñ ’ø’°÷Ä’¢’•÷Ä ’ø’∏’≤’•÷Ä’´ ’æ÷Ä’° ’£÷Ä’•’¨ ’∞÷Ä’°’¥’°’∂’∂’•÷Ä’® ÷á Python-’® ’æ’•÷Ä÷á’´÷Å ’∂’•÷Ä÷Ñ÷á ’Ø’°÷Ä’§’°’¨’∏’æ ’Ø’Ø’°’ø’°÷Ä’´ ’∞÷Ä’°’¥’°’∂’∂’•÷Ä’®\n‘±’º’°’ª’°’§÷Ä’°’∂÷Ñ ’ø’∫’•’¨ ‚ÄúHello World!‚Äù ’ø’•÷Ñ’Ω’ø’® ÷á ’∏÷Ç÷Ä’°’≠’°’∂’°’¨\n’•’©’• ’∏÷Ç’¶’∏÷Ç’¥ ’•’∂÷Ñ ’∏÷Ä ’°÷Ä’™’•÷Ñ’∂’•÷Ä’® ’´÷Ä’°÷Ä’´÷Å ’°’º’°’∂’±’∂’°÷Å’æ’•’∂ ’∏’π’©’• ’¢’°÷Å’°’ø’∏’æ ’°’µ’¨ ’¥’•÷Ä ’∏÷Ç’¶’°’Æ ’¢’°’∂’∏’æ, ’°’∫’° ’∫’•’ø÷Ñ ’ß print ’∞÷Ä’°’¥’°’∂’´ ’¥’•’ª ’°’æ’•’¨’°÷Å’∂’•’∂÷Ñ’ù print('’´’∂’π-’∏÷Ä ’¢’°’∂', '’´’∂’π-’∏÷Ä ’∏÷Ç÷Ä’´’∑ ’¢’°’∂', sep='¬Ø\\_(„ÉÑ)_/¬Ø')\nsep-’® ’£’°’¨’´’Ω’° ’°’∂’£’¨’•÷Ä’•’∂ separator ’¢’°’º’´÷Å, separate` ’¢’°’™’°’∂’•’¨, separator’ù ’¢’°’™’°’∂’´’π ’Ø’°’¥ ’°’º’°’∂’±’∂’°÷Å’∂’´’π (’π’£’´’ø’•’¥ ’∏’∂÷Å ’©’°÷Ä’£’¥’°’∂’•’¥ :))\nprint('’´’∂’π-’∏÷Ä ’¢’°’∂', '’´’∂’π-’∏÷Ä ’∏÷Ç÷Ä’´’∑ ’¢’°’∂', sep=\" \")\nprint('’´’∂’π-’∏÷Ä ’¢’°’∂', '’´’∂’π-’∏÷Ä ’∏÷Ç÷Ä’´’∑ ’¢’°’∂', sep='¬Ø\\_(„ÉÑ)_/¬Ø')\n# print('’´’∂’π-’∏÷Ä ’¢’°’∂', sep=\"*\", '’´’∂’π-’∏÷Ä ’∏÷Ç÷Ä’´’∑ ’¢’°’∂')\n\nprint(\"1\", 2, 3, sep=\"*\")\n\n# default -&gt; \" \"\n\n’´’∂’π-’∏÷Ä ’¢’°’∂ ’´’∂’π-’∏÷Ä ’∏÷Ç÷Ä’´’∑ ’¢’°’∂\n’´’∂’π-’∏÷Ä ’¢’°’∂¬Ø\\_(„ÉÑ)_/¬Ø’´’∂’π-’∏÷Ä ’∏÷Ç÷Ä’´’∑ ’¢’°’∂\n1*2*3",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>‘±÷Ä’ø’°’Æ’∏÷Ç’¥ (Print(’ø’∫’•’¨))</span>"
    ]
  },
  {
    "objectID": "python/Python_01_Intro.html#’©’æ’°’¢’°’∂’°’Ø’°’∂-’£’∏÷Ä’Æ’∏’≤’∏÷Ç’©’µ’∏÷Ç’∂’∂’•÷Ä",
    "href": "python/Python_01_Intro.html#’©’æ’°’¢’°’∂’°’Ø’°’∂-’£’∏÷Ä’Æ’∏’≤’∏÷Ç’©’µ’∏÷Ç’∂’∂’•÷Ä",
    "title": "3¬† ‘±÷Ä’ø’°’Æ’∏÷Ç’¥ (Print(’ø’∫’•’¨))",
    "section": "6.1 ‘π’æ’°’¢’°’∂’°’Ø’°’∂ ’£’∏÷Ä’Æ’∏’≤’∏÷Ç’©’µ’∏÷Ç’∂’∂’•÷Ä",
    "text": "6.1 ‘π’æ’°’¢’°’∂’°’Ø’°’∂ ’£’∏÷Ä’Æ’∏’≤’∏÷Ç’©’µ’∏÷Ç’∂’∂’•÷Ä\n\n6.1.1 ‘≥’∏÷Ç’¥’°÷Ä’∏÷Ç’¥, ’∞’°’∂’∏÷Ç’¥, ’¢’°’¶’¥’°’∫’°’ø’Ø’∏÷Ç’¥, ’Ω’∏’æ’∏÷Ä’°’Ø’°’∂ ÷á ’°’¥’¢’∏’≤’©’´’æ ’¢’°’™’°’∂’∏÷Ç’¥, ’°’Ω’ø’´’≥’°’∂ ’¢’°÷Ä’±÷Ä’°÷Å’∂’•’¨, ’¥’∂’°÷Å’∏÷Ä’§\n\nprint(1+2+  53)\nprint(1 + 2 + 53)\nprint(1+2+53)\n\n56\n56\n56\n\n\n\nprint(1 - 4)\nprint(-5)\n\n-3\n-5\n\n\n\na = 1 + 1\nprint(a)\nprint(type(a))\n\nprint(1 + 2.1)\nprint(type(1 + 2.1)) # int + float = float\n\nprint(1 + 2.0) # ’•’©’• ’∏÷Ç’≤’≤’°’Ø’´ . ’§’∂’•’∂÷Ñ ’°÷Ä’§’•’∂ float ’Ø’§’°’º’∂’°\nprint(type(1 + 2.)) # ’•’©’• ’∏÷Ç’≤’≤’°’Ø’´ . ’§’∂’•’∂÷Ñ ’°÷Ä’§’•’∂ float ’Ø’§’°’º’∂’°\n\n\n# a = 3.7 + 1j\n# b = 509 + 4j\n\n# print(a + b)\n# # .9\n# # 1.\n\n2\n&lt;class 'int'&gt;\n3.1\n&lt;class 'float'&gt;\n3.0\n&lt;class 'float'&gt;\n(512.7+5j)\n\n\n\nx = 1.0 + 2.0\n# Video ToDo\nprint(x)\nprint(type(x))\n\n3.0\n&lt;class 'float'&gt;\n\n\n\nprint(5 * 12)\nprint(5 * 12.0)\n\n60\n60.0\n\n\n\n# / - ’¥’´’∑’ø ’ø’°’¨’´’Ω ’° float\nprint(14 / 3)\nprint(14 / 2)\n\n4.666666666666667\n7.0\n\n\n\n print(14 // 3) # int(14/3)\n print(type(14//3))\n\n# 14 = 4 * 3 + 2\n# print(int(14/3))\n\n4\n&lt;class 'int'&gt;\n\n\n\n_print = 3\nbarev = 3\n\n\nprint(-5 // 3) # ’°’¥’¢’∏’≤’ª’°’©’´’æ ’¢’°’™’°’∂’∏÷Ç’¥ -5 = -2 * 3 + 1\n\n# 14 // 3\n# 4.6667\n# -5 / 3 = -1.666\n# -1 * 3 - 2\n# -2 * 3 + 1\n\n-2\n\n\n\nprint(7 % 2) # ’¥’∂’°÷Å’∏÷Ä’§ 7 = 3 * 2 + 1\n\n1\n\n\n\nprint(2**3) # ’ß÷Ñ’Ω’∫’∏’∂’•’∂’ø\nprint(2**(1/2))\n# 2 * 2 * 2\n\n8\n1.4142135623730951\n\n\n\n\n6.1.2 ’£’∏÷Ä’Æ’∏’≤’∏÷Ç’©’µ’∏÷Ç’∂’∂’•÷Ä’´ ’∞’•÷Ä’©’°’Ø’°’∂’∏÷Ç’©’µ’∏÷Ç’∂’®\n\nprint(2 * (3 + 1)) # ’Ω’Ø’¶’¢’∏÷Ç’¥ ÷É’°’Ø’°’£’Æ’´ ’¥’´’ª’´’∂’®, ’∞’•’ø’∏ ’°÷Ä’ø’°’§÷Ä’µ’°’¨’®\n\n8\n\n\n\nprint(2*3 + 1) # ’Ω’Ø’¶’¢’∏÷Ç’¥ ’¢’°’¶’¥’°’∫’°’ø’Ø’∏÷Ç’¥ ’∞’•’ø’∏ ’£’∏÷Ç’¥’°÷Ä’∏÷Ç’¥\n\n7\n\n\n\n\n6.1.3 ‘±’æ’•’¨’´ ’Ø’°÷Ä’≥ ’æ’•÷Ä’°’£÷Ä’•’¨’∏÷Ç ’±÷á\n\ntariq = 18\nprint(tariq)\ntariq = tariq + 1\nprint(tariq)\n\n18\n19\n\n\n\ntariq = 18\n# tariq = tariq  1.5\ntariq += 1.5 # ’∂’∏÷Ç’µ’∂’∂’° ’´’∂’π tariq = tariq + 1.5\n\nprint(tariq)\n\n667.3645546259062\n\n\n\nb = 4\n\nb *= 3 # b = b * 3\nprint(b)\nb //= 3\nprint(b)\n\n12\n4",
    "crumbs": [
      "Python",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>‘±÷Ä’ø’°’Æ’∏÷Ç’¥ (Print(’ø’∫’•’¨))</span>"
    ]
  },
  {
    "objectID": "ml/01_Regression_Main_Concepts/Code/C1_Regression_Main_Concepts.html",
    "href": "ml/01_Regression_Main_Concepts/Code/C1_Regression_Main_Concepts.html",
    "title": "4¬† 1d Data Gen",
    "section": "",
    "text": "5 Kaggle data\nhttps://www.kaggle.com/datasets/iamsouravbanerjee/house-rent-prediction-dataset/data\nBHK: Number of Bedrooms, Hall, Kitchen.\nRent: Rent of the Houses/Apartments/Flats.\nSize: Size of the Houses/Apartments/Flats in Square Feet.\nFloor: Houses/Apartments/Flats situated in which Floor and Total Number of Floors (Example: Ground out of 2, 3 out of 5, etc.)\nArea Type: Size of the Houses/Apartments/Flats calculated on either Super Area or Carpet Area or Build Area.\nArea Locality: Locality of the Houses/Apartments/Flats.\nCity: City where the Houses/Apartments/Flats are Located.\nFurnishing Status: Furnishing Status of the Houses/Apartments/Flats, either it is Furnished or Semi-Furnished or Unfurnished.\nTenant Preferred: Type of Tenant Preferred by the Owner or Agent.\nBathroom: Number of Bathrooms.\nPoint of Contact: Whom should you contact for more information regarding the Houses/Apartments/Flats.\nimport kagglehub\n\n# Download latest version\npath = kagglehub.dataset_download(\"iamsouravbanerjee/house-rent-prediction-dataset\")\n\nprint(\"Path to dataset files:\", path)\nimport os\nimport pandas as pd\n\n# os.listdir(path)\n# df = pd.read_csv(os.path.join(path, \"House_Rent_Dataset.csv\")) # /, \\\ndf = pd.read_csv(os.path.join(\"House_Rent_Dataset.csv\")) # /, \\",
    "crumbs": [
      "ML",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Regression Main Concepts</span>"
    ]
  },
  {
    "objectID": "ml/01_Regression_Main_Concepts/Code/C1_Regression_Main_Concepts.html#one-hot-encoding",
    "href": "ml/01_Regression_Main_Concepts/Code/C1_Regression_Main_Concepts.html#one-hot-encoding",
    "title": "4¬† 1d Data Gen",
    "section": "7.1 One Hot Encoding",
    "text": "7.1 One Hot Encoding\n\ndf.value_counts(\"City\")\n\n\n\n\nimage.png\n\n\npip install scikit-learn or\nconda install scikit-learn\nNOT SKLEARN\n\nfrom sklearn.preprocessing import OneHotEncoder\n\n# https://scikit-learn.org/dev/modules/generated/sklearn.preprocessing.OneHotEncoder.html\n\n\ncity = df[[\"City\"]]\ncity\n\n\nohe = OneHotEncoder(sparse_output=False)\n\nohe.fit(city)\n\n\ncity_transformed = ohe.transform(city)\n\n\ncity_transformed\n\n\npd.DataFrame(city_transformed)\n\n\nohe.get_feature_names_out()\n\n\nencoded_city = pd.DataFrame(city_transformed, columns=ohe.get_feature_names_out())\nencoded_city\n\n\npd.concat([df, encoded_city], axis=1)\n\n\n7.1.1 Problem with OHE\n\nencoded_city\n\n\n\n\nimage.png\n\n\n\nencoded_city.iloc[:, :-1]\n\n\n1 - encoded_city.iloc[:, :-1].sum(axis=1)\n\n\nsum_of_rest = encoded_city.iloc[:, :-1].sum(axis=1)\n\n\nres = 1 - sum_of_rest\n\n\nencoded_city.iloc[:, -1] == res\n\n\nall(encoded_city.iloc[:, -1] == res)\n\n\nimport numpy as np\n\n\nX_X_t = encoded_city @ encoded_city.T\n\n\nnp.linalg.inv(X_X_t)\n\n\nnp.linalg.det(X_X_t)\n\n\n7.1.1.1 Removing duplicates\n\ndf.duplicated()\n\n\nnp.any(df.duplicated())\n\n\ndf = df[~df.duplicated()]\n\n\n\n\n7.1.2 Solution\n\nohe = OneHotEncoder(sparse_output=False, drop=\"first\")\n\nencoded_city_fixed = ohe.fit_transform(city)\n\n\npd.DataFrame(encoded_city_fixed, columns=ohe.get_feature_names_out())\n\n\n\n7.1.3 OHE with Pandas\n\npd.get_dummies(city)\n\n\npd.get_dummies(city, drop_first=True)",
    "crumbs": [
      "ML",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Regression Main Concepts</span>"
    ]
  },
  {
    "objectID": "ml/01_Regression_Main_Concepts/Code/C1_Regression_Main_Concepts.html#labelencoding",
    "href": "ml/01_Regression_Main_Concepts/Code/C1_Regression_Main_Concepts.html#labelencoding",
    "title": "4¬† 1d Data Gen",
    "section": "7.2 LabelEncoding",
    "text": "7.2 LabelEncoding\n\nfurnish = df[\"Furnishing Status\"]\n\n\nfrom sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\n\nfurnish_le = le.fit_transform(furnish)\nfurnish_le\n\n\ndf[\"Furnish_le\"] = furnish_le\n\ndf\n\n\ndf.drop_duplicates(\"Furnishing Status\")\n\n\ndf.value_counts(\"Area Type\")\n\n\narea_type_le = le.fit_transform(df[\"Area Type\"])\n\n\ndf[\"area_type_le\"] = area_type_le\n\n\ndf.drop_duplicates(\"Area Type\")\n\n\n7.2.1 Problem with LE\nSuper Area - Built Area = 2 * Carpet Area",
    "crumbs": [
      "ML",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Regression Main Concepts</span>"
    ]
  },
  {
    "objectID": "ml/01_Regression_Main_Concepts/Code/C1_Regression_Main_Concepts.html#solution---ordinal-encoding",
    "href": "ml/01_Regression_Main_Concepts/Code/C1_Regression_Main_Concepts.html#solution---ordinal-encoding",
    "title": "4¬† 1d Data Gen",
    "section": "7.3 Solution - Ordinal Encoding",
    "text": "7.3 Solution - Ordinal Encoding\n\nmappings = {\n    \"Unfurnished\": 0,\n    \"Semi-Furnished\": 0.5,\n    \"Furnished\": 1\n}\n\ndf[\"Furnishing OE\"] = df[\"Furnishing Status\"].map(mappings)\n\n\ndf",
    "crumbs": [
      "ML",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Regression Main Concepts</span>"
    ]
  },
  {
    "objectID": "ml/01_Regression_Main_Concepts/Code/C1_Regression_Main_Concepts.html#target-encoding-be-carefull",
    "href": "ml/01_Regression_Main_Concepts/Code/C1_Regression_Main_Concepts.html#target-encoding-be-carefull",
    "title": "4¬† 1d Data Gen",
    "section": "7.4 Target Encoding (BE CAREFULL!!!)",
    "text": "7.4 Target Encoding (BE CAREFULL!!!)\nhttps://www.youtube.com/watch?v=589nCGeWG1w\n\ndf[\"Point of Contact\"].value_counts()\n\n\ndf.groupby(\"Point of Contact\")[\"Rent\"].mean()\n\npip install category_encoders https://contrib.scikit-learn.org/category_encoders/\n\n!pip install category_encoders\n\n\n!pip uninstall -y scikit-learn\n!pip install scikit-learn==1.5.2\n\n\n!pip install category_encoders==2.5.0\n\n\nimport category_encoders as ce\n\ncol = \"Point of Contact\"\n\ntarget_enc = ce.TargetEncoder(cols=[col])\n\ntarget_enc.fit(df[col], df['Rent'])\n\ndf['Point_of_Coutact_encoded'] = target_enc.transform(df[col])\n\n\ndf\n\n\ndf.groupby(\"Point of Contact\")[\"Rent\"].mean()",
    "crumbs": [
      "ML",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Regression Main Concepts</span>"
    ]
  },
  {
    "objectID": "ml/01_Regression_Main_Concepts/Code/C1_Regression_Main_Concepts.html#some-preprocessing",
    "href": "ml/01_Regression_Main_Concepts/Code/C1_Regression_Main_Concepts.html#some-preprocessing",
    "title": "4¬† 1d Data Gen",
    "section": "7.5 Some preprocessing",
    "text": "7.5 Some preprocessing\n\ndf[\"Area Type\"].value_counts()\n\n\ndf = df[df[\"Area Type\"] != \"Built Area\"]\n\ndf[\"Area Type\"].value_counts()\n\n\ndf[\"Point of Contact\"].value_counts()\n\n\ndf = df[df[\"Point of Contact\"] != \"Contact Builder\"]",
    "crumbs": [
      "ML",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Regression Main Concepts</span>"
    ]
  },
  {
    "objectID": "ml/01_Regression_Main_Concepts/Code/C1_Regression_Main_Concepts.html#putting-all-together",
    "href": "ml/01_Regression_Main_Concepts/Code/C1_Regression_Main_Concepts.html#putting-all-together",
    "title": "4¬† 1d Data Gen",
    "section": "7.6 Putting all together",
    "text": "7.6 Putting all together\n\ndf.drop(columns=[\"Point_of_Coutact_encoded\"], inplace=True)\n\n\nCOLS_OHE = [\"Area Type\", \"City\", \"Tenant Preferred\", \\\n            \"Point of Contact\"]\n\n\ndata_OHE = pd.get_dummies(df[COLS_OHE], drop_first=True)\n\n\ndf = pd.concat([df, data_OHE], axis=1)\n\n\ndf.drop(columns=COLS_OHE, inplace=True)\n\n\nmappings = {\n    \"Unfurnished\": 0,\n    \"Semi-Furnished\": 0.5,\n    \"Furnished\": 1\n}\ndf[\"Furnishing Status\"] = df[\"Furnishing Status\"].map(mappings)\n\n\ndf",
    "crumbs": [
      "ML",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Regression Main Concepts</span>"
    ]
  },
  {
    "objectID": "ml/01_Regression_Main_Concepts/Code/C1_Regression_Main_Concepts.html#min-max-scaling",
    "href": "ml/01_Regression_Main_Concepts/Code/C1_Regression_Main_Concepts.html#min-max-scaling",
    "title": "4¬† 1d Data Gen",
    "section": "10.1 Min Max Scaling",
    "text": "10.1 Min Max Scaling\n\npx.histogram(df, \"Size\")\n\n\ndef min_max_scale(df, col):\n    return (df[col] - df[col].min()) / (df[col].max() - df[col].min())\n\ndf[\"Size_min_max\"] = min_max_scale(df, \"Size\")\ndf[\"Bathroom_min_max\"] = min_max_scale(df, \"Bathroom\")\n\npx.histogram(df, [\"Size_min_max\", \"Bathroom_min_max\"])\n\n\n\nfrom sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()\n\nscaler.fit(df[['Size', 'Bathroom']])\n\nscaled_data = scaler.transform(df[['Size', 'Bathroom']])\n\ndf[['Size_minmax', 'Bathroom_minmax']] = scaled_data\n\ndf.iloc[:,-4:]",
    "crumbs": [
      "ML",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Regression Main Concepts</span>"
    ]
  },
  {
    "objectID": "ml/01_Regression_Main_Concepts/Code/C1_Regression_Main_Concepts.html#standard-scaling",
    "href": "ml/01_Regression_Main_Concepts/Code/C1_Regression_Main_Concepts.html#standard-scaling",
    "title": "4¬† 1d Data Gen",
    "section": "10.2 Standard Scaling",
    "text": "10.2 Standard Scaling\n\ndef standard_scale(df, col):\n    \"\"\"\n    Standardize a single column to have mean 0 and std dev 1:\n         z = (x - mean) / std\n    \"\"\"\n    return (df[col] - df[col].mean()) / df[col].std()\n\ndf[\"Size_std_manual\"] = standard_scale(df, \"Size\")\ndf[\"Bathroom_std_manual\"] = standard_scale(df, \"Bathroom\")\n\n\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n\nscaler.fit(df[['Size', 'Bathroom']])\n\nscaled_data = scaler.transform(df[['Size', 'Bathroom']])\n\ndf[['Size_standard', 'Bathroom_standard']] = scaled_data\n\ndf.iloc[:, -4:]\n\n\npx.histogram(df, [\"Size_std_manual\", \"Bathroom_std_manual\"])",
    "crumbs": [
      "ML",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Regression Main Concepts</span>"
    ]
  },
  {
    "objectID": "ml/01_Regression_Main_Concepts/Code/C1_Regression_Main_Concepts.html#examples",
    "href": "ml/01_Regression_Main_Concepts/Code/C1_Regression_Main_Concepts.html#examples",
    "title": "4¬† 1d Data Gen",
    "section": "12.1 Examples",
    "text": "12.1 Examples\n\n12.1.1 Overfitting: Astrological Predictions in Ancient Civilizations**\nHistorical Backdrop: From Babylonian times onward, countless astrologers meticulously charted the positions of celestial bodies, connecting them with floods, famines, victories in war, and the births of royals.\nWhy it‚Äôs Overfitting: - Ancient astrologers often looked at every tiny ‚Äúcoincidence‚Äù between a planetary alignment and historical events, building extremely specific rules. (E.g., ‚ÄúWhen Mars is in Taurus and the moon is half full, there will be a great harvest if the newborn prince is left-handed!‚Äù) - These detailed ‚Äúmodels‚Äù fit prior observations too well, often capturing noise and coincidences rather than robust truths.\nMoral of the Story: Squeezing meaning out of every alignment of the stars is like overfitting on random noise in a dataset!\n\n\n12.1.2 Underfitting: ‚ÄúBleeding‚Äù as a Medieval Medical Treatment\nHistorical Backdrop: For centuries, a common medical practice in Europe was bloodletting‚Äîdraining blood to ‚Äúrebalance the humors‚Äù and cure ailments from headaches to fevers.\nWhy it‚Äôs Underfitting: - The medical ‚Äúmodel‚Äù at the time was extremely simplistic: ‚ÄúSomething‚Äôs wrong? Let‚Äôs remove blood.‚Äù - They applied the same one-size-fits-all approach to all sorts of diseases, ignoring the huge variability between different medical conditions (and patients). - Because the underlying theory was so rudimentary (the four humors concept), the ‚Äúmodel‚Äù rarely fit the real complexity of physiology.\nMoral of the Story: When your theory is too general and ignores most of the nuanced details, you‚Äôre underfitting the complexity of reality (and might end up making people worse).\n\n\n12.1.3 Overfitting: Your Uncle‚Äôs Hyper-Specific Sports Superstitions\nEveryday Fun Example: Maybe you have an uncle who insists on wearing the exact same (unwashed) socks during every big game, needs to place the remote exactly 5 inches from the TV, and can only eat ‚Äúlucky peanuts‚Äù if the score is tied.\nWhy it‚Äôs Overfitting: - He‚Äôs discovered a string of coincidences: whenever he did those specific rituals, his team happened to win. - He‚Äôs latched onto every tiny detail‚Äîlike someone building an overly complex machine-learning model that memorizes all the noise in the training data. - The moment ‚Äúnew data‚Äù arrives‚Äîi.e., the team loses despite the lucky peanuts‚Äîhis model is proven to have no real predictive power.\nMoral of the Story: If your ‚Äúmodel‚Äù requires that many hyper-specific conditions to ‚Äúsucceed,‚Äù it‚Äôs probably not robust!",
    "crumbs": [
      "ML",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Regression Main Concepts</span>"
    ]
  },
  {
    "objectID": "ml/01_Regression_Main_Concepts/Code/C1_Regression_Main_Concepts.html#demo",
    "href": "ml/01_Regression_Main_Concepts/Code/C1_Regression_Main_Concepts.html#demo",
    "title": "4¬† 1d Data Gen",
    "section": "12.2 Demo",
    "text": "12.2 Demo\n\nimport numpy as np\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\n\n# ------------------------------------------------------------------------------\n# 1. Generate Synthetic Data (True degree = 3)\n# ------------------------------------------------------------------------------\nnp.random.seed(509)\npoly_degree = 20\n\n# True polynomial function (3rd degree)\ndef true_function(x):\n    # Example: y = 1 + 2x + 3x^2 + 4x^3, plus random noise\n    return 1 + 2*x + 3*x**2 + 4*x**3 + np.random.normal(0, 20, size=x.shape)\n\ndef true_function_without_noise(x):\n    return 1 + 2*x + 3*x**2 + 4*x**3\n\n# Generate data\nN = 100\nX = np.linspace(-3, 3, N).reshape(-1, 1)\ny = true_function(X.ravel())\n\n# Split into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size=0.3, \n                                                    random_state=42)\n\n# ------------------------------------------------------------------------------\n# 2. Fit Polynomial Models of Degree 1 to 10 & Record MSE\n# ------------------------------------------------------------------------------\ndegrees = range(1, poly_degree)\ntrain_mses = []\ntest_mses = []\npolynomial_predictions = {}  # Store predictions for plotting\n\n# A dense grid for plotting model predictions\nX_plot = np.linspace(X.min(), X.max(), 200).reshape(-1, 1)\n\nfor d in degrees:\n    # Create polynomial features\n    poly = PolynomialFeatures(degree=d)\n    X_train_poly = poly.fit_transform(X_train)\n    X_test_poly  = poly.transform(X_test)\n    X_plot_poly  = poly.transform(X_plot)\n    \n    # Fit linear regression on polynomial features\n    model = LinearRegression()\n    model.fit(X_train_poly, y_train)\n    \n    # Predict on train and test\n    y_train_pred = model.predict(X_train_poly)\n    y_test_pred  = model.predict(X_test_poly)\n    \n    # Calculate MSE\n    train_mse = mean_squared_error(y_train, y_train_pred)\n    test_mse  = mean_squared_error(y_test,  y_test_pred)\n    \n    train_mses.append(train_mse)\n    test_mses.append(test_mse)\n    \n    # Store predictions for the plotting slider\n    y_plot_pred = model.predict(X_plot_poly)\n    polynomial_predictions[d] = y_plot_pred\n\n# ------------------------------------------------------------------------------\n# 3. Plot MSE vs. Polynomial Degree (Line Chart)\n# ------------------------------------------------------------------------------\ndf_mse = {\n    'Degree': list(degrees),\n    'Train MSE': train_mses,\n    'Test MSE': test_mses\n}\n\nfig_mse = px.line(\n    df_mse, \n    x='Degree', \n    y=['Train MSE', 'Test MSE'], \n    markers=True,\n    title=\"Train & Test MSE vs. Polynomial Degree\"\n)\nfig_mse.update_layout(\n    xaxis = dict(dtick=1),\n    yaxis_title=\"MSE\"\n)\n\n# ------------------------------------------------------------------------------\n# 4. Interactive Plot: Data + Fitted Polynomials (Slider)\n# ------------------------------------------------------------------------------\n# We'll create a figure with:\n#   - Scatter of the training data\n#   - Scatter of the test data (optional, or we can mark them differently)\n#   - A line that updates for each polynomial degree using frames.\n\n# Base scatter (training data)\nscatter_train = go.Scatter(\n    x=X_train.ravel(), \n    y=y_train, \n    mode='markers',\n    name='Train Data',\n    marker=dict(color='blue', size=6)\n)\n\n# Optionally, scatter for test data\nscatter_test = go.Scatter(\n    x=X_test.ravel(), \n    y=y_test, \n    mode='markers',\n    name='Test Data',\n    marker=dict(color='red', size=6)\n)\n\n# We'll build frames for each polynomial degree\nframes = []\nfor d in degrees:\n    # Create a line trace for the polynomial prediction at degree d\n    line_pred = go.Scatter(\n        x=X_plot.ravel(),\n        y=polynomial_predictions[d],\n        mode='lines',\n        line=dict(width=3),\n        name=f\"Degree {d} fit\"\n    )\n    frames.append(\n        go.Frame(\n            data=[scatter_train, scatter_test, line_pred],\n            name=str(d)\n        )\n    )\n\n# Initial line (degree=1 by default)\ninit_line = go.Scatter(\n    x=X_plot.ravel(),\n    y=polynomial_predictions[1],\n    mode='lines',\n    line=dict(width=3),\n    name=f\"Degree 1 fit\"\n)\n\n# Build the figure with the first frame's data\nfig_poly = go.Figure(\n    data=[scatter_train, scatter_test, init_line],\n    layout=go.Layout(\n        title=\"Polynomial Fits (Degree Slider)\",\n        xaxis=dict(title=\"X\"),\n        yaxis=dict(title=\"y\"),\n        updatemenus=[  # Slider button settings\n            dict(\n                type=\"buttons\",\n                showactive=False,\n                x=1.15,\n                y=1.15,\n                xanchor=\"right\",\n                yanchor=\"top\",\n                buttons=[\n                    dict(label=\"Play\",\n                         method=\"animate\",\n                         args=[None, \n                               dict(frame=dict(duration=500, redraw=True),\n                                    fromcurrent=True,\n                                    transition=dict(duration=300)\n                                   )\n                              ]\n                        )\n                ]\n            )\n        ],\n        # We'll define sliders next\n        sliders=[{\n            'currentvalue': {'prefix': 'Degree: ', 'xanchor': 'right'},\n            'steps': [\n                {\n                    'label': str(d),\n                    'method': 'animate',\n                    'args': [[str(d)], \n                             dict(mode='immediate',\n                                  frame=dict(duration=300, redraw=True),\n                                  transition=dict(duration=300))]\n                }\n                for d in degrees\n            ]\n        }]\n    ),\n    frames=frames\n)\n\n# add the true function to fig_poly\ntrue_y = true_function_without_noise(X_plot.ravel())\nfig_poly.add_trace(go.Scatter(x=X_plot.ravel(), y=true_y, mode='lines', name='True Function'))\n\n\n\n\n# ------------------------------------------------------------------------------\n# 5. Show the plots\n# ------------------------------------------------------------------------------\nfig_poly.show()\nfig_mse.show()\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json",
    "crumbs": [
      "ML",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Regression Main Concepts</span>"
    ]
  },
  {
    "objectID": "ml/01_Regression_Main_Concepts/Code/C1_Regression_Main_Concepts.html#bias-variance-decomposition",
    "href": "ml/01_Regression_Main_Concepts/Code/C1_Regression_Main_Concepts.html#bias-variance-decomposition",
    "title": "4¬† 1d Data Gen",
    "section": "12.3 Bias Variance decomposition",
    "text": "12.3 Bias Variance decomposition\nhttps://scott.fortmann-roe.com/docs/BiasVariance.html\n\n\n\nimage.png\n\n\n\nx_vals",
    "crumbs": [
      "ML",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Regression Main Concepts</span>"
    ]
  },
  {
    "objectID": "ml/01_Regression_Main_Concepts/Code/C1_Regression_Main_Concepts.html#classics",
    "href": "ml/01_Regression_Main_Concepts/Code/C1_Regression_Main_Concepts.html#classics",
    "title": "4¬† 1d Data Gen",
    "section": "14.1 Classics",
    "text": "14.1 Classics\nGo to PDF",
    "crumbs": [
      "ML",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Regression Main Concepts</span>"
    ]
  },
  {
    "objectID": "ml/01_Regression_Main_Concepts/Code/C1_Regression_Main_Concepts.html#pearson-and-spearman-correlation",
    "href": "ml/01_Regression_Main_Concepts/Code/C1_Regression_Main_Concepts.html#pearson-and-spearman-correlation",
    "title": "4¬† 1d Data Gen",
    "section": "14.2 Pearson and Spearman Correlation",
    "text": "14.2 Pearson and Spearman Correlation\n\n### Pearson's vs Spearman's Correlation in Python\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.stats import pearsonr, spearmanr\n\n# Generate sample data\nnp.random.seed(509)\nx = np.linspace(1, 100, 100)\ny_linear = 2 * x + np.random.normal(0, 10, size=len(x))  # Linear relationship\ny_monotonic = np.log(x) + np.random.normal(0, 0.2, size=len(x))  # Monotonic but not linear\ny_outlier = y_linear.copy()\ny_outlier[-1] += 50_900  # Add an outlier\n\n\n# Calculate correlation values\npearson_linear = pearsonr(x, y_linear)[0]\nspearman_linear = spearmanr(x, y_linear)[0]\n\npearson_monotonic = pearsonr(x, y_monotonic)[0]\nspearman_monotonic = spearmanr(x, y_monotonic)[0]\n\npearson_outlier = pearsonr(x, y_outlier)[0]\nspearman_outlier = spearmanr(x, y_outlier)[0]\n\n\nimport plotly.express as px\nimport plotly.graph_objects as go\n\n# Pearson's linear correlation example\nfig1 = px.scatter(x=x, y=y_linear, title=f\"Linear Relationship | Pearson - {pearson_linear:.2f} | Spearman - {spearman_linear:.2f}\")\nfig1.add_trace(go.Scatter(x=x, y=y_linear, mode='markers'))\nfig1.update_layout(xaxis_title=\"X\", yaxis_title=\"Y\")\nfig1.show()\n\n# Spearman's monotonic relationship example\nfig2 = px.scatter(x=x, y=y_monotonic, title=f\"Monotonic Relationship | Pearson - {pearson_monotonic:.2f} | Spearman - {spearman_monotonic:.2f}\")\nfig2.add_trace(go.Scatter(x=x, y=y_monotonic, mode='markers'))\nfig2.update_layout(xaxis_title=\"X\", yaxis_title=\"Y (Rank-based)\")\nfig2.show()\n\n# Pearson with outlier example\nfig3 = px.scatter(x=x, y=y_outlier, title=f\"Outlier | Pearson - {pearson_outlier:.2f} | Spearman - {spearman_outlier:.2f}\")\nfig3.add_trace(go.Scatter(x=x, y=y_outlier, mode='markers'))\nfig3.update_layout(xaxis_title=\"X\", yaxis_title=\"Y\")\nfig3.show()\n\n# Spearman on sine wave\nx = np.linspace(0, 1 * np.pi, 100)\ny = np.sin(x)\n\npearson_sine = pearsonr(x, y)[0]\nspearman_sine = spearmanr(x, y)[0]\n\nfig4 = px.scatter(x=x, y=y, title=f\"Sine Wave | Pearson - {pearson_sine:.2f} | Spearman - {spearman_sine:.2f}\")\nfig4.add_trace(go.Scatter(x=x, y=y, mode='markers'))\nfig4.update_layout(xaxis_title=\"X\", yaxis_title=\"Y\")\nfig4.show()\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json",
    "crumbs": [
      "ML",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Regression Main Concepts</span>"
    ]
  },
  {
    "objectID": "ml/01_Regression_Main_Concepts/Code/C1_Regression_Main_Concepts.html#pearsons-correlation-r",
    "href": "ml/01_Regression_Main_Concepts/Code/C1_Regression_Main_Concepts.html#pearsons-correlation-r",
    "title": "4¬† 1d Data Gen",
    "section": "14.3 Pearson‚Äôs Correlation (r)",
    "text": "14.3 Pearson‚Äôs Correlation (r)\n\nMeasures the linear relationship between two variables.\nAssumes data is normally distributed.\nSensitive to outliers.\nValues range from -1 to +1:\n\n+1: Perfect positive linear correlation.\n-1: Perfect negative linear correlation.\n0: No linear correlation.\n\n\n\n14.3.1 Formula:\n\\[\nr = \\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum (x_i - \\bar{x})^2 \\sum (y_i - \\bar{y})^2}}\n\\]",
    "crumbs": [
      "ML",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Regression Main Concepts</span>"
    ]
  },
  {
    "objectID": "ml/01_Regression_Main_Concepts/Code/C1_Regression_Main_Concepts.html#spearmans-correlation-œÅ",
    "href": "ml/01_Regression_Main_Concepts/Code/C1_Regression_Main_Concepts.html#spearmans-correlation-œÅ",
    "title": "4¬† 1d Data Gen",
    "section": "14.4 Spearman‚Äôs Correlation (œÅ)",
    "text": "14.4 Spearman‚Äôs Correlation (œÅ)\n\nMeasures the monotonic relationship between two variables.\nDoes not assume normality (non-parametric).\nRobust against outliers.\nValues range from -1 to +1:\n\n+1: Perfect positive monotonic relationship.\n-1: Perfect negative monotonic relationship.\n0: No monotonic relationship.\n\n\n\n14.4.1 Formula:\n\\[\n\\rho = 1 - \\frac{6 \\sum d_i^2}{n(n^2 - 1)}\n\\] - \\(d_i\\): Difference between ranks of \\(x_i\\) and \\(y_i\\). - \\(n\\): Number of pairs.",
    "crumbs": [
      "ML",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Regression Main Concepts</span>"
    ]
  },
  {
    "objectID": "ml/01_Regression_Main_Concepts/Code/C1_Regression_Main_Concepts.html#when-to-use",
    "href": "ml/01_Regression_Main_Concepts/Code/C1_Regression_Main_Concepts.html#when-to-use",
    "title": "4¬† 1d Data Gen",
    "section": "14.5 When to Use:",
    "text": "14.5 When to Use:\n\n\n\n\n\n\n\n\nCriteria\nPearson\nSpearman\n\n\n\n\nLinear Relationship\nYes\nNo\n\n\nMonotonic Relationship\nNo\nYes\n\n\nOutlier Sensitivity\nHigh\nLow\n\n\nNormality Assumption\nYes\nNo\n\n\n\n\n‚Äú‚Äú‚Äù",
    "crumbs": [
      "ML",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Regression Main Concepts</span>"
    ]
  },
  {
    "objectID": "ml/01_Regression_Main_Concepts/Code/C1_Regression_Main_Concepts.html#visual-evaluation-codes",
    "href": "ml/01_Regression_Main_Concepts/Code/C1_Regression_Main_Concepts.html#visual-evaluation-codes",
    "title": "4¬† 1d Data Gen",
    "section": "14.6 Visual evaluation + Codes",
    "text": "14.6 Visual evaluation + Codes\n\nfrom sklearn.linear_model import LinearRegression\n\nmodel = LinearRegression()\nmodel.fit(X_train_poly, y_train)\n\n\ny_train_pred = model.predict(X_train_poly)\ny_test_pred = model.predict(X_test_poly)\n\n\n# https://github.com/HaykTarkhanyan/coder_moder/blob/main/ml/regression_evaluation_report.py\ndef evaluate_regression(actual, predictions,\n    model_name=None, filename=None, notes=None,\n    return_metrics=False, show_plots=False,\n    show_metrics=True, plots=False, round_digits=3):\n    \"\"\"\n    Function to evaluate a regression model.\n\n    .. warning::\n        Assumes that ``scipy``, ``sklearn``, and ``matplotlib`` are installed\n        in your environment.\n\n    This function:\n        - Prints R2, MAE, MSE, RMSE metrics.\n        - Prints Kendall's tau, Pearson's R, Spearman's rho correlation metrics.\n        - Plots actual vs. predicted values.\n        - Plots residuals vs. predicted values.\n        - Plots distribution of residuals.\n        - Plots predicted vs. actual distribution.\n        - Saves results to file (if specified).\n        - Returns metrics as a dictionary (if specified).\n    Args:\n        actual (array-like): Ground-truth target values.\n        predictions (array-like): Model predictions.\n        model_name (str, optional): Name of the model (for display/record-keeping).\n        filename (str, optional): Path to an HTML file to save the results.\n        notes (str, optional): Additional notes to include in the saved file (if `filename` is provided).\n        return_metrics (bool, optional): If True, returns a dictionary of metrics. Defaults to False.\n        show_plots (bool, optional): If True, calls `plt.show()` for each figure. Defaults to False.\n        show_metrics (bool, optional): If True, prints the metrics and correlations to stdout. Defaults to True.\n        plots (bool, optional): If True, generates plots. Defaults to False.\n        round_digits (int, optional): Number of digits to round the metrics. Defaults to 3.\n\n    Returns:\n        dict or None: \n            A dictionary of computed metrics if `return_metrics=True`, otherwise None.\n    \"\"\"\n    import numpy as np\n    import matplotlib.pyplot as plt\n    from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n    from scipy.stats import kendalltau, pearsonr, spearmanr\n    from datetime import datetime\n    from io import BytesIO\n    import base64\n    # Ensure inputs are NumPy arrays\n    actual = np.array(actual)\n    predictions = np.array(predictions)\n\n    def save_figure_to_file(fig):\n        \"\"\"\n        Helper function:\n        Convert a Matplotlib figure to a base64-encoded PNG for embedding in HTML.\n        \"\"\"\n        tmpfile = BytesIO()\n        fig.savefig(tmpfile, format='png')\n        encoded = base64.b64encode(tmpfile.getvalue()).decode('utf-8')\n        return encoded\n\n    # 1. Calculate regression metrics\n    r2 = round(r2_score(actual, predictions), round_digits)\n    mae = round(mean_absolute_error(actual, predictions), round_digits)\n    mape = round(mean_absolute_percentage_error(actual, predictions), round_digits)\n    mse = round(mean_squared_error(actual, predictions), round_digits)\n    rmse = round(np.sqrt(mean_squared_error(actual, predictions)), round_digits)\n\n    # 2. Calculate correlation metrics\n    pearson = round(pearsonr(actual, predictions)[0], round_digits)\n    spearman = round(spearmanr(actual, predictions)[0], round_digits)\n    kendall = round(kendalltau(actual, predictions)[0], round_digits)\n\n    # 3. Print metrics if needed\n    if show_metrics:\n        print(f\"Model: {model_name or 'N/A'}\")\n        print(f\"R2: {r2}\")\n        print(f\"MAE: {mae}\")\n        print(f\"MAPE: {mape}\")\n        print(f\"MSE: {mse}\")\n        print(f\"RMSE: {rmse}\")\n        print(f\"Pearson Correlation: {pearson}\")\n        print(f\"Spearman Rho: {spearman}\")\n        print(f\"Kendall Tau: {kendall}\")\n\n    # 4. Generate plots if requested\n    if plots:\n        residuals = actual - predictions\n\n        # (a) Predicted vs. Actual\n        fig1 = plt.figure()\n        plt.scatter(actual, predictions, edgecolor='k', alpha=0.7)\n        plt.xlabel(\"Actual\")\n        plt.ylabel(\"Predicted\")\n        plt.title(\"Predicted vs. Actual\")\n        # add a diagonal line\n        plt.plot([actual.min(), actual.max()], [actual.min(), actual.max()], 'k--', lw=2)\n        if show_plots:\n            plt.show()\n        prediction_vs_actual = save_figure_to_file(fig1)\n        plt.close(fig1)\n\n        # (b) Residuals vs. Predicted\n        fig2 = plt.figure()\n        plt.scatter(predictions, residuals, edgecolor='k', alpha=0.7)\n        plt.axhline(y=0, color='r', linestyle='--')\n        plt.xlabel(\"Predicted\")\n        plt.ylabel(\"Residual\")\n        plt.title(\"Residuals vs. Predicted\")\n        if show_plots:\n            plt.show()\n        residuals_vs_predicted = save_figure_to_file(fig2)\n        plt.close(fig2)\n\n        # (c) Distribution of Residuals\n        fig3 = plt.figure()\n        plt.hist(residuals, bins=30, edgecolor='k', alpha=0.7)\n        plt.xlabel(\"Residual\")\n        plt.ylabel(\"Count\")\n        plt.title(\"Distribution of Residuals\")\n        if show_plots:\n            plt.show()\n        residuals_distribution = save_figure_to_file(fig3)\n        plt.close(fig3)\n\n        # (d) Distribution of Predicted vs. Actual\n        fig4 = plt.figure()\n        plt.hist(actual, bins=30, alpha=0.5, label=\"Actual\", edgecolor='k')\n        plt.hist(predictions, bins=30, alpha=0.5, label=\"Predicted\", edgecolor='k')\n        plt.xlabel(\"Value\")\n        plt.ylabel(\"Count\")\n        plt.title(\"Distribution of Predicted vs. Actual\")\n        plt.legend()\n        if show_plots:\n            plt.show()\n        predicted_vs_actual_distribution = save_figure_to_file(fig4)\n        plt.close(fig4)\n\n    # 5. Save results to file (HTML) if requested\n    if filename:\n        with open(filename, \"w\") as f:\n            f.write(f\"&lt;html&gt;&lt;body&gt;\\n\")\n            f.write(f\"&lt;h2&gt;Report generated: {datetime.now()}&lt;/h2&gt;\\n\")\n            if model_name:\n                f.write(f\"&lt;h2&gt;Model Name: {model_name}&lt;/h2&gt;\\n\")\n\n            if notes:\n                f.write(f\"&lt;h3&gt;Notes:&lt;/h3&gt;\\n&lt;p&gt;{notes}&lt;/p&gt;\\n\")\n\n            f.write(\"&lt;h3&gt;Metrics&lt;/h3&gt;\\n\")\n            f.write(f\"&lt;b&gt;R2:&lt;/b&gt; {r2} &lt;br&gt;\\n\")\n            f.write(f\"&lt;b&gt;MAE:&lt;/b&gt; {mae} &lt;br&gt;\\n\")\n            f.write(f\"&lt;b&gt;MAPE:&lt;/b&gt; {mape} &lt;br&gt;\\n\")\n            f.write(f\"&lt;b&gt;MSE:&lt;/b&gt; {mse} &lt;br&gt;\\n\")\n            f.write(f\"&lt;b&gt;RMSE:&lt;/b&gt; {rmse} &lt;br&gt;\\n\")\n\n            f.write(\"&lt;h3&gt;Correlations&lt;/h3&gt;\\n\")\n            f.write(f\"Pearson: {pearson} &lt;br&gt;\\n\")\n            f.write(f\"Spearman: {spearman} &lt;br&gt;\\n\")\n            f.write(f\"Kendall Tau: {kendall} &lt;br&gt;\\n\")\n\n            if plots:\n                f.write(\"&lt;h3&gt;Plots&lt;/h3&gt;\\n\")\n                f.write(f'&lt;img src=\"data:image/png;base64,{prediction_vs_actual}\"&gt;&lt;br&gt;&lt;br&gt;\\n')\n                f.write(f'&lt;img src=\"data:image/png;base64,{residuals_vs_predicted}\"&gt;&lt;br&gt;&lt;br&gt;\\n')\n                f.write(f'&lt;img src=\"data:image/png;base64,{residuals_distribution}\"&gt;&lt;br&gt;&lt;br&gt;\\n')\n                f.write(f'&lt;img src=\"data:image/png;base64,{predicted_vs_actual_distribution}\"&gt;&lt;br&gt;&lt;br&gt;\\n')\n\n            f.write(\"&lt;/body&gt;&lt;/html&gt;\\n\")\n\n    # 6. Optionally return a dictionary of metrics\n    if return_metrics:\n        return {\n            \"model_name\": model_name,\n            \"notes\": notes,\n            \"r2\": r2,\n            \"mae\": mae,\n            \"mape\": mape,\n            \"mse\": mse,\n            \"rmse\": rmse,\n            \"pearson\": pearson,\n            \"spearman\": spearman,\n            \"kendall\": kendall\n        }\n\n\nevaluate_regression(y_train, y_train_pred, model_name=\"Linear Regression\", filename=\"report.html\", show_plots=True, plots=True)\n\nModel: Linear Regression\nR2: 0.851\nMAE: 14.434\nMAPE: 1.377\nMSE: 352.374\nRMSE: 18.772\nPearson Correlation: 0.923\nSpearman Rho: 0.846\nKendall Tau: 0.683\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nevaluate_regression(y_test, y_test_pred, model_name=\"Linear Regression\", filename=\"report.html\", show_plots=True, plots=True)\n\nModel: Linear Regression\nR2: -1.17\nMAE: 33.684\nMAPE: 1.183\nMSE: 6155.855\nRMSE: 78.459\nPearson Correlation: 0.121\nSpearman Rho: 0.623\nKendall Tau: 0.526",
    "crumbs": [
      "ML",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Regression Main Concepts</span>"
    ]
  },
  {
    "objectID": "ml/01_Regression_Main_Concepts/Code/C1_Hyperparameter_Optimization.html",
    "href": "ml/01_Regression_Main_Concepts/Code/C1_Hyperparameter_Optimization.html",
    "title": "5¬† Hyperparameter tuning",
    "section": "",
    "text": "5.1 Train Val Test split\nRole of Each Split 1. Training Set: Used to fit or train your model parameters (weights, decision tree splits, etc.). 2. Validation Set: Used to tune hyperparameters or compare different models. It provides feedback on how different settings affect performance before touching the test set. 3. Test Set: Used once at the end to get an unbiased estimate of the final chosen model‚Äôs performance.\nThe same way we need a dataset separate from the training set to tune hyperparameters, we need a separate dataset to evaluate the model‚Äôs final performance. This is the test set.",
    "crumbs": [
      "ML",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Hyperparameter tuning</span>"
    ]
  },
  {
    "objectID": "ml/01_Regression_Main_Concepts/Code/C1_Hyperparameter_Optimization.html#train-val-test-split",
    "href": "ml/01_Regression_Main_Concepts/Code/C1_Hyperparameter_Optimization.html#train-val-test-split",
    "title": "5¬† Hyperparameter tuning",
    "section": "",
    "text": "5.1.1 Data Prep\n\nimport os\nimport pandas as pd \nimport plotly.express as px\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import Ridge\nfrom sklearn.metrics import mean_squared_error\n\n\nif \"House_Rent_Dataset.csv\" in os.listdir():\n    path = \"House_Rent_Dataset.csv\"\nelse:\n    import kagglehub\n    path = kagglehub.dataset_download(\"iamsouravbanerjee/house-rent-prediction-dataset\")\n    path = os.path.join(path, \"House_Rent_Dataset.csv\")\n    \ndf = pd.read_csv(path)\n\n\ntarget = \"Rent\"\n\n# Split into train val, test\ntrain_size = 0.7\n\nX = df.drop(columns=[target])\ny = df[target]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    train_size=train_size, random_state=5090)\nX_test, X_val, y_test, y_val = train_test_split(X_test, y_test, \n                                                  test_size=0.5, random_state=5090)\n\nprint(f\"Sizes: Train={len(X_train)}, Val={len(X_val)}, Test={len(X_test)}\")\n\nSizes: Train=3322, Val=712, Test=712\n\n\nNote: It would have been better to do cross-validation here\n\nCOLS_TO_DROP = [\"Posted On\", \"Floor\", \"Area Locality\"]\n\ndef basic_preprocessing(df):\n    df = df.drop(columns=COLS_TO_DROP)\n    df.rename(columns={\"Furnishing Status\": \"Furnish\"}, inplace=True)\n    return df\n\n\nX_train = basic_preprocessing(X_train)\nX_val = basic_preprocessing(X_val)\nX_test = basic_preprocessing(X_test)\n\nX_val\n\n\n\n\n\n\n\n\nBHK\nSize\nArea Type\nCity\nFurnish\nTenant Preferred\nBathroom\nPoint of Contact\n\n\n\n\n284\n2\n900\nSuper Area\nKolkata\nUnfurnished\nBachelors/Family\n1\nContact Owner\n\n\n257\n2\n450\nCarpet Area\nKolkata\nUnfurnished\nBachelors\n1\nContact Owner\n\n\n872\n2\n400\nCarpet Area\nMumbai\nFurnished\nBachelors/Family\n1\nContact Owner\n\n\n3749\n3\n1610\nSuper Area\nChennai\nSemi-Furnished\nFamily\n3\nContact Agent\n\n\n2732\n2\n1000\nSuper Area\nDelhi\nSemi-Furnished\nFamily\n1\nContact Owner\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2357\n2\n850\nSuper Area\nBangalore\nSemi-Furnished\nBachelors/Family\n2\nContact Owner\n\n\n3842\n2\n1000\nCarpet Area\nChennai\nUnfurnished\nBachelors\n2\nContact Owner\n\n\n1799\n1\n500\nSuper Area\nBangalore\nUnfurnished\nBachelors/Family\n1\nContact Owner\n\n\n3931\n2\n1800\nSuper Area\nHyderabad\nFurnished\nBachelors/Family\n2\nContact Owner\n\n\n1137\n1\n380\nCarpet Area\nMumbai\nUnfurnished\nBachelors/Family\n2\nContact Agent\n\n\n\n\n712 rows √ó 8 columns\n\n\n\n\nCOLS_OHE = [\"City\"]\nCOLS_ORDINAL = [\"Furnish\"]\nCOLS_STANDARD_SCALE = [\"BHK\", \"Bathroom\", \"Size\"]\n\n\nfrom sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler, \\\n    PolynomialFeatures\n\nohe = OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\", drop=\"first\")\nordinal = OrdinalEncoder(categories=[[\"Unfurnished\", \"Semi-Furnished\", \"Furnished\"]])\nstandard = StandardScaler()\npoly = PolynomialFeatures(degree=3)\n\nFIT ONLY ON TRAIN, OTHERWISE DATA LEAKAGE\n\nohe.fit(X_train[COLS_OHE])\nordinal.fit(X_train[COLS_ORDINAL])\npoly.fit(X_train[COLS_STANDARD_SCALE])\n\nX_train_poly = poly.transform(X_train[COLS_STANDARD_SCALE])\n\nstandard.fit(X_train_poly)\n\nStandardScaler()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.¬†¬†StandardScaler?Documentation for StandardScaleriFittedStandardScaler() \n\n\n\ndef preprocess_data(X, ohe, ordinal, standard):\n    X_ohe = ohe.transform(X[COLS_OHE])\n    X_ordinal = ordinal.transform(X[COLS_ORDINAL])\n    x_poly = poly.transform(X[COLS_STANDARD_SCALE])\n    X_poly_standard = standard.transform(x_poly)\n\n    X_poly_standard = pd.DataFrame(X_poly_standard, columns=poly.get_feature_names_out())\n\n    X_ohe = pd.DataFrame(X_ohe, columns=ohe.get_feature_names_out(COLS_OHE))\n    X_ordinal = pd.DataFrame(X_ordinal, columns=COLS_ORDINAL)\n    X_poly_standard = pd.DataFrame(X_poly_standard, columns=poly.get_feature_names_out(COLS_STANDARD_SCALE))\n    \n    X = pd.concat([X_poly_standard, X_ohe, X_ordinal], axis=1)\n    return X\n\n\nX_train = preprocess_data(X_train, ohe, ordinal, standard)\nX_val = preprocess_data(X_val, ohe, ordinal, standard)\nX_test = preprocess_data(X_test, ohe, ordinal, standard)\n\nX_train\n\n\n\n\n\n\n\n\n1\nBHK\nBathroom\nSize\nBHK^2\nBHK Bathroom\nBHK Size\nBathroom^2\nBathroom Size\nSize^2\n...\nBathroom^3\nBathroom^2 Size\nBathroom Size^2\nSize^3\nCity_Chennai\nCity_Delhi\nCity_Hyderabad\nCity_Kolkata\nCity_Mumbai\nFurnish\n\n\n\n\n0\n0.0\n1.110492\n1.189251\n0.829344\n0.999475\n1.114653\n0.806053\n0.928818\n0.707418\n0.344902\n...\n0.506051\n0.356248\n0.175618\n0.041691\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n\n\n1\n0.0\n-0.097846\n-1.091749\n-0.563145\n-0.254682\n-0.677640\n-0.450178\n-0.759804\n-0.549198\n-0.376971\n...\n-0.427136\n-0.317066\n-0.237390\n-0.183322\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n2\n0.0\n-0.097846\n0.048751\n-0.408424\n-0.254682\n-0.165556\n-0.374043\n-0.126570\n-0.291431\n-0.327319\n...\n-0.175894\n-0.202237\n-0.197317\n-0.174276\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n3\n0.0\n-0.097846\n0.048751\n0.133100\n-0.254682\n-0.165556\n-0.107570\n-0.126570\n-0.065884\n-0.093378\n...\n-0.175894\n-0.129164\n-0.118141\n-0.116251\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n\n\n4\n0.0\n-0.097846\n-1.091749\n-0.021621\n-0.254682\n-0.677640\n-0.183705\n-0.759804\n-0.436425\n-0.169767\n...\n-0.427136\n-0.298798\n-0.202326\n-0.137638\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n3317\n0.0\n-0.097846\n0.048751\n0.055739\n-0.254682\n-0.165556\n-0.145637\n-0.126570\n-0.098105\n-0.132528\n...\n-0.175894\n-0.139603\n-0.131391\n-0.127479\n0.0\n0.0\n1.0\n0.0\n0.0\n1.0\n\n\n3318\n0.0\n-1.306183\n-1.091749\n-0.795227\n-1.007177\n-0.933682\n-0.735686\n-0.759804\n-0.597530\n-0.437127\n...\n-0.427136\n-0.324895\n-0.247570\n-0.192217\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n\n\n3319\n0.0\n-0.097846\n0.048751\n0.055739\n-0.254682\n-0.165556\n-0.145637\n-0.126570\n-0.098105\n-0.132528\n...\n-0.175894\n-0.139603\n-0.131391\n-0.127479\n0.0\n1.0\n0.0\n0.0\n0.0\n1.0\n\n\n3320\n0.0\n-1.306183\n0.048751\n-0.717866\n-1.007177\n-0.677640\n-0.716652\n-0.126570\n-0.420314\n-0.418985\n...\n-0.175894\n-0.243993\n-0.228341\n-0.189804\n0.0\n0.0\n0.0\n0.0\n1.0\n2.0\n\n\n3321\n0.0\n1.110492\n1.189251\n1.602950\n0.999475\n1.114653\n1.377068\n0.928818\n1.190732\n1.013303\n...\n0.506051\n0.591125\n0.514943\n0.371127\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n\n\n3322 rows √ó 26 columns",
    "crumbs": [
      "ML",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Hyperparameter tuning</span>"
    ]
  },
  {
    "objectID": "ml/01_Regression_Main_Concepts/Code/C1_Hyperparameter_Optimization.html#hyperparameter-tuning-with-random-search-and-grid-search",
    "href": "ml/01_Regression_Main_Concepts/Code/C1_Hyperparameter_Optimization.html#hyperparameter-tuning-with-random-search-and-grid-search",
    "title": "5¬† Hyperparameter tuning",
    "section": "5.2 Hyperparameter Tuning with Random Search and Grid Search",
    "text": "5.2 Hyperparameter Tuning with Random Search and Grid Search\n\nimport numpy as np\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom sklearn.linear_model import Lasso\n\n# ConvergenceWarning\n# ignore the warning\n# from warnings import simplefilter\n# simplefilter(\"ignore\")\n\nNotes: - confusing terminology, the regularization parameter is called alpha in sklearn - lambda (alpha) is usually searched on a log scale\n\nalphas = np.logspace(1, 3, 20)\nprint(alphas)\n\n[  10.           12.74274986   16.23776739   20.69138081   26.36650899\n   33.59818286   42.81332399   54.55594781   69.51927962   88.58667904\n  112.88378917  143.84498883  183.29807108  233.57214691  297.63514416\n  379.26901907  483.29302386  615.84821107  784.75997035 1000.        ]\n\n\n\n10 ** -1\n\n0.1\n\n\n\n5.2.1 Grid Search\n\n# extend the grid \nparam_grid = {\n    \"alpha\": [10 ** i for i in range(-3, 4)],\n    'max_iter': [10, 100, 50_000]\n}\n\n# get Decartian product of all the hyperparameters\nparam_grid\n\n{'alpha': [0.001, 0.01, 0.1, 1, 10, 100, 1000], 'max_iter': [10, 100, 50000]}\n\n\n\n\nres_list = []\nfor lamb in param_grid['alpha']:\n    for max_iter in param_grid['max_iter']:\n        model = Lasso(alpha=lamb, max_iter=max_iter)\n        model.fit(X_train, y_train)\n        \n        y_pred_train = model.predict(X_train)\n        y_pred_val = model.predict(X_val)\n        \n        # r2\n        r2_train = model.score(X_train, y_train)\n        r2_val = model.score(X_val, y_val)\n        \n        res = {\n            \"lambda\": lamb,\n            \"max_iter\": max_iter,\n            \"r2_train\": r2_train,\n            \"r2_val\": r2_val\n        }\n        \n        print(res)\n        res_list.append(res)\n        \n\n{'lambda': 0.001, 'max_iter': 10, 'r2_train': 0.286300900903835, 'r2_val': 0.5469368390683846}\n{'lambda': 0.001, 'max_iter': 100, 'r2_train': 0.2957594907787815, 'r2_val': 0.5626080457488305}\n{'lambda': 0.001, 'max_iter': 50000, 'r2_train': 0.30240718968076064, 'r2_val': 0.5734121131426642}\n{'lambda': 0.01, 'max_iter': 10, 'r2_train': 0.28630087985195296, 'r2_val': 0.5469369298770144}\n{'lambda': 0.01, 'max_iter': 100, 'r2_train': 0.2957593913671396, 'r2_val': 0.5626080842528678}\n{'lambda': 0.01, 'max_iter': 50000, 'r2_train': 0.3024071886295646, 'r2_val': 0.5734129905395922}\n{'lambda': 0.1, 'max_iter': 10, 'r2_train': 0.28630066925527053, 'r2_val': 0.546937837867848}\n{'lambda': 0.1, 'max_iter': 100, 'r2_train': 0.29575839666805526, 'r2_val': 0.5626084686353577}\n{'lambda': 0.1, 'max_iter': 50000, 'r2_train': 0.3024070457697213, 'r2_val': 0.5734215832461952}\n{'lambda': 1, 'max_iter': 10, 'r2_train': 0.2862985555024221, 'r2_val': 0.546946908229855}\n{'lambda': 1, 'max_iter': 100, 'r2_train': 0.29574839141060527, 'r2_val': 0.5626122466719441}\n{'lambda': 1, 'max_iter': 50000, 'r2_train': 0.3023958904596029, 'r2_val': 0.5734382202362194}\n{'lambda': 10, 'max_iter': 10, 'r2_train': 0.2862759984679407, 'r2_val': 0.5470263966801132}\n{'lambda': 10, 'max_iter': 100, 'r2_train': 0.2956438812251351, 'r2_val': 0.5626463119101777}\n{'lambda': 10, 'max_iter': 50000, 'r2_train': 0.3019441918919157, 'r2_val': 0.5716460477070587}\n{'lambda': 100, 'max_iter': 10, 'r2_train': 0.2859912802103456, 'r2_val': 0.5478256953197767}\n{'lambda': 100, 'max_iter': 100, 'r2_train': 0.29462512075138036, 'r2_val': 0.5636114381784972}\n{'lambda': 100, 'max_iter': 50000, 'r2_train': 0.29711504164066593, 'r2_val': 0.5664434496196209}\n{'lambda': 1000, 'max_iter': 10, 'r2_train': 0.2822369767509024, 'r2_val': 0.5519400187789713}\n{'lambda': 1000, 'max_iter': 100, 'r2_train': 0.2826332175940921, 'r2_val': 0.555507368159095}\n{'lambda': 1000, 'max_iter': 50000, 'r2_train': 0.28265746301940353, 'r2_val': 0.5563976587973434}\n\n\n\nres_df = pd.DataFrame(res_list)\nres_df\n\n\n\n\n\n\n\n\nlambda\nmax_iter\nr2_train\nr2_val\n\n\n\n\n0\n0.001\n10\n0.286301\n0.546937\n\n\n1\n0.001\n100\n0.295759\n0.562608\n\n\n2\n0.001\n50000\n0.302407\n0.573412\n\n\n3\n0.010\n10\n0.286301\n0.546937\n\n\n4\n0.010\n100\n0.295759\n0.562608\n\n\n5\n0.010\n50000\n0.302407\n0.573413\n\n\n6\n0.100\n10\n0.286301\n0.546938\n\n\n7\n0.100\n100\n0.295758\n0.562608\n\n\n8\n0.100\n50000\n0.302407\n0.573422\n\n\n9\n1.000\n10\n0.286299\n0.546947\n\n\n10\n1.000\n100\n0.295748\n0.562612\n\n\n11\n1.000\n50000\n0.302396\n0.573438\n\n\n12\n10.000\n10\n0.286276\n0.547026\n\n\n13\n10.000\n100\n0.295644\n0.562646\n\n\n14\n10.000\n50000\n0.301944\n0.571646\n\n\n15\n100.000\n10\n0.285991\n0.547826\n\n\n16\n100.000\n100\n0.294625\n0.563611\n\n\n17\n100.000\n50000\n0.297115\n0.566443\n\n\n18\n1000.000\n10\n0.282237\n0.551940\n\n\n19\n1000.000\n100\n0.282633\n0.555507\n\n\n20\n1000.000\n50000\n0.282657\n0.556398\n\n\n\n\n\n\n\n\npx.scatter(res_df, x=\"lambda\", y=\"max_iter\", log_x=True, log_y=True, color=\"r2_val\", title=\"Grid Search\")\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json",
    "crumbs": [
      "ML",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Hyperparameter tuning</span>"
    ]
  },
  {
    "objectID": "ml/01_Regression_Main_Concepts/Code/C1_Hyperparameter_Optimization.html#grid-search-with-sklearn",
    "href": "ml/01_Regression_Main_Concepts/Code/C1_Hyperparameter_Optimization.html#grid-search-with-sklearn",
    "title": "5¬† Hyperparameter tuning",
    "section": "5.3 Grid Search with sklearn",
    "text": "5.3 Grid Search with sklearn\n\nparam_grid = {\n    \"alpha\": [10 ** i for i in range(-3, 2)],\n    'max_iter': [10, 100, 50_000], \n    'tol': [1e-4,  1e-2],\n    \"fit_intercept\": [True, False]  \n}\n\n\ngrid_search = GridSearchCV(\n    estimator=Lasso(),\n    param_grid=param_grid,\n    scoring='r2',       # Optimize R^2\n    cv=3,               # 5-fold cross-validation\n    n_jobs=-1,          # Use all CPU cores\n    verbose=True        # Print more information  \n)\n\nX_use = pd.concat([X_train, X_val]).head(1_000)\ny_use = pd.concat([y_train, y_val]).head(1_000)\n\ngrid_search.fit(X_use, y_use)\n\nFitting 3 folds for each of 60 candidates, totalling 180 fits\n\n\n\n---------------------------------------------------------------------------\nKeyboardInterrupt                         Traceback (most recent call last)\nInput In [22], in &lt;cell line: 13&gt;()\n     10 X_use = pd.concat([X_train, X_val]).head(1_000)\n     11 y_use = pd.concat([y_train, y_val]).head(1_000)\n---&gt; 13 grid_search.fit(X_use, y_use)\n\nFile c:\\Users\\hayk_\\.conda\\envs\\thesis\\lib\\site-packages\\sklearn\\base.py:1473, in _fit_context.&lt;locals&gt;.decorator.&lt;locals&gt;.wrapper(estimator, *args, **kwargs)\n   1466     estimator._validate_params()\n   1468 with config_context(\n   1469     skip_parameter_validation=(\n   1470         prefer_skip_nested_validation or global_skip_validation\n   1471     )\n   1472 ):\n-&gt; 1473     return fit_method(estimator, *args, **kwargs)\n\nFile c:\\Users\\hayk_\\.conda\\envs\\thesis\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1019, in BaseSearchCV.fit(self, X, y, **params)\n   1013     results = self._format_results(\n   1014         all_candidate_params, n_splits, all_out, all_more_results\n   1015     )\n   1017     return results\n-&gt; 1019 self._run_search(evaluate_candidates)\n   1021 # multimetric is determined here because in the case of a callable\n   1022 # self.scoring the return type is only known after calling\n   1023 first_test_score = all_out[0][\"test_scores\"]\n\nFile c:\\Users\\hayk_\\.conda\\envs\\thesis\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1573, in GridSearchCV._run_search(self, evaluate_candidates)\n   1571 def _run_search(self, evaluate_candidates):\n   1572     \"\"\"Search all candidates in param_grid\"\"\"\n-&gt; 1573     evaluate_candidates(ParameterGrid(self.param_grid))\n\nFile c:\\Users\\hayk_\\.conda\\envs\\thesis\\lib\\site-packages\\sklearn\\model_selection\\_search.py:965, in BaseSearchCV.fit.&lt;locals&gt;.evaluate_candidates(candidate_params, cv, more_results)\n    957 if self.verbose &gt; 0:\n    958     print(\n    959         \"Fitting {0} folds for each of {1} candidates,\"\n    960         \" totalling {2} fits\".format(\n    961             n_splits, n_candidates, n_candidates * n_splits\n    962         )\n    963     )\n--&gt; 965 out = parallel(\n    966     delayed(_fit_and_score)(\n    967         clone(base_estimator),\n    968         X,\n    969         y,\n    970         train=train,\n    971         test=test,\n    972         parameters=parameters,\n    973         split_progress=(split_idx, n_splits),\n    974         candidate_progress=(cand_idx, n_candidates),\n    975         **fit_and_score_kwargs,\n    976     )\n    977     for (cand_idx, parameters), (split_idx, (train, test)) in product(\n    978         enumerate(candidate_params),\n    979         enumerate(cv.split(X, y, **routed_params.splitter.split)),\n    980     )\n    981 )\n    983 if len(out) &lt; 1:\n    984     raise ValueError(\n    985         \"No fits were performed. \"\n    986         \"Was the CV iterator empty? \"\n    987         \"Were there no candidates?\"\n    988     )\n\nFile c:\\Users\\hayk_\\.conda\\envs\\thesis\\lib\\site-packages\\sklearn\\utils\\parallel.py:74, in Parallel.__call__(self, iterable)\n     69 config = get_config()\n     70 iterable_with_config = (\n     71     (_with_config(delayed_func, config), args, kwargs)\n     72     for delayed_func, args, kwargs in iterable\n     73 )\n---&gt; 74 return super().__call__(iterable_with_config)\n\nFile c:\\Users\\hayk_\\.conda\\envs\\thesis\\lib\\site-packages\\joblib\\parallel.py:2007, in Parallel.__call__(self, iterable)\n   2001 # The first item from the output is blank, but it makes the interpreter\n   2002 # progress until it enters the Try/Except block of the generator and\n   2003 # reach the first `yield` statement. This starts the aynchronous\n   2004 # dispatch of the tasks to the workers.\n   2005 next(output)\n-&gt; 2007 return output if self.return_generator else list(output)\n\nFile c:\\Users\\hayk_\\.conda\\envs\\thesis\\lib\\site-packages\\joblib\\parallel.py:1650, in Parallel._get_outputs(self, iterator, pre_dispatch)\n   1647     yield\n   1649     with self._backend.retrieval_context():\n-&gt; 1650         yield from self._retrieve()\n   1652 except GeneratorExit:\n   1653     # The generator has been garbage collected before being fully\n   1654     # consumed. This aborts the remaining tasks if possible and warn\n   1655     # the user if necessary.\n   1656     self._exception = True\n\nFile c:\\Users\\hayk_\\.conda\\envs\\thesis\\lib\\site-packages\\joblib\\parallel.py:1762, in Parallel._retrieve(self)\n   1757 # If the next job is not ready for retrieval yet, we just wait for\n   1758 # async callbacks to progress.\n   1759 if ((len(self._jobs) == 0) or\n   1760     (self._jobs[0].get_status(\n   1761         timeout=self.timeout) == TASK_PENDING)):\n-&gt; 1762     time.sleep(0.01)\n   1763     continue\n   1765 # We need to be careful: the job list can be filling up as\n   1766 # we empty it and Python list are not thread-safe by\n   1767 # default hence the use of the lock\n\nKeyboardInterrupt: \n\n\n\n\nprint(\"\\n== Grid Search Results ==\")\nprint(\"Best parameters found:\", grid_search.best_params_)\nprint(\"Best cross-validation R^2:\", grid_search.best_score_)\n\n\n== Grid Search Results ==\nBest parameters found: {'alpha': 10, 'fit_intercept': True, 'max_iter': 50000, 'tol': 0.01}\nBest cross-validation R^2: 0.5424913981816509\n\n\n\nfrom sklearn.metrics import r2_score, root_mean_squared_error\n\nbest_lasso = grid_search.best_estimator_\n\ny_train_pred = best_lasso.predict(X_train)\ny_val_pred = best_lasso.predict(X_val)\ny_test_pred = best_lasso.predict(X_test)\n\ndef evaluate_model(y_true, y_pred):\n    mse = root_mean_squared_error(y_true, y_pred)\n    r2 = r2_score(y_true, y_pred)\n    return mse, r2\n\nmse_train, r2_train = evaluate_model(y_train, y_train_pred)\nmse_val, r2_val = evaluate_model(y_val, y_val_pred)\nmse_test, r2_test = evaluate_model(y_test, y_test_pred)\n\nprint(f\"Train: RMSE={mse_train:.2f}, R^2={r2_train:.2f}\")\nprint(f\"Val: RMSE={mse_val:.2f}, R^2={r2_val:.2f}\")\nprint(f\"Test: RMSE={mse_test:.2f}, R^2={r2_test:.2f}\")\n\nTrain: RMSE=83108.20, R^2=0.02\nVal: RMSE=111435.42, R^2=-0.12\nTest: RMSE=37364.36, R^2=0.54",
    "crumbs": [
      "ML",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Hyperparameter tuning</span>"
    ]
  },
  {
    "objectID": "ml/01_Regression_Main_Concepts/Code/C1_Hyperparameter_Optimization.html#random-search",
    "href": "ml/01_Regression_Main_Concepts/Code/C1_Hyperparameter_Optimization.html#random-search",
    "title": "5¬† Hyperparameter tuning",
    "section": "5.4 Random Search",
    "text": "5.4 Random Search\n\nfrom scipy.stats import loguniform, randint\n\n\nparam_dist = {\n    'alpha': loguniform(1e-3, 1e4),  # alpha in [1e-3, 1e4]\n    'max_iter': randint(10, 5000)    # integer in [10, 5000)\n}\n\nprint(\"Random Search will sample from these hyperparameter distributions:\")\nfor p, dist in param_dist.items():\n    print(f\"  {p}: {dist}\")\n\nRandom Search will sample from these hyperparameter distributions:\n  alpha: &lt;scipy.stats._distn_infrastructure.rv_continuous_frozen object at 0x00000206FCD59C90&gt;\n  max_iter: &lt;scipy.stats._distn_infrastructure.rv_discrete_frozen object at 0x00000206A376B5E0&gt;\n\n\n\nrandom_search = RandomizedSearchCV(\n    estimator=Lasso(),\n    param_distributions=param_dist,\n    n_iter=100,         # try 100 combinations of hyperparameters\n    scoring='r2',       # optimize R^2\n    cv=3,               # 3-fold cross-validation\n    n_jobs=-1,          # use all CPU cores\n    verbose=1,          # get more information\n    random_state=509   # random seed for reproducibility\n)\n\n\nX_use = pd.concat([X_train, X_val]).head(1_000)\ny_use = pd.concat([y_train, y_val]).head(1_000)\n\n\nrandom_search.fit(X_use, y_use)\n\nFitting 3 folds for each of 100 candidates, totalling 300 fits\n\n\nRandomizedSearchCV(cv=3, estimator=Lasso(), n_iter=100, n_jobs=-1,\n                   param_distributions={'alpha': &lt;scipy.stats._distn_infrastructure.rv_continuous_frozen object at 0x0000022C10AFCD90&gt;,\n                                        'max_iter': &lt;scipy.stats._distn_infrastructure.rv_discrete_frozen object at 0x0000022C10AFC2B0&gt;},\n                   random_state=509, scoring='r2', verbose=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.¬†¬†RandomizedSearchCV?Documentation for RandomizedSearchCViFittedRandomizedSearchCV(cv=3, estimator=Lasso(), n_iter=100, n_jobs=-1,\n                   param_distributions={'alpha': &lt;scipy.stats._distn_infrastructure.rv_continuous_frozen object at 0x0000022C10AFCD90&gt;,\n                                        'max_iter': &lt;scipy.stats._distn_infrastructure.rv_discrete_frozen object at 0x0000022C10AFC2B0&gt;},\n                   random_state=509, scoring='r2', verbose=1) best_estimator_: LassoLasso(alpha=11.87976242665855, max_iter=4585) ¬†Lasso?Documentation for LassoLasso(alpha=11.87976242665855, max_iter=4585) \n\n\n\n\nprint(\"\\n=== RandomizedSearchCV Results ===\")\nprint(\"Best params:\", random_search.best_params_)\nprint(\"Best CV R^2:\", random_search.best_score_)\n\n\n=== RandomizedSearchCV Results ===\nBest params: {'alpha': 11.87976242665855, 'max_iter': 4585}\nBest CV R^2: 0.5420305853566706\n\n\n\nrandom_search.cv_results_\n\n{'mean_fit_time': array([0.00737635, 0.01639032, 0.01452788, 0.00501291, 0.04865607,\n        0.01253215, 0.05014253, 0.00518378, 0.08894293, 0.02473195,\n        0.05683327, 0.07208387, 0.00651677, 0.01185878, 0.00600743,\n        0.03046282, 0.09061797, 0.00571314, 0.11936307, 0.05818748,\n        0.06519278, 0.00818102, 0.01420371, 0.02640192, 0.02172613,\n        0.10365264, 0.01105404, 0.00752838, 0.04767585, 0.0122145 ,\n        0.04615887, 0.02392403, 0.06422218, 0.00534638, 0.00819031,\n        0.02107159, 0.00620969, 0.02257133, 0.02215878, 0.0058589 ,\n        0.03905479, 0.02523359, 0.00855168, 0.06150262, 0.00893354,\n        0.04353158, 0.04662975, 0.04958463, 0.03075496, 0.04376769,\n        0.00590595, 0.06301188, 0.03838595, 0.00551462, 0.00738827,\n        0.03568268, 0.06727751, 0.06562376, 0.06309589, 0.01212605,\n        0.0331049 , 0.00838892, 0.01657891, 0.01964808, 0.05005956,\n        0.06235242, 0.02657437, 0.00701213, 0.0794069 , 0.00569105,\n        0.00764918, 0.06112719, 0.03223697, 0.01855874, 0.00620063,\n        0.03276849, 0.06079261, 0.03860291, 0.02122442, 0.01420856,\n        0.03156503, 0.07353059, 0.05423363, 0.02161821, 0.04045947,\n        0.03861626, 0.01353343, 0.02875113, 0.01235851, 0.03482517,\n        0.00718737, 0.03590711, 0.00666952, 0.03047832, 0.06564569,\n        0.01893965, 0.00896454, 0.07621082, 0.06790129, 0.03906608]),\n 'std_fit_time': array([1.67958452e-03, 8.73724982e-03, 1.63978843e-03, 4.05613952e-04,\n        2.21663559e-02, 2.84767037e-03, 7.39208222e-03, 8.41094755e-04,\n        1.10046686e-02, 7.44232506e-03, 5.85322258e-03, 8.26613458e-03,\n        1.48905132e-03, 4.69303474e-04, 8.12185572e-04, 6.96491254e-03,\n        2.99161743e-02, 6.60576794e-04, 3.23299963e-02, 1.13489552e-02,\n        1.94861430e-02, 1.65986809e-03, 2.02777760e-03, 1.64585967e-02,\n        1.62047012e-02, 2.78802631e-02, 7.08299118e-03, 2.15770102e-03,\n        5.81393877e-03, 4.11876840e-03, 5.58135156e-03, 2.48585881e-03,\n        8.28960495e-03, 2.32252032e-04, 3.77624596e-03, 3.21952392e-05,\n        1.73483992e-03, 5.36638990e-03, 6.79017369e-03, 4.64458281e-04,\n        3.33697065e-04, 2.64668318e-03, 3.59085691e-03, 5.72440339e-03,\n        1.24292022e-03, 1.68863401e-03, 3.51517577e-03, 2.70577632e-03,\n        2.25786544e-03, 1.34450798e-02, 5.41650784e-04, 3.08020392e-03,\n        2.68891909e-02, 8.17642187e-04, 2.31822920e-03, 5.23972233e-03,\n        3.12567154e-03, 1.41872583e-03, 3.27083835e-03, 6.12090416e-03,\n        1.44103654e-03, 2.29257339e-03, 5.05138115e-03, 7.17119131e-03,\n        7.00255602e-03, 1.31615073e-03, 7.11508852e-03, 2.05102492e-03,\n        8.28487641e-03, 2.46682246e-04, 3.07663336e-03, 8.17167660e-04,\n        2.16356222e-02, 1.86209100e-03, 4.78287769e-04, 2.73417813e-03,\n        1.65906831e-03, 1.49385865e-03, 9.11587829e-03, 7.32717450e-03,\n        2.16568832e-03, 7.42729709e-03, 6.29911919e-03, 3.50143653e-03,\n        2.07802997e-02, 2.15676420e-03, 6.05368415e-03, 7.15206657e-03,\n        1.18764201e-03, 2.40109224e-03, 2.39985707e-03, 1.52378124e-03,\n        1.24834843e-03, 9.95697505e-03, 1.04496721e-02, 2.19201311e-03,\n        7.91854996e-04, 7.63317247e-03, 1.58837620e-03, 7.95502729e-03]),\n 'mean_score_time': array([0.00385745, 0.00419386, 0.0058674 , 0.00251285, 0.00518211,\n        0.00318352, 0.00317963, 0.00468183, 0.005011  , 0.00402204,\n        0.0051918 , 0.00366863, 0.00285157, 0.00301218, 0.00367649,\n        0.00450945, 0.00434534, 0.00417097, 0.00299939, 0.00535496,\n        0.00301282, 0.00435313, 0.0158724 , 0.00350563, 0.00334223,\n        0.00517472, 0.00834982, 0.00334899, 0.00416454, 0.00300447,\n        0.00366783, 0.00384339, 0.00332562, 0.00400408, 0.00284489,\n        0.00451851, 0.00333802, 0.00424115, 0.00385801, 0.00333444,\n        0.00390561, 0.00467396, 0.00321468, 0.00333842, 0.00450985,\n        0.00451104, 0.00455006, 0.00300646, 0.00367602, 0.00411669,\n        0.004028  , 0.00417972, 0.00535909, 0.00334024, 0.0028193 ,\n        0.00351357, 0.00300209, 0.00385682, 0.00401982, 0.00451159,\n        0.00970014, 0.00394758, 0.00518258, 0.00334064, 0.00335344,\n        0.00350539, 0.00467531, 0.0066251 , 0.00617671, 0.00334104,\n        0.00286595, 0.00334183, 0.0035398 , 0.00720326, 0.00400281,\n        0.00501704, 0.0048426 , 0.00384816, 0.00518163, 0.00366743,\n        0.00471203, 0.00467825, 0.0051775 , 0.00435758, 0.00449316,\n        0.0041786 , 0.00300821, 0.0036788 , 0.00486533, 0.00451406,\n        0.00567921, 0.00416509, 0.00518664, 0.0063533 , 0.0036792 ,\n        0.0031089 , 0.00317566, 0.00473134, 0.00438126, 0.00410954]),\n 'std_score_time': array([6.47102674e-04, 1.29569666e-03, 2.63484896e-03, 7.19423057e-04,\n        3.07026565e-03, 8.57176065e-04, 6.28801129e-04, 2.72112457e-03,\n        4.17508730e-04, 8.18936743e-04, 1.85290143e-03, 9.41843587e-04,\n        6.42283555e-04, 4.13389812e-04, 1.32014099e-03, 1.48168434e-03,\n        2.24646807e-03, 1.04385510e-03, 1.80877156e-06, 2.63640415e-03,\n        4.04703972e-04, 1.56059063e-03, 1.78494611e-02, 4.07778039e-04,\n        4.69509136e-04, 1.65576367e-03, 6.86664480e-03, 4.85106541e-04,\n        2.43201157e-04, 1.62572025e-05, 9.42418190e-04, 1.02898922e-03,\n        4.55074613e-04, 8.23933960e-04, 6.28931291e-04, 2.11400786e-03,\n        4.74142421e-04, 1.40483303e-03, 6.38469474e-04, 4.71932673e-04,\n        2.96640133e-04, 8.55301988e-04, 2.14902519e-04, 4.74044023e-04,\n        1.22904976e-03, 1.48633288e-03, 8.41452551e-04, 4.05807239e-04,\n        2.28031248e-04, 1.04471786e-03, 4.10935399e-04, 2.40524916e-03,\n        2.36380545e-03, 4.73921245e-04, 2.09616172e-04, 3.98099537e-04,\n        6.90434857e-04, 4.88456359e-04, 8.21897634e-04, 2.13566639e-03,\n        5.95525119e-03, 1.38965938e-03, 6.19898250e-04, 4.68437373e-04,\n        4.89375311e-04, 4.08442704e-04, 2.72281049e-03, 2.85458159e-03,\n        3.80312721e-03, 4.68497718e-04, 5.00933126e-04, 4.77691098e-04,\n        4.24909402e-04, 3.48565799e-03, 8.18095988e-04, 3.25158220e-03,\n        8.49565309e-04, 2.45371241e-04, 1.65294875e-03, 4.69069245e-04,\n        6.32594848e-04, 2.01834502e-03, 2.40536788e-03, 1.54915370e-03,\n        4.20679852e-04, 8.53528575e-04, 6.92463586e-06, 6.23267968e-04,\n        1.46452099e-03, 1.47243287e-03, 2.72863694e-03, 1.03868517e-03,\n        1.70533600e-03, 2.32324549e-03, 1.31701217e-03, 9.52650083e-04,\n        2.41834515e-04, 1.31617976e-03, 3.01597279e-03, 2.23220670e-03]),\n 'param_alpha': masked_array(data=[195.48405222863136, 82.56029576197498,\n                    0.44024351225185143, 364.0701112808811,\n                    53.727140757694336, 0.0043970564429580234,\n                    0.03590927096469022, 9980.865434069025,\n                    0.002859570703723829, 68.23620415720254,\n                    0.02786619091895085, 0.005574267134998345,\n                    1528.0547398319738, 21.63340002058571,\n                    351.8379927192362, 0.021022167265373932,\n                    2.321345973691301, 5252.8186891347095,\n                    0.08187807832790248, 0.030739933220278234,\n                    4.942470567858895, 179.13521431417263,\n                    0.001177995364294815, 4.562168492227108,\n                    0.002207275376044895, 0.014025771845171363,\n                    3390.080431689054, 496.44710663064103,\n                    0.9401173957764195, 279.9544005549383,\n                    0.004040045367342484, 1.8372523401167777,\n                    11.671067370512267, 3673.757325304093,\n                    270.122183566781, 0.002651348387177266,\n                    1236.452077007676, 2.0369493311953026,\n                    49.540735270318905, 9017.145602941444,\n                    3.1897003451724237, 15.909005271591043,\n                    270.5949990606467, 15.137747350461387,\n                    350.89109688767945, 0.008675038900473228,\n                    1.2870872989629856, 0.003803559141377889,\n                    0.07258854342439401, 37.43482657508818,\n                    2008.7689880811952, 0.04881183888256398,\n                    63.8819190813346, 3554.968069875446, 792.7423587690853,\n                    0.49233448299839666, 24.14622420657272,\n                    0.5558866110877222, 23.21656369692641,\n                    331.8103818659524, 0.002128244975686662,\n                    520.0636441296363, 140.40308635230468,\n                    77.27545764780585, 0.36743797569455094,\n                    0.20534478509228804, 8.932900065010093,\n                    5642.591615624239, 0.008000368916087281,\n                    4.873423508938212, 1213.525631228544,\n                    0.5447174208915585, 62.18010099049566,\n                    0.13219595827549685, 1105.3277407711132,\n                    0.00308637973690833, 5.4049607969252085,\n                    5.194844024734802, 72.3076580431818, 138.0676055792673,\n                    11.827851064389034, 8.47955879881921, 3.46048879759531,\n                    0.04203177850537053, 0.005418819150254471,\n                    0.026016689740516158, 108.43077219430774,\n                    0.03141610961665743, 0.01668099427846113,\n                    1.1480918541790852, 6091.199063147831,\n                    0.5057140744171967, 734.3400569136479,\n                    17.596719675694793, 5.798883543365199,\n                    48.83292778582255, 185.40778026381705,\n                    11.87976242665855, 0.6138735273878335,\n                    0.005426812420837048],\n              mask=[False, False, False, False, False, False, False, False,\n                    False, False, False, False, False, False, False, False,\n                    False, False, False, False, False, False, False, False,\n                    False, False, False, False, False, False, False, False,\n                    False, False, False, False, False, False, False, False,\n                    False, False, False, False, False, False, False, False,\n                    False, False, False, False, False, False, False, False,\n                    False, False, False, False, False, False, False, False,\n                    False, False, False, False, False, False, False, False,\n                    False, False, False, False, False, False, False, False,\n                    False, False, False, False, False, False, False, False,\n                    False, False, False, False, False, False, False, False,\n                    False, False, False, False],\n        fill_value=1e+20),\n 'param_max_iter': masked_array(data=[3865, 607, 1070, 2827, 4182, 526, 2264, 4712, 4893,\n                    2164, 3097, 4409, 139, 585, 1400, 1170, 4282, 4238,\n                    4628, 3488, 2584, 1975, 726, 978, 323, 4637, 593, 4239,\n                    2364, 4507, 2702, 1107, 3690, 2265, 4365, 1321, 1415,\n                    901, 1114, 1777, 2421, 1399, 3203, 4266, 708, 2458,\n                    2527, 3109, 1826, 3587, 3527, 3698, 4611, 3260, 1908,\n                    1801, 4268, 3988, 3549, 449, 1889, 466, 3433, 2071,\n                    2629, 3489, 1265, 2679, 4675, 132, 2778, 3595, 3895,\n                    935, 1996, 1624, 3828, 2393, 2066, 2585, 1986, 4185,\n                    3198, 1000, 1703, 2709, 3285, 1677, 597, 2374, 2999,\n                    2309, 2184, 1642, 3276, 1074, 4879, 4585, 4319, 2148],\n              mask=[False, False, False, False, False, False, False, False,\n                    False, False, False, False, False, False, False, False,\n                    False, False, False, False, False, False, False, False,\n                    False, False, False, False, False, False, False, False,\n                    False, False, False, False, False, False, False, False,\n                    False, False, False, False, False, False, False, False,\n                    False, False, False, False, False, False, False, False,\n                    False, False, False, False, False, False, False, False,\n                    False, False, False, False, False, False, False, False,\n                    False, False, False, False, False, False, False, False,\n                    False, False, False, False, False, False, False, False,\n                    False, False, False, False, False, False, False, False,\n                    False, False, False, False],\n        fill_value=999999),\n 'params': [{'alpha': 195.48405222863136, 'max_iter': 3865},\n  {'alpha': 82.56029576197498, 'max_iter': 607},\n  {'alpha': 0.44024351225185143, 'max_iter': 1070},\n  {'alpha': 364.0701112808811, 'max_iter': 2827},\n  {'alpha': 53.727140757694336, 'max_iter': 4182},\n  {'alpha': 0.0043970564429580234, 'max_iter': 526},\n  {'alpha': 0.03590927096469022, 'max_iter': 2264},\n  {'alpha': 9980.865434069025, 'max_iter': 4712},\n  {'alpha': 0.002859570703723829, 'max_iter': 4893},\n  {'alpha': 68.23620415720254, 'max_iter': 2164},\n  {'alpha': 0.02786619091895085, 'max_iter': 3097},\n  {'alpha': 0.005574267134998345, 'max_iter': 4409},\n  {'alpha': 1528.0547398319738, 'max_iter': 139},\n  {'alpha': 21.63340002058571, 'max_iter': 585},\n  {'alpha': 351.8379927192362, 'max_iter': 1400},\n  {'alpha': 0.021022167265373932, 'max_iter': 1170},\n  {'alpha': 2.321345973691301, 'max_iter': 4282},\n  {'alpha': 5252.8186891347095, 'max_iter': 4238},\n  {'alpha': 0.08187807832790248, 'max_iter': 4628},\n  {'alpha': 0.030739933220278234, 'max_iter': 3488},\n  {'alpha': 4.942470567858895, 'max_iter': 2584},\n  {'alpha': 179.13521431417263, 'max_iter': 1975},\n  {'alpha': 0.001177995364294815, 'max_iter': 726},\n  {'alpha': 4.562168492227108, 'max_iter': 978},\n  {'alpha': 0.002207275376044895, 'max_iter': 323},\n  {'alpha': 0.014025771845171363, 'max_iter': 4637},\n  {'alpha': 3390.080431689054, 'max_iter': 593},\n  {'alpha': 496.44710663064103, 'max_iter': 4239},\n  {'alpha': 0.9401173957764195, 'max_iter': 2364},\n  {'alpha': 279.9544005549383, 'max_iter': 4507},\n  {'alpha': 0.004040045367342484, 'max_iter': 2702},\n  {'alpha': 1.8372523401167777, 'max_iter': 1107},\n  {'alpha': 11.671067370512267, 'max_iter': 3690},\n  {'alpha': 3673.757325304093, 'max_iter': 2265},\n  {'alpha': 270.122183566781, 'max_iter': 4365},\n  {'alpha': 0.002651348387177266, 'max_iter': 1321},\n  {'alpha': 1236.452077007676, 'max_iter': 1415},\n  {'alpha': 2.0369493311953026, 'max_iter': 901},\n  {'alpha': 49.540735270318905, 'max_iter': 1114},\n  {'alpha': 9017.145602941444, 'max_iter': 1777},\n  {'alpha': 3.1897003451724237, 'max_iter': 2421},\n  {'alpha': 15.909005271591043, 'max_iter': 1399},\n  {'alpha': 270.5949990606467, 'max_iter': 3203},\n  {'alpha': 15.137747350461387, 'max_iter': 4266},\n  {'alpha': 350.89109688767945, 'max_iter': 708},\n  {'alpha': 0.008675038900473228, 'max_iter': 2458},\n  {'alpha': 1.2870872989629856, 'max_iter': 2527},\n  {'alpha': 0.003803559141377889, 'max_iter': 3109},\n  {'alpha': 0.07258854342439401, 'max_iter': 1826},\n  {'alpha': 37.43482657508818, 'max_iter': 3587},\n  {'alpha': 2008.7689880811952, 'max_iter': 3527},\n  {'alpha': 0.04881183888256398, 'max_iter': 3698},\n  {'alpha': 63.8819190813346, 'max_iter': 4611},\n  {'alpha': 3554.968069875446, 'max_iter': 3260},\n  {'alpha': 792.7423587690853, 'max_iter': 1908},\n  {'alpha': 0.49233448299839666, 'max_iter': 1801},\n  {'alpha': 24.14622420657272, 'max_iter': 4268},\n  {'alpha': 0.5558866110877222, 'max_iter': 3988},\n  {'alpha': 23.21656369692641, 'max_iter': 3549},\n  {'alpha': 331.8103818659524, 'max_iter': 449},\n  {'alpha': 0.002128244975686662, 'max_iter': 1889},\n  {'alpha': 520.0636441296363, 'max_iter': 466},\n  {'alpha': 140.40308635230468, 'max_iter': 3433},\n  {'alpha': 77.27545764780585, 'max_iter': 2071},\n  {'alpha': 0.36743797569455094, 'max_iter': 2629},\n  {'alpha': 0.20534478509228804, 'max_iter': 3489},\n  {'alpha': 8.932900065010093, 'max_iter': 1265},\n  {'alpha': 5642.591615624239, 'max_iter': 2679},\n  {'alpha': 0.008000368916087281, 'max_iter': 4675},\n  {'alpha': 4.873423508938212, 'max_iter': 132},\n  {'alpha': 1213.525631228544, 'max_iter': 2778},\n  {'alpha': 0.5447174208915585, 'max_iter': 3595},\n  {'alpha': 62.18010099049566, 'max_iter': 3895},\n  {'alpha': 0.13219595827549685, 'max_iter': 935},\n  {'alpha': 1105.3277407711132, 'max_iter': 1996},\n  {'alpha': 0.00308637973690833, 'max_iter': 1624},\n  {'alpha': 5.4049607969252085, 'max_iter': 3828},\n  {'alpha': 5.194844024734802, 'max_iter': 2393},\n  {'alpha': 72.3076580431818, 'max_iter': 2066},\n  {'alpha': 138.0676055792673, 'max_iter': 2585},\n  {'alpha': 11.827851064389034, 'max_iter': 1986},\n  {'alpha': 8.47955879881921, 'max_iter': 4185},\n  {'alpha': 3.46048879759531, 'max_iter': 3198},\n  {'alpha': 0.04203177850537053, 'max_iter': 1000},\n  {'alpha': 0.005418819150254471, 'max_iter': 1703},\n  {'alpha': 0.026016689740516158, 'max_iter': 2709},\n  {'alpha': 108.43077219430774, 'max_iter': 3285},\n  {'alpha': 0.03141610961665743, 'max_iter': 1677},\n  {'alpha': 0.01668099427846113, 'max_iter': 597},\n  {'alpha': 1.1480918541790852, 'max_iter': 2374},\n  {'alpha': 6091.199063147831, 'max_iter': 2999},\n  {'alpha': 0.5057140744171967, 'max_iter': 2309},\n  {'alpha': 734.3400569136479, 'max_iter': 2184},\n  {'alpha': 17.596719675694793, 'max_iter': 1642},\n  {'alpha': 5.798883543365199, 'max_iter': 3276},\n  {'alpha': 48.83292778582255, 'max_iter': 1074},\n  {'alpha': 185.40778026381705, 'max_iter': 4879},\n  {'alpha': 11.87976242665855, 'max_iter': 4585},\n  {'alpha': 0.6138735273878335, 'max_iter': 4319},\n  {'alpha': 0.005426812420837048, 'max_iter': 2148}],\n 'split0_test_score': array([0.66253522, 0.67167592, 0.67648791, 0.65588383, 0.67545591,\n        0.67516686, 0.68145967, 0.35585537, 0.68650425, 0.67335251,\n        0.68408879, 0.68617856, 0.62986633, 0.67646047, 0.65617531,\n        0.67684697, 0.68660344, 0.50116831, 0.68637276, 0.68494438,\n        0.68360245, 0.6632763 , 0.67545733, 0.67652143, 0.67545981,\n        0.68635756, 0.57540059, 0.65322439, 0.68200221, 0.65783852,\n        0.68297784, 0.67671005, 0.6858444 , 0.56542738, 0.65805894,\n        0.67745887, 0.63739958, 0.67605719, 0.67602361, 0.36938335,\n        0.68267345, 0.68004765, 0.65804838, 0.68505362, 0.65619783,\n        0.68216799, 0.68266453, 0.68411223, 0.67966412, 0.67749421,\n        0.61592583, 0.68531873, 0.67392623, 0.569662  , 0.64745221,\n        0.6796006 , 0.68223141, 0.68590111, 0.68274139, 0.65664818,\n        0.67992745, 0.65279154, 0.66615542, 0.67206349, 0.68282415,\n        0.6849944 , 0.67830876, 0.48383965, 0.68638097, 0.67463271,\n        0.63796016, 0.68528023, 0.67418561, 0.67602085, 0.64054597,\n        0.67877241, 0.68616338, 0.68297496, 0.67271039, 0.66641929,\n        0.68253103, 0.68644195, 0.68503934, 0.67622713, 0.67912001,\n        0.68300461, 0.66963977, 0.67900761, 0.67523228, 0.68207782,\n        0.46283184, 0.68171488, 0.64864908, 0.68093306, 0.68515703,\n        0.67612174, 0.66300149, 0.68680439, 0.68628195, 0.6810011 ]),\n 'split1_test_score': array([0.60973181, 0.60923967, 0.6018751 , 0.60572335, 0.60716037,\n        0.60439775, 0.59550651, 0.37964056, 0.58332131, 0.60930811,\n        0.59072505, 0.58492286, 0.59176321, 0.60682279, 0.60570417,\n        0.60138906, 0.58768976, 0.51294919, 0.58425961, 0.58875991,\n        0.59610923, 0.6101766 , 0.60322616, 0.60292092, 0.60620197,\n        0.58414978, 0.56115113, 0.60539417, 0.59531811, 0.60569824,\n        0.59289928, 0.60198179, 0.59557535, 0.55503628, 0.60647029,\n        0.6007215 , 0.59699226, 0.60281189, 0.60569497, 0.38583732,\n        0.5960716 , 0.60341396, 0.6064402 , 0.5964127 , 0.6057014 ,\n        0.59433342, 0.59457541, 0.59064311, 0.59810794, 0.60290937,\n        0.58376656, 0.58780874, 0.60869041, 0.55749827, 0.60279787,\n        0.59838149, 0.60009867, 0.58708137, 0.59852111, 0.60565311,\n        0.59772199, 0.60523035, 0.61090927, 0.61025691, 0.59352156,\n        0.58891023, 0.60271054, 0.49997018, 0.58401755, 0.60949635,\n        0.59735775, 0.58872371, 0.60846876, 0.60235482, 0.59897956,\n        0.59920821, 0.59169046, 0.59708526, 0.609766  , 0.61091454,\n        0.60080737, 0.59305747, 0.5925966 , 0.60208232, 0.59877873,\n        0.59287186, 0.61142735, 0.59892912, 0.60392885, 0.59535454,\n        0.48377601, 0.59544423, 0.60336842, 0.6028404 , 0.59361747,\n        0.60546618, 0.61001789, 0.59474388, 0.58590504, 0.59618986]),\n 'split2_test_score': array([0.32097615, 0.33533594, 0.3329735 , 0.32961835, 0.33759228,\n        0.32982083, 0.34162957, 0.26391336, 0.35270058, 0.33618418,\n        0.34683763, 0.35176105, 0.34928982, 0.33083587, 0.32911074,\n        0.33386313, 0.34954649, 0.33648756, 0.35219459, 0.34871197,\n        0.3406831 , 0.32244665, 0.33164028, 0.32845134, 0.32249174,\n        0.35224722, 0.35134073, 0.33464799, 0.34203637, 0.3260837 ,\n        0.34457274, 0.33199724, 0.34303893, 0.34999889, 0.32565122,\n        0.33481107, 0.34709514, 0.3306835 , 0.33841482, 0.27239916,\n        0.34043488, 0.33609605, 0.3256719 , 0.34152997, 0.32907348,\n        0.34298477, 0.34301691, 0.34691209, 0.3383952 , 0.33956279,\n        0.35162744, 0.34955786, 0.33653925, 0.35060089, 0.34171991,\n        0.3380455 , 0.33455114, 0.35028167, 0.33654048, 0.32829275,\n        0.33889164, 0.33546193, 0.32822448, 0.3354893 , 0.34398648,\n        0.34862243, 0.33037538, 0.33122415, 0.35232338, 0.30158671,\n        0.34687742, 0.34887339, 0.33665499, 0.33253497, 0.34576181,\n        0.33692368, 0.34691904, 0.33945535, 0.3358804 , 0.32871433,\n        0.33875723, 0.34611638, 0.34446   , 0.3329252 , 0.33750434,\n        0.34460839, 0.33264205, 0.33730221, 0.33069946, 0.34203822,\n        0.32409784, 0.3417941 , 0.34083064, 0.33800635, 0.34501063,\n        0.3386464 , 0.32151599, 0.34454349, 0.35119575, 0.34080516]),\n 'mean_test_score': array([0.53108106, 0.53875051, 0.53711217, 0.53040851, 0.54006952,\n        0.53646181, 0.53953192, 0.33313643, 0.54084204, 0.53961493,\n        0.54055049, 0.54095416, 0.52363979, 0.53803971, 0.53033007,\n        0.53736639, 0.5412799 , 0.45020169, 0.54094232, 0.54080542,\n        0.54013159, 0.53196651, 0.53677459, 0.53596456, 0.53471784,\n        0.54091819, 0.49596415, 0.53108885, 0.53978556, 0.52987348,\n        0.54014995, 0.53689636, 0.54148623, 0.49015419, 0.53006015,\n        0.53766382, 0.52716233, 0.53651753, 0.54004447, 0.34253994,\n        0.53972665, 0.53985255, 0.53005349, 0.54099876, 0.53032423,\n        0.53982873, 0.54008562, 0.54055581, 0.53872242, 0.53998879,\n        0.51710661, 0.54089511, 0.53971863, 0.49258705, 0.53065666,\n        0.53867586, 0.53896041, 0.54108805, 0.53926766, 0.53019801,\n        0.53884703, 0.53116127, 0.53509639, 0.5392699 , 0.54011073,\n        0.54084236, 0.53713156, 0.43834466, 0.5409073 , 0.52857192,\n        0.52739844, 0.54095911, 0.53976979, 0.53697021, 0.52842911,\n        0.53830143, 0.54159096, 0.53983852, 0.53945226, 0.53534939,\n        0.54069854, 0.54187193, 0.54069864, 0.53707821, 0.53846769,\n        0.54016162, 0.53790306, 0.53841298, 0.5366202 , 0.53982352,\n        0.42356856, 0.53965107, 0.53094938, 0.54059327, 0.54126171,\n        0.54007811, 0.53151179, 0.54203059, 0.54112758, 0.53933204]),\n 'std_test_score': array([0.15012241, 0.14607689, 0.14752675, 0.14344926, 0.14586259,\n        0.14894616, 0.14427056, 0.04990196, 0.1395459 , 0.14620414,\n        0.14217994, 0.14002072, 0.12426154, 0.14924795, 0.14376775,\n        0.147159  , 0.14146202, 0.08055174, 0.13982383, 0.14139256,\n        0.14548435, 0.14973048, 0.14801891, 0.14977886, 0.15270691,\n        0.13978354, 0.10242953, 0.14027043, 0.14423844, 0.14566482,\n        0.1430999 , 0.14806263, 0.14508187, 0.09919551, 0.14606529,\n        0.1468198 , 0.1283909 , 0.14858657, 0.14543593, 0.05004985,\n        0.14528793, 0.14743522, 0.14604732, 0.14561399, 0.14379122,\n        0.14373444, 0.14391428, 0.14214438, 0.14551311, 0.14495668,\n        0.11774569, 0.14103075, 0.14611713, 0.10052211, 0.13483651,\n        0.14569041, 0.14837746, 0.14082292, 0.1474155 , 0.1442785 ,\n        0.14531816, 0.13973592, 0.14800906, 0.14628719, 0.14339289,\n        0.14146711, 0.1494208 , 0.07603136, 0.13974379, 0.16269071,\n        0.12871938, 0.14142956, 0.14610816, 0.14765274, 0.13027522,\n        0.14605334, 0.14295489, 0.14596649, 0.14622272, 0.14785969,\n        0.14663995, 0.14357423, 0.14380223, 0.14749735, 0.14583868,\n        0.1430892 , 0.14707421, 0.14591618, 0.14848919, 0.14426712,\n        0.07085423, 0.14427102, 0.13569926, 0.14675538, 0.14371438,\n        0.14532515, 0.15005664, 0.14461362, 0.14041475, 0.1445866 ]),\n 'rank_test_score': array([ 77,  50,  61,  80,  30,  68,  43, 100,  17,  42,  23,  11,  91,\n         56,  81,  59,   5,  96,  12,  18,  26,  73,  65,  69,  72,  13,\n         93,  76,  37,  86,  25,  64,   4,  95,  84,  58,  90,  67,  31,\n         99,  39,  33,  85,   9,  82,  35,  28,  22,  51,  32,  92,  15,\n         40,  94,  79,  52,  48,   8,  47,  83,  49,  75,  71,  46,  27,\n         16,  60,  97,  14,  87,  89,  10,  38,  63,  88,  55,   3,  34,\n         44,  70,  20,   2,  19,  62,  53,  24,  57,  54,  66,  36,  98,\n         41,  78,  21,   6,  29,  74,   1,   7,  45])}\n\n\n\nimport pandas as pd\n\n# Suppose you already have a fitted RandomizedSearchCV object named random_search\nrandom_results_df = pd.DataFrame(random_search.cv_results_)\n\n# Each hyperparameter is in a column with 'param_' prefix:\n# e.g., 'param_alpha', 'param_max_iter', etc.\n# You can display them along with the mean CV score, etc.\ndf_random_search = random_results_df[\n    ['param_alpha', 'param_max_iter', 'mean_test_score', 'rank_test_score']\n].sort_values(by='mean_test_score', ascending=False)\n\ndf_random_search\n\n\n\n\n\n\n\n\nparam_alpha\nparam_max_iter\nmean_test_score\nrank_test_score\n\n\n\n\n97\n11.879762\n4585\n0.542031\n1\n\n\n81\n8.479559\n4185\n0.541872\n2\n\n\n76\n5.404961\n3828\n0.541591\n3\n\n\n32\n11.671067\n3690\n0.541486\n4\n\n\n16\n2.321346\n4282\n0.541280\n5\n\n\n...\n...\n...\n...\n...\n\n\n17\n5252.818689\n4238\n0.450202\n96\n\n\n67\n5642.591616\n2679\n0.438345\n97\n\n\n90\n6091.199063\n2999\n0.423569\n98\n\n\n39\n9017.145603\n1777\n0.342540\n99\n\n\n7\n9980.865434\n4712\n0.333136\n100\n\n\n\n\n100 rows √ó 4 columns\n\n\n\n\npx.scatter(df_random_search, x=\"param_alpha\", y=\"param_max_iter\", color=\"mean_test_score\", \n           title=\"Random Search\", log_x=True, log_y=True, color_continuous_scale=\"viridis\") \n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json",
    "crumbs": [
      "ML",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Hyperparameter tuning</span>"
    ]
  },
  {
    "objectID": "ml/01_Regression_Main_Concepts/Code/C1_Hyperparameter_Optimization.html#bayesian-optimization",
    "href": "ml/01_Regression_Main_Concepts/Code/C1_Hyperparameter_Optimization.html#bayesian-optimization",
    "title": "5¬† Hyperparameter tuning",
    "section": "5.5 Bayesian Optimization",
    "text": "5.5 Bayesian Optimization\nSklearn\n!pip install scikit-optimize\nfrom skopt import BayesSearchCV\nHyperopt\n\n!pip install hyperopt\n\nCollecting hyperopt\n  Downloading hyperopt-0.2.7-py2.py3-none-any.whl.metadata (1.7 kB)\nRequirement already satisfied: numpy in c:\\users\\hayk_\\.conda\\envs\\thesis\\lib\\site-packages (from hyperopt) (1.26.4)\nRequirement already satisfied: scipy in c:\\users\\hayk_\\.conda\\envs\\thesis\\lib\\site-packages (from hyperopt) (1.13.0)\nRequirement already satisfied: six in c:\\users\\hayk_\\.conda\\envs\\thesis\\lib\\site-packages (from hyperopt) (1.16.0)\nRequirement already satisfied: networkx&gt;=2.2 in c:\\users\\hayk_\\.conda\\envs\\thesis\\lib\\site-packages (from hyperopt) (3.3)\nCollecting future (from hyperopt)\n  Using cached future-1.0.0-py3-none-any.whl.metadata (4.0 kB)\nRequirement already satisfied: tqdm in c:\\users\\hayk_\\.conda\\envs\\thesis\\lib\\site-packages (from hyperopt) (4.66.2)\nCollecting cloudpickle (from hyperopt)\n  Downloading cloudpickle-3.1.1-py3-none-any.whl.metadata (7.1 kB)\nCollecting py4j (from hyperopt)\n  Downloading py4j-0.10.9.9-py2.py3-none-any.whl.metadata (1.3 kB)\nRequirement already satisfied: colorama in c:\\users\\hayk_\\.conda\\envs\\thesis\\lib\\site-packages (from tqdm-&gt;hyperopt) (0.4.6)\nDownloading hyperopt-0.2.7-py2.py3-none-any.whl (1.6 MB)\n   ---------------------------------------- 1.6/1.6 MB 2.3 MB/s eta 0:00:00\nDownloading cloudpickle-3.1.1-py3-none-any.whl (20 kB)\nUsing cached future-1.0.0-py3-none-any.whl (491 kB)\nDownloading py4j-0.10.9.9-py2.py3-none-any.whl (203 kB)\n   ---------------------------------------- 203.0/203.0 kB 2.4 MB/s eta 0:00:00\nInstalling collected packages: py4j, future, cloudpickle, hyperopt\nSuccessfully installed cloudpickle-3.1.1 future-1.0.0 hyperopt-0.2.7 py4j-0.10.9.9\n\n\n\nfrom sklearn.model_selection import cross_val_score, train_test_split, KFold\n\n\nfrom hyperopt import fmin, tpe, hp, Trials, STATUS_OK\n\n\n# 1) Define the search space for alpha and max_iter\nspace = {\n    'alpha': hp.loguniform('alpha', np.log(1e-3), np.log(1e3)),\n    'max_iter': hp.randint('max_iter', 5000)  # integer in [0, 5000)\n}\n\n# 2) We‚Äôll do 5-fold cross-validation to evaluate each set of params\ncv = KFold(n_splits=5, shuffle=True, random_state=42)\n\ndef objective(params):\n    # Extract hyperparameters\n    alpha = params['alpha']\n    max_iter = int(params['max_iter'])  # ensure integer\n    \n    # Create Lasso model\n    model = Lasso(alpha=alpha, max_iter=max_iter, random_state=509)\n    \n    # Cross-validate (negative MSE, so we can minimize it)\n    mse_scores = -cross_val_score(\n        model, X_train, y_train,\n        cv=cv, scoring='neg_mean_squared_error'\n    )\n    avg_mse = np.mean(mse_scores)\n    \n    # Hyperopt tries to minimize the returned value\n    return {\n        'loss': avg_mse,\n        'status': STATUS_OK,\n        # Optional: attach other info if you like\n        'params': params\n    }\n\n\ntrials = Trials()  # To store results\n\nbest = fmin(\n    fn=objective,\n    space=space,\n    algo=tpe.suggest,\n    max_evals=20,       # Number of trials (increase for better search)\n    trials=trials,\n    rstate=np.random.default_rng(509)  # for reproducibility\n)\n\nprint(\"\\nBest hyperparameters found:\")\nprint(best)\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:06&lt;00:00,  3.11trial/s, best loss: 5158148188.803587]\n\nBest hyperparameters found:\n{'alpha': 82.26205843443854, 'max_iter': 3194}\n\n\n\nbest_alpha = best['alpha']\nbest_max_iter = int(best['max_iter'])\n\nmodel_best = Lasso(alpha=best_alpha, max_iter=best_max_iter, random_state=42)\nmodel_best.fit(X_train, y_train)\n\n# Evaluate on the test set\ny_pred_test = model_best.predict(X_test)\n\nrmse, r2 = evaluate_model(y_test, y_pred_test)\nprint(f\"Test set: RMSE={rmse:.2f}, R^2={r2:.2f}\")\n\nTest set: RMSE=35930.87, R^2=0.58\n\n\n\nresults_df = pd.DataFrame(trials.results)\nresults_df = results_df.sort_values(by='loss')\n\nresults_df[\"alpha\"] = results_df[\"params\"].apply(lambda x: x[\"alpha\"])\nresults_df[\"max_iter\"] = results_df[\"params\"].apply(lambda x: x[\"max_iter\"])\nresults_df\n\n\n\n\n\n\n\n\nloss\nstatus\nparams\nalpha\nmax_iter\n\n\n\n\n14\n5.158148e+09\nok\n{'alpha': 82.26205843443854, 'max_iter': 3194}\n82.262058\n3194\n\n\n3\n5.167013e+09\nok\n{'alpha': 23.927000650080377, 'max_iter': 206}\n23.927001\n206\n\n\n13\n5.290333e+09\nok\n{'alpha': 347.92671616536, 'max_iter': 1292}\n347.926716\n1292\n\n\n11\n5.358360e+09\nok\n{'alpha': 28.505108573753763, 'max_iter': 2196}\n28.505109\n2196\n\n\n1\n5.384187e+09\nok\n{'alpha': 0.013918988887753607, 'max_iter': 429}\n0.013919\n429\n\n\n10\n5.839504e+09\nok\n{'alpha': 0.0027521976271328466, 'max_iter': 792}\n0.002752\n792\n\n\n16\n6.432845e+09\nok\n{'alpha': 0.037966985847236734, 'max_iter': 1196}\n0.037967\n1196\n\n\n8\n6.677457e+09\nok\n{'alpha': 0.0014052766974551073, 'max_iter': 1...\n0.001405\n1355\n\n\n6\n7.403906e+09\nok\n{'alpha': 0.002452008538580964, 'max_iter': 1820}\n0.002452\n1820\n\n\n18\n7.584504e+09\nok\n{'alpha': 9.753940120503824, 'max_iter': 2707}\n9.753940\n2707\n\n\n12\n7.606741e+09\nok\n{'alpha': 0.003579672035114573, 'max_iter': 1949}\n0.003580\n1949\n\n\n9\n8.507748e+09\nok\n{'alpha': 6.800025058217788, 'max_iter': 2774}\n6.800025\n2774\n\n\n5\n8.672568e+09\nok\n{'alpha': 0.37204177639824826, 'max_iter': 2589}\n0.372042\n2589\n\n\n2\n8.991760e+09\nok\n{'alpha': 0.010823774548987567, 'max_iter': 2817}\n0.010824\n2817\n\n\n19\n9.788528e+09\nok\n{'alpha': 1.4345076433491972, 'max_iter': 3130}\n1.434508\n3130\n\n\n4\n1.011915e+10\nok\n{'alpha': 0.1789505548304415, 'max_iter': 3469}\n0.178951\n3469\n\n\n15\n1.099852e+10\nok\n{'alpha': 2.932516616065772, 'max_iter': 3607}\n2.932517\n3607\n\n\n17\n1.115259e+10\nok\n{'alpha': 0.281485107534971, 'max_iter': 4041}\n0.281485\n4041\n\n\n7\n1.126640e+10\nok\n{'alpha': 0.01712767443580885, 'max_iter': 4151}\n0.017128\n4151\n\n\n0\n1.148764e+10\nok\n{'alpha': 0.23009145552081606, 'max_iter': 4237}\n0.230091\n4237\n\n\n\n\n\n\n\n\npx.scatter(results_df, x=\"alpha\", y=\"max_iter\", title=\"Hyperopt Search\",\n              log_x=True)\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json",
    "crumbs": [
      "ML",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Hyperparameter tuning</span>"
    ]
  }
]