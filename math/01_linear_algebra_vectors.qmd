---
title: "01 Vectors and Linear Algebra Fundamentals"
format:
  html:
    css: homework-styles.css
---

<script src="homework-scripts.js"></script>

![image.png](../background_photos/math_01_shinararutun.jpg)
[’¨’∏÷Ç’Ω’°’∂’Ø’°÷Ä’´ ’∞’≤’∏÷Ç’¥’®](https://unsplash.com/photos/black-and-yellow-crane-near-building-during-daytime-JcRhkLqvICA), ’Ä’•’≤’´’∂’°’Ø’ù [Suren Sargsyan](https://unsplash.com/@s_u_ren)
      

# üìö ’Ü’µ’∏÷Ç’©’®

- [üìö ‘±’¥’¢’∏’≤’ª’°’Ø’°’∂ ’∂’µ’∏÷Ç’©’®](01_vectors_linear_algebra.qmd)
- [üì∫ ’è’•’Ω’°’£÷Ä’∏÷Ç’©’µ’∏÷Ç’∂’®(ToDo)]()
- [üéûÔ∏è ’ç’¨’°’µ’§’•÷Ä - Vectors ToDo](Lectures/L01_Vectors.pdf)
- [üéûÔ∏è ’ç’¨’°’µ’§’•÷Ä - Geometry ToDo ](Lectures/L02_Geometry_of_Vectors__Matrices.pdf)
- [üõ†Ô∏èüì∫ ‘≥’∏÷Ä’Æ’∂’°’Ø’°’∂’´ ’ø’•’Ω’°’£÷Ä’∏÷Ç’©’µ’∏÷Ç’∂’® ToDo](https://youtu.be/vectors_practical)
- [üõ†Ô∏èüóÇÔ∏è ‘≥’∏÷Ä’Æ’∂’°’Ø’°’∂’´ PDF-’® ToDo](Homeworks/hw_01_vectors.pdf)
  

ToDo
‘±’µ’Ω ’§’°’Ω’´’∂ ’Ø’Æ’°’∂’∏’©’°’∂’°’∂÷Ñ ’æ’•’Ø’ø’∏÷Ä’∂’•÷Ä’´ ÷á ’¥’°’ø÷Ä’´÷Å’∂’•÷Ä’´ ’∞’´’¥’∏÷Ç’∂÷Ñ’∂’•÷Ä’´’∂’ù

1. ’é’•’Ø’ø’∏÷Ä’∂’•÷Ä (’£’∏÷Ä’Æ’∏’≤’∏÷Ç’©’µ’∏÷Ç’∂’∂’•÷Ä, ’Ω’Ø’°’¨’µ’°÷Ä ’°÷Ä’ø’°’§÷Ä’µ’°’¨, ’∂’∏÷Ä’¥’°)
2. ’é’•’Ø’ø’∏÷Ä’∂’•÷Ä’´ ’•÷Ä’Ø÷Ä’°’π’°÷É’∏÷Ç’©’µ’∏÷Ç’∂ (’°’∂’Ø’µ’∏÷Ç’∂’∂’•÷Ä, ’∏÷Ç’≤’≤’°’∞’°’µ’°÷Å’∏÷Ç’©’µ’∏÷Ç’∂)
3. ’Ñ’°’ø÷Ä’´÷Å’∂’•÷Ä ÷á ’£’Æ’°’µ’´’∂ ÷É’∏’≠’°’Ø’•÷Ä’∫’∏÷Ç’¥’∂’•÷Ä
4. ‘≥’Æ’°’µ’´’∂ ’∞’°’¥’°’Ø÷Å’∏÷Ç’©’µ’∏÷Ç’∂’∂’•÷Ä ÷á ’ø’°÷Ä’°’Æ’∏÷Ç’©’µ’∏÷Ç’∂’∂’•÷Ä

# üè° ’è’∂’°’µ’´’∂

::: {.callout-note collapse="false"}
1. ‚ùó‚ùó‚ùó DON'T CHECK THE SOLUTIONS BEFORE TRYING TO DO THE HOMEWORK BY YOURSELF‚ùó‚ùó‚ùó
2. Please don't hesitate to ask questions, never forget about the üçäkaralyoküçä principle!
3. The harder the problem is, the more üßÄcheesesüßÄ it has.
4. Problems with üéÅ are just extra bonuses. It would be good to try to solve them, but also it's not the highest priority task.
5. If the problem involve many boring calculations, feel free to skip them - important part is understanding the concepts.
6. Submit your solutions [here](https://forms.gle/CFEvNqFiTSsDLiFc6) (even if it's unfinished)
:::

TODO
## Vector Operations

### 01: RGB color mixing with vectors {data-difficulty="1"}

::: {.callout-tip collapse="true" appearance="minimal"}
#### Context
In computer graphics and image processing, colors are represented as RGB vectors where each component (Red, Green, Blue) ranges from 0 to 255. Vector operations on these RGB values correspond to color mixing and transformations.
:::

Consider these RGB color vectors:
- Red: $\vec{r} = (255, 0, 0)$
- Yellow: $\vec{y} = (255, 255, 0)$
- A custom color: $\vec{c} = (128, 64, 192)$

a) Calculate what color you get by adding red and yellow: $\vec{r} + \vec{y}$. What happens when RGB values exceed 255?

b) Find the "average" color between red and yellow: $\frac{1}{2}(\vec{r} + \vec{y})$

c) Use a color picker (like [Google's color picker](https://g.co/kgs/color-picker) or any online tool) to verify your answers from parts (a) and (b). What colors do you actually see?

d) Calculate $\vec{r} - \frac{1}{2}\vec{y}$. What does this operation represent in terms of color mixing?

### 02: Feature vector normalization {data-difficulty="2"}

::: {.callout-note collapse="true" appearance="minimal"}
#### Context
In ML preprocessing, we often normalize feature vectors to have unit length. This helps algorithms that are sensitive to the scale of input features, like k-nearest neighbors or neural networks.
:::

A customer profile is represented by the vector $\vec{v} = (25, 50000, 3)$ where components represent [age, income in $, number of purchases].

a) Calculate the Euclidean norm (magnitude) $||\vec{v}||_2$
b) Find the unit vector $\hat{v} = \frac{\vec{v}}{||\vec{v}||_2}$
c) Verify that $||\hat{v}||_2 = 1$

### 02.5: Multi-scale feature normalization {data-difficulty="2"}

::: {.callout-important collapse="true" appearance="minimal"}
#### Context
When features have vastly different scales (like salary in thousands vs age in tens), machine learning algorithms can be biased toward features with larger magnitudes. Min-max normalization scales features to a [0,1] range, while z-score normalization centers data around mean=0 with standard deviation=1.
:::

Consider a dataset with two features for house prediction:
- House size: $\vec{s} = (800, 1200, 2000, 1500, 900)$ square feet
- Distance to city center: $\vec{d} = (2.5, 8.1, 15.2, 6.3, 4.7)$ kilometers

a) **Min-Max Normalization:** Transform both features to [0,1] range using:
   $$\vec{v}_{norm} = \frac{\vec{v} - \min(\vec{v})}{\max(\vec{v}) - \min(\vec{v})}$$

b) **Z-Score Normalization:** Transform both features using:
   $$\vec{v}_{std} = \frac{\vec{v} - \mu_v}{\sigma_v}$$
   where $\mu_v$ is the mean and $\sigma_v$ is the standard deviation.

c) **Comparison:** Calculate the Euclidean distance between the first and last data points using:
   - Original features: $d_{orig} = ||(800, 2.5) - (900, 4.7)||$
   - Min-max normalized features
   - Z-score normalized features
   
d) **Discussion:** Which normalization method preserves the relative distances better? Why might this matter for clustering algorithms?

### 03: Distance between data points {data-difficulty="2"}
Two data points in a dataset are represented as $\vec{p_1} = (1, 3, -2)$ and $\vec{p_2} = (4, -1, 1)$.

a) Calculate the Euclidean distance between these points
b) Calculate the Manhattan distance (L1 norm of the difference)
c) Which distance metric would be more robust to outliers? Explain briefly.

### 03.5: k-Nearest Neighbors Classification {data-difficulty="3"}

::: {.callout-important collapse="true" appearance="minimal"}
#### Context
k-Nearest Neighbors (KNN) is a fundamental machine learning algorithm that classifies data points based on the class labels of their nearest neighbors. The algorithm relies entirely on distance calculations between feature vectors, making it a perfect application of vector operations.
:::

You have a 2D dataset for classifying emails as "spam" or "ham" (not spam) based on two features:
- **Feature 1**: Number of exclamation marks per 100 words
- **Feature 2**: Percentage of words in ALL CAPS

**Training data:**
- Spam: $S_1 = (8, 25)$, $S_2 = (12, 30)$, $S_3 = (6, 20)$
- Ham: $H_1 = (1, 5)$, $H_2 = (2, 8)$, $H_3 = (0, 3)$

**New email to classify:** $\vec{x} = (4, 15)$

a) **Distance Calculation:** Calculate the Euclidean distance from $\vec{x}$ to each of the 6 training points.

b) **3-NN Classification:** Using $k=3$, identify the 3 nearest neighbors to $\vec{x}$. What class would you assign to $\vec{x}$ based on majority voting?

c) **1-NN vs 5-NN:** What would be the classification if you used $k=1$? What about $k=5$? 

d) **Distance Metric Impact:** Repeat part (b) using Manhattan distance instead of Euclidean distance. Does the classification change?

e) **Discussion:** Why might the choice of $k$ and distance metric matter for KNN performance? What are potential advantages and disadvantages of KNN compared to other algorithms?

## Dot Products and Angles

### 04: Similarity measurement {data-difficulty="2"}

::: {.callout-important collapse="true" appearance="minimal"}
#### Context
The dot product is fundamental in measuring similarity between vectors. In recommendation systems, we often use cosine similarity (based on dot products) to find similar users or items.
:::

Two user preference vectors are $\vec{u_1} = (5, 3, 1, 4)$ and $\vec{u_2} = (3, 5, 2, 2)$ where each component represents rating for different movie genres.

a) Calculate the dot product $\vec{u_1} \cdot \vec{u_2}$
b) Calculate the cosine similarity: $\cos(\theta) = \frac{\vec{u_1} \cdot \vec{u_2}}{||\vec{u_1}|| \cdot ||\vec{u_2}||}$
c) What does a cosine similarity close to 1 indicate about user preferences?

### 04.5: Word embeddings similarity {data-difficulty="2"}

::: {.callout-tip collapse="true" appearance="minimal"}
#### Context
In NLP, words can be represented as vectors called embeddings. By comparing these vectors using different distance metrics, we can determine semantic similarity between words. This is the foundation of modern language models and search engines.
:::

Given the following 2D word embeddings:
- **cheese:** $(1, 2)$
- **mushroom:** $(3, 1)$  
- **tasty:** $(2, 2)$

a) **Euclidean Distance Analysis:**
   - Compute the Euclidean distance between **tasty** and **cheese**
   - Compute the Euclidean distance between **tasty** and **mushroom**
   - Which word is closer to **tasty** based on Euclidean distance?

b) **Cosine Similarity Analysis:**
   - Compute the cosine similarity between **tasty** and **cheese**
   - Compute the cosine similarity between **tasty** and **mushroom**
   - Which word is closer to **tasty** based on cosine similarity?

c) **Discussion:** Compare the outcomes from parts (a) and (b). Why might one metric be preferred over the other in different NLP applications?

::: {.callout-note collapse="true" appearance="minimal"}
**Fun fact:** Check out this [3Blue1Brown video on word vectors](https://youtu.be/wjZofJX0v4M?t=751) for more insights!
:::

### 05: Matrix transformations {data-difficulty="2"}

What vectors do you get by applying the matrix $A = \begin{pmatrix} 3 & -3 \\ 3 & 3 \end{pmatrix}$ on the vectors:

a) $\vec{a} = \begin{pmatrix} 1 \\ 0 \end{pmatrix}$

b) $\vec{b} = \begin{pmatrix} 0 \\ 1 \end{pmatrix}$

c) $\vec{c} = \begin{pmatrix} 1 \\ 1 \end{pmatrix}$

d) (Additional) Draw the vectors before and after multiplying with $A$. What can you say visually about the matrix? Can you guess how it will act on the vector $\begin{pmatrix} 2 \\ -2 \end{pmatrix}$?

### 06: Finding perpendicular vectors {data-difficulty="2"}

::: {.callout-note collapse="true" appearance="minimal"}
#### Context
Finding perpendicular vectors is fundamental in many applications, from computer graphics (surface normals) to optimization (gradient descent directions) and data analysis (principal component analysis).
:::

Given the vector $\vec{v} = (2, 3)$:

a) Find a non-zero vector $\vec{w} = (x, y)$ such that $\vec{v}$ and $\vec{w}$ are perpendicular.

b) Verify that your chosen vector $\vec{w}$ satisfies $\vec{v} \cdot \vec{w} = 0$.

c) Find a unit vector in the direction of $\vec{w}$ by computing $\frac{\vec{w}}{||\vec{w}||}$.

d) Explain why there are infinitely many vectors perpendicular to $\vec{v}$ and describe the general form of all such vectors.

### 06.5: Orthogonality check {data-difficulty="1"}
Determine which pairs of vectors are orthogonal (perpendicular):

a) $\vec{a} = (1, 2, -1)$ and $\vec{b} = (2, -1, 0)$
b) $\vec{c} = (3, 4)$ and $\vec{d} = (-4, 3)$
c) $\vec{e} = (1, 1, 1)$ and $\vec{f} = (1, -2, 1)$

### 07: Matrix products {data-difficulty="2"}

Compute the following products:

a) $AB$, where $A = \begin{pmatrix} 6 & 5 \\ -2 & 7 \end{pmatrix}$, $B = \begin{pmatrix} -5 & 3 \\ 1 & 4 \end{pmatrix}$

b) $(A - B)(A + B)$, where $A = \begin{pmatrix} 2 & 2 & 4 \\ -3 & -2 & 4 \\ -2 & 0 & 2 \end{pmatrix}$, $B = \begin{pmatrix} 2 & 1 & 3 \\ -1 & 2 & 2 \\ 1 & 4 & -1 \end{pmatrix}$

c) $A^2 - B^2$, with the same $A$ and $B$ as in part (b).

### 08: Deriving the cosine angle formula {data-difficulty="3"}

Derive the formula for the cosine of the angle between two vectors: $\cos(\theta) = \frac{\vec{a} \cdot \vec{b}}{||\vec{a}|| \cdot ||\vec{b}||}$

::: {.callout-warning collapse="true" appearance="minimal"}
#### Hint
Start with the law of cosines for a triangle: $c^2 = a^2 + b^2 - 2ab\cos(\theta)$. Consider a triangle formed by vectors $\vec{a}$, $\vec{b}$, and $\vec{a} - \vec{b}$. The side lengths are $||\vec{a}||$, $||\vec{b}||$, and $||\vec{a} - \vec{b}||$. Express $||\vec{a} - \vec{b}||^2$ using the dot product and substitute into the law of cosines.
:::

a) Write down the law of cosines for the triangle with sides $||\vec{a}||$, $||\vec{b}||$, and $||\vec{a} - \vec{b}||$

b) Express $||\vec{a} - \vec{b}||^2$ in terms of dot products by expanding $(\vec{a} - \vec{b}) \cdot (\vec{a} - \vec{b})$

c) Substitute your result from part (b) into the law of cosines and solve for $\cos(\theta)$

d) Verify your derived formula using vectors $\vec{u} = (3, 4)$ and $\vec{v} = (1, 0)$

### 09: Shear matrix transformations {data-difficulty="2"}

Consider the following matrix (it is called the shear matrix): $S = \begin{pmatrix} 1 & 1 \\ 0 & 1 \end{pmatrix}$

a) What would you get if you apply $S$ on the vector $\begin{pmatrix} 0 \\ 1 \end{pmatrix}$?

b) What would you get if you apply $S$ again on the result of the previous point?

c) What if you apply $S$ one more time?

d) What do you think happens when we apply $S$ 100 times on that vector?

e) Can you compute $S^{100}$?

### 10: Projection and components {data-difficulty="3"}
Given vectors $\vec{a} = (4, 3)$ and $\vec{b} = (1, 2)$:

a) Find the projection of $\vec{a}$ onto $\vec{b}$: $\text{proj}_{\vec{b}}\vec{a} = \frac{\vec{a} \cdot \vec{b}}{||\vec{b}||^2}\vec{b}$
b) Find the component of $\vec{a}$ perpendicular to $\vec{b}$
c) Verify that $\vec{a} = \text{proj}_{\vec{b}}\vec{a} + \vec{a}_{\perp}$

## Geometric Interpretation

### 11: Triangle inequality {data-difficulty="2"}
For vectors $\vec{u} = (3, 4)$ and $\vec{v} = (5, -12)$:

a) Calculate $||\vec{u}||$, $||\vec{v}||$, and $||\vec{u} + \vec{v}||$
b) Verify the triangle inequality: $||\vec{u} + \vec{v}|| \leq ||\vec{u}|| + ||\vec{v}||$
c) When does equality hold in the triangle inequality?

### 12: Cross product applications {data-difficulty="3"}

::: {.callout-note collapse="true" appearance="minimal"}
#### Context
Cross products are used in computer graphics for calculating surface normals, determining the orientation of objects, and computing areas of parallelograms.
:::

Given vectors $\vec{a} = (2, 1, -1)$ and $\vec{b} = (1, 3, 2)$:

a) Calculate the cross product $\vec{a} \times \vec{b}$
b) Verify that $\vec{a} \times \vec{b}$ is orthogonal to both $\vec{a}$ and $\vec{b}$
c) Find the area of the parallelogram spanned by $\vec{a}$ and $\vec{b}$

## Linear Combinations and Spans

### 13: Linear combinations {data-difficulty="2"}
Given vectors $\vec{v_1} = (1, 2, 1)$, $\vec{v_2} = (2, -1, 3)$, and $\vec{v_3} = (1, -4, 1)$:

a) Express $\vec{w} = (5, 0, 7)$ as a linear combination of $\vec{v_1}$ and $\vec{v_2}$ if possible
b) Can $\vec{v_3}$ be written as a linear combination of $\vec{v_1}$ and $\vec{v_2}$?
c) What does it mean geometrically if three vectors are linearly dependent?

### 14: Basis vectors {data-difficulty="2"}
Consider the vectors $\vec{e_1} = (1, 0, 0)$, $\vec{e_2} = (0, 1, 0)$, and $\vec{e_3} = (0, 0, 1)$:

a) Express the vector $\vec{v} = (7, -3, 5)$ in terms of $\vec{e_1}$, $\vec{e_2}$, and $\vec{e_3}$
b) Why are $\vec{e_1}$, $\vec{e_2}$, and $\vec{e_3}$ called the standard basis vectors for $\mathbb{R}^3$?
c) Can any vector in $\mathbb{R}^3$ be expressed uniquely as a linear combination of these basis vectors?

### 15: Span and linear independence {data-difficulty="3"}
Determine whether the following sets of vectors span $\mathbb{R}^3$ and whether they are linearly independent:

a) $\{(1, 0, 1), (0, 1, 1), (1, 1, 0)\}$
b) $\{(1, 2, 3), (2, 4, 6), (0, 1, 2)\}$
c) $\{(1, 0, 0), (1, 1, 0), (1, 1, 1), (0, 1, 1)\}$

### 16: Principal Component Analysis (PCA) intuition {data-difficulty="3"}

::: {.callout-important collapse="true" appearance="minimal"}
#### Context
PCA is a dimensionality reduction technique that finds the directions (principal components) along which data varies the most. These directions are eigenvectors of the covariance matrix.
:::

Given a 2D dataset with points: $(1, 1)$, $(2, 2)$, $(3, 3)$, $(1, 3)$, $(3, 1)$:

a) Calculate the mean vector $\vec{\mu}$ of the dataset
b) Center the data by subtracting the mean from each point
c) Which direction would you expect the first principal component to point? Explain intuitively.

### 17: Vector in machine learning cost functions {data-difficulty="3"}

::: {.callout-tip collapse="true" appearance="minimal"}
#### Context
In linear regression, we minimize the sum of squared errors. This can be expressed elegantly using vector notation, where the error vector's magnitude represents the total cost.
:::

In a simple linear regression with predictions $\vec{y}_{pred} = (2.1, 4.8, 6.9, 9.2)$ and actual values $\vec{y}_{true} = (2, 5, 7, 9)$:

a) Calculate the error vector $\vec{e} = \vec{y}_{true} - \vec{y}_{pred}$
b) Calculate the mean squared error: $MSE = \frac{1}{n}||\vec{e}||^2$
c) How does minimizing $||\vec{e}||^2$ relate to finding the best-fit line?

### 18: Model selection with regularization {data-difficulty="3"}

::: {.callout-important collapse="true" appearance="minimal"}
#### Context
In machine learning, complex models with large weights can "memorize" the training data instead of learning generalizable patterns. This leads to overfitting - the model performs well on training data but poorly on new data. Regularization helps by penalizing large weight values, encouraging simpler models that generalize better. L1 regularization (Lasso) uses the Manhattan norm and can drive some weights to zero, while L2 regularization (Ridge) uses the Euclidean norm and keeps all weights small. The total loss = prediction error + Œª √ó regularization penalty.
:::

Two models have learned different weight vectors:
- Model A: $\vec{w_A} = (2.5, -1.8, 0.3, 4.1)$
- Model B: $\vec{w_B} = (1.2, -0.9, 0.7, 2.3)$

Both models achieve the same prediction error of $E = 12.5$ on the validation set.

a) Calculate the L1 regularization term for each model: $R_{L1} = ||\vec{w}||_1 = \sum_i |w_i|$

b) Calculate the L2 regularization term for each model: $R_{L2} = ||\vec{w}||_2^2 = \sum_i w_i^2$

c) For regularization parameter $\lambda = 0.1$, calculate the total regularized loss for each model using both L1 and L2 regularization:
   - $Loss_{L1} = E + \lambda \cdot R_{L1}$  
   - $Loss_{L2} = E + \lambda \cdot R_{L2}$

d) Which model would you choose under L1 regularization? Which under L2 regularization? Explain the difference.

### 19: Vector spaces {data-difficulty="2"}

::: {.callout-note collapse="true" appearance="minimal"}
#### Context
A vector space is a set of vectors that is closed under vector addition and scalar multiplication, and satisfies certain axioms. Understanding vector spaces is crucial for linear algebra and provides the foundation for many ML concepts like feature spaces and function spaces.
:::

Consider the set $S = \{(a, 2a, 3a) : a \in \mathbb{R}\}$ (all vectors of the form $(a, 2a, 3a)$ where $a$ is any real number).

a) Show that $S$ is closed under vector addition by taking two arbitrary vectors from $S$ and showing their sum is also in $S$.

b) Show that $S$ is closed under scalar multiplication by taking an arbitrary vector from $S$ and an arbitrary scalar.

c) Does $S$ form a vector space? What about the zero vector requirement?

d) Give a geometric interpretation of what the set $S$ represents in $\mathbb{R}^3$.

### 20: Vector subspaces {data-difficulty="3"}

::: {.callout-important collapse="true" appearance="minimal"}
#### Context
A vector subspace is a subset of a vector space that is itself a vector space. In machine learning, feature subspaces are often used for dimensionality reduction, and understanding subspaces helps with concepts like the null space and column space of matrices.
:::

Determine which of the following sets are vector subspaces of $\mathbb{R}^3$:

a) $V_1 = \{(x, y, z) : x + y + z = 0\}$ (vectors whose components sum to zero)

b) $V_2 = \{(x, y, z) : x + y + z = 1\}$ (vectors whose components sum to one)

c) $V_3 = \{(x, 0, z) : x, z \in \mathbb{R}\}$ (vectors with zero as the middle component)

d) $V_4 = \{(x, y, z) : x^2 + y^2 + z^2 \leq 1\}$ (vectors inside or on the unit sphere)

For each set, check the three subspace requirements: (1) contains zero vector, (2) closed under addition, (3) closed under scalar multiplication.

### 21: High-dimensional vector geometry {.bonus-problem data-difficulty="3"}
In high-dimensional spaces (common in ML), our intuition about geometry can be misleading.

Consider the unit sphere in $\mathbb{R}^n$ (all vectors with norm 1):

a) In 2D, what fraction of a unit square $[-1,1] \times [-1,1]$ is occupied by the unit circle?
b) Estimate this fraction for a unit cube in 3D
c) Research: What happens to this fraction as the dimension $n$ increases? This is known as the "curse of dimensionality."

## Matrix Transformations

### 22: Matrix transformations and geometric interpretation {data-difficulty="2"}

What vectors do you get by applying the matrix $A = \begin{pmatrix} 3 & -3 \\ 3 & 3 \end{pmatrix}$ on the vectors:

a) $\vec{a} = \begin{pmatrix} 1 \\ 0 \end{pmatrix}$

b) $\vec{b} = \begin{pmatrix} 0 \\ 1 \end{pmatrix}$

c) $\vec{c} = \begin{pmatrix} 1 \\ 1 \end{pmatrix}$

**Additional:** Draw the vectors before and after multiplying with $A$. What can you say visually about the matrix? Can you guess how it will act on the vector $\begin{pmatrix} 2 \\ -2 \end{pmatrix}$?

### 23: Matrix operations {data-difficulty="2"}

Compute the following products:

a) $AB$, where $A = \begin{pmatrix} 6 & 5 \\ -2 & 7 \end{pmatrix}$, $B = \begin{pmatrix} -5 & 3 \\ 1 & 4 \end{pmatrix}$

b) $(A - B)(A + B)$, where $A = \begin{pmatrix} 2 & 2 & 4 \\ -3 & -2 & 4 \\ -2 & 0 & 2 \end{pmatrix}$, $B = \begin{pmatrix} 2 & 1 & 3 \\ -1 & 2 & 2 \\ 1 & 4 & -1 \end{pmatrix}$

c) $A^2 - B^2$, with the same $A$ and $B$ as in part (b).

### 24: Shear matrix analysis {.bonus-problem data-difficulty="3"}

::: {.callout-note collapse="true" appearance="minimal"}
#### Context
Shear transformations are commonly used in computer graphics for creating italic text effects, perspective corrections, and geometric distortions. They preserve area but change angles and shapes.
:::

Consider the shear matrix $S = \begin{pmatrix} 1 & 1 \\ 0 & 1 \end{pmatrix}$:

a) What would you get if you apply $S$ on the vector $\begin{pmatrix} 0 \\ 1 \end{pmatrix}$?

b) What would you get if you apply $S$ again on the result of the previous point?

c) What if you apply $S$ one more time?

d) What do you think happens when we apply $S$ 100 times on that vector?

e) Can you compute $S^{100}$?

### 24.5: Diagonal matrix powers {data-difficulty="2"}

::: {.callout-note collapse="true" appearance="minimal"}
#### Context
Diagonal matrices are particularly useful in linear algebra because their powers are easy to compute. This property is extensively used in eigenvalue decomposition and diagonalization of matrices.
:::

Consider the diagonal matrix $A = \begin{pmatrix} 2 & 0 \\ 0 & -1 \end{pmatrix}$.

a) Compute $A^2$, $A^3$, and $A^4$.

b) Find a general formula for $A^n$ where $n$ is any positive integer.

c) What does this transformation represent geometrically? How does it affect the unit circle when applied repeatedly?

d) **Bonus:** What happens when you apply this transformation to the vector $\begin{pmatrix} 1 \\ 1 \end{pmatrix}$ multiple times?

## Vector Spaces and Subspaces

### 25: Identifying vector spaces and non-vector spaces {data-difficulty="3"}

::: {.callout-important collapse="true" appearance="minimal"}
#### Context
Understanding what constitutes a vector space is fundamental in linear algebra. A set must satisfy specific axioms to be considered a vector space: closure under addition and scalar multiplication, existence of zero element, existence of additive inverses, and several other properties.
:::

For each of the following sets, determine whether it is a vector space or not. If it is a vector space, prove it by verifying all the required axioms. If it is not a vector space, identify which axiom(s) fail and provide counterexamples.

a) $A = \left\{\begin{pmatrix} a \\ 0 \end{pmatrix} \mid a \in \mathbb{R}\right\}$ (vectors with second component zero)

b) $B = \left\{\begin{pmatrix} a \\ -a \end{pmatrix} \mid a \in \mathbb{R}\right\}$ (vectors where second component is negative of first)

c) $C = \mathbb{N}$ (the set of natural numbers)

d) $D = \left\{\begin{pmatrix} a \\ 1 \end{pmatrix} \mid a \in \mathbb{R}\right\}$ (vectors with second component always 1)

**Hint:** For the non-vector spaces, show that there are some "bad" elements such that if we add them or multiply with some number (not necessarily positive), the result would not belong to the set.


# üõ†Ô∏è ‘≥’∏÷Ä’Æ’∂’°’Ø’°’∂
- [üõ†Ô∏èüì∫ ‘≥’∏÷Ä’Æ’∂’°’Ø’°’∂’´ ’ø’•’Ω’°’£÷Ä’∏÷Ç’©’µ’∏÷Ç’∂’®](https://youtu.be/vectors_practical)
- [üõ†Ô∏èüóÇÔ∏è ‘≥’∏÷Ä’Æ’∂’°’Ø’°’∂’´ PDF-’®](Homeworks/hw_01_vectors_practical.pdf)

Practice problems include:
- Implementing vector operations in Python/NumPy
- Visualizing vectors and transformations
- Computing similarity matrices for recommendation systems
- PCA implementation on real datasets

::: {.callout-warning collapse="true" appearance="minimal"}
#### TODO: Python Implementation
**Word Similarity Search Exercise** - Create a hands-on Python exercise where students:
1. Load pre-trained word embeddings (e.g., GloVe or Word2Vec)
2. Implement cosine similarity and Euclidean distance functions
3. Build a word similarity search function that finds the N most similar words to a query
4. Compare results between different distance metrics
5. Visualize word embeddings in 2D using t-SNE or PCA
6. Apply to real NLP tasks like finding synonyms or analogy completion

This would complement Problem 04.5 (Word embeddings similarity) with practical implementation experience.
:::

# üé≤ 38 (01) TODO
- ‚ñ∂Ô∏è[3Blue1Brown - Essence of Linear Algebra](https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab)
- üîó[Linear Algebra Visualizations](https://www.3blue1brown.com/topics/linear-algebra)
- üá¶üá≤üé∂[’ç’∫’´’ø’°’Ø ‘æ’°’º’•÷Ä (’Ä’°’æ’•’ø)](https://www.youtube.com/watch?v=white_trees_forever)
- üåêüé∂[Radiohead (Everything In Its Right Place)](https://www.youtube.com/watch?v=onRk0sjSgFU)
- ü§å[‘ø’°÷Ä’£’´’∂]()
