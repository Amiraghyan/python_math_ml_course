---
title: "01 Vectors and Linear Algebra Fundamentals"
format:
  html:
    css: homework-styles.css
---

<script src="homework-scripts.js"></script>

![image.png](../background_photos/math_01_shinararutun.jpg)
[’¨’∏÷Ç’Ω’°’∂’Ø’°÷Ä’´ ’∞’≤’∏÷Ç’¥’®](https://unsplash.com/photos/black-and-yellow-crane-near-building-during-daytime-JcRhkLqvICA), ’Ä’•’≤’´’∂’°’Ø’ù [Suren Sargsyan](https://unsplash.com/@s_u_ren)
      

# üìö ’Ü’µ’∏÷Ç’©’®

- [üìö ‘±’¥’¢’∏’≤’ª’°’Ø’°’∂ ’∂’µ’∏÷Ç’©’®](01_vectors_linear_algebra.qmd)
- [üì∫ ’è’•’Ω’°’£÷Ä’∏÷Ç’©’µ’∏÷Ç’∂’®(ToDo)]()
- [üéûÔ∏è ’ç’¨’°’µ’§’•÷Ä - Vectors ToDo](Lectures/L01_Vectors.pdf)
- [üéûÔ∏è ’ç’¨’°’µ’§’•÷Ä - Geometry ToDo ](Lectures/L02_Geometry_of_Vectors__Matrices.pdf)
- [üõ†Ô∏èüì∫ ‘≥’∏÷Ä’Æ’∂’°’Ø’°’∂’´ ’ø’•’Ω’°’£÷Ä’∏÷Ç’©’µ’∏÷Ç’∂’® ToDo](https://youtu.be/vectors_practical)
- [üõ†Ô∏èüóÇÔ∏è ‘≥’∏÷Ä’Æ’∂’°’Ø’°’∂’´ PDF-’® ToDo](Homeworks/hw_01_vectors.pdf)
  

ToDo
‘±’µ’Ω ’§’°’Ω’´’∂ ’Ø’Æ’°’∂’∏’©’°’∂’°’∂÷Ñ ’æ’•’Ø’ø’∏÷Ä’∂’•÷Ä’´ ÷á ’¥’°’ø÷Ä’´÷Å’∂’•÷Ä’´ ’∞’´’¥’∏÷Ç’∂÷Ñ’∂’•÷Ä’´’∂’ù

1. ’é’•’Ø’ø’∏÷Ä’∂’•÷Ä (’£’∏÷Ä’Æ’∏’≤’∏÷Ç’©’µ’∏÷Ç’∂’∂’•÷Ä, ’Ω’Ø’°’¨’µ’°÷Ä ’°÷Ä’ø’°’§÷Ä’µ’°’¨, ’∂’∏÷Ä’¥’°)
2. ’é’•’Ø’ø’∏÷Ä’∂’•÷Ä’´ ’•÷Ä’Ø÷Ä’°’π’°÷É’∏÷Ç’©’µ’∏÷Ç’∂ (’°’∂’Ø’µ’∏÷Ç’∂’∂’•÷Ä, ’∏÷Ç’≤’≤’°’∞’°’µ’°÷Å’∏÷Ç’©’µ’∏÷Ç’∂)
3. ’Ñ’°’ø÷Ä’´÷Å’∂’•÷Ä ÷á ’£’Æ’°’µ’´’∂ ÷É’∏’≠’°’Ø’•÷Ä’∫’∏÷Ç’¥’∂’•÷Ä
4. ‘≥’Æ’°’µ’´’∂ ’∞’°’¥’°’Ø÷Å’∏÷Ç’©’µ’∏÷Ç’∂’∂’•÷Ä ÷á ’ø’°÷Ä’°’Æ’∏÷Ç’©’µ’∏÷Ç’∂’∂’•÷Ä

# üè° ’è’∂’°’µ’´’∂

::: {.callout-note collapse="false"}
1. ‚ùó‚ùó‚ùó DON'T CHECK THE SOLUTIONS BEFORE TRYING TO DO THE HOMEWORK BY YOURSELF‚ùó‚ùó‚ùó
2. Please don't hesitate to ask questions, never forget about the üçäkaralyoküçä principle!
3. The harder the problem is, the more üßÄcheesesüßÄ it has.
4. Problems with üéÅ are just extra bonuses. It would be good to try to solve them, but also it's not the highest priority task.
5. If the problem involve many boring calculations, feel free to skip them - important part is understanding the concepts.
6. Submit your solutions [here](https://forms.gle/CFEvNqFiTSsDLiFc6) (even if it's unfinished)
:::


## Vector Operations

### 01 RGB color mixing with vectors {data-difficulty="1"}

::: {.callout-tip collapse="true" appearance="minimal"}
#### Context
In computer graphics and image processing, colors can be represented as RGB vectors where each component (Red, Green, Blue) ranges from 0 to 255. Vector operations on these RGB values correspond to color mixing and transformations.
:::

Consider these RGB color vectors:

- Red: $\vec{r} = (255, 0, 0)$
- Cyan: $\vec{c} = (0, 255, 255)$

1. Calculate what color you get by adding red and cyan: $\vec{r} + \vec{c}$.
2. Find the "average" color between red and cyan: $\frac{1}{2}(\vec{r} + \vec{c})$.
3. Use a [color picker](https://share.google/yadDErXuKGKRwIHnq) to verify your answers from parts (a) and (b). What colors do you actually see?

### 02 Feature vector normalization {data-difficulty="2"}

::: {.callout-note collapse="true" appearance="minimal"}
#### Context
In machine learning, we often work with data that has very different scales - like comparing a person's age (around 20-80) with their salary (around 20,000-100,000). Without normalization (bringing all the values to a similar scale (e.g. having length of 1)), algorithms might think salary is much more important just because the numbers are bigger. Normalizing vectors to unit length helps ensure all features are treated equally.
:::

A customer is represented by the vector $\vec{v} = (25, 50000, 3)$ where components represent [age, income in $, number of purchases].

1. Calculate the Euclidean norm (magnitude) $||\vec{v}||_2$
2. Find the unit vector $\hat{v} = \frac{\vec{v}}{||\vec{v}||_2}$
3. Verify that $||\hat{v}||_2 = 1$

*Note:* No need to carry out the calculations explicitly.



### 03.5: k-Nearest Neighbors Classification {data-difficulty="3"}

::: {.callout-important collapse="true" appearance="minimal"}
#### Context
k-Nearest Neighbors (KNN) is a fundamental machine learning algorithm that classifies data points based on the class labels of their nearest neighbors. The algorithm relies entirely on distance calculations between feature vectors, making it a perfect application of vector operations.
:::

You have a 2D dataset for classifying emails as "spam" or "ham" (not spam) based on two features:
- **Feature 1**: Number of exclamation marks per 100 words
- **Feature 2**: Percentage of words in ALL CAPS

**Training data:**
- Spam: $S_1 = (8, 25)$, $S_2 = (12, 30)$, $S_3 = (6, 20)$
- Ham: $H_1 = (1, 5)$, $H_2 = (2, 8)$, $H_3 = (0, 3)$

**New email to classify:** $\vec{x} = (4, 15)$

a) **Distance Calculation:** Calculate the Euclidean distance from $\vec{x}$ to each of the 6 training points.

b) **3-NN Classification:** Using $k=3$, identify the 3 nearest neighbors to $\vec{x}$. What class would you assign to $\vec{x}$ based on majority voting?

c) **1-NN vs 5-NN:** What would be the classification if you used $k=1$? What about $k=5$? 

d) **Distance Metric Impact:** Repeat part (b) using Manhattan distance instead of Euclidean distance. Does the classification change?

e) **Discussion:** Why might the choice of $k$ and distance metric matter for KNN performance? What are potential advantages and disadvantages of KNN compared to other algorithms?

## Dot Products and Angles

### 04: Similarity measurement {data-difficulty="2"}

::: {.callout-important collapse="true" appearance="minimal"}
#### Context
The dot product is fundamental in measuring similarity between vectors. In recommendation systems, we often use cosine similarity (based on dot products) to find similar users or items.
:::

Two user preference vectors are $\vec{u_1} = (5, 3, 1, 4)$ and $\vec{u_2} = (3, 5, 2, 2)$ where each component represents rating for different movie genres.

a) Calculate the dot product $\vec{u_1} \cdot \vec{u_2}$
b) Calculate the cosine similarity: $\cos(\theta) = \frac{\vec{u_1} \cdot \vec{u_2}}{||\vec{u_1}|| \cdot ||\vec{u_2}||}$
c) What does a cosine similarity close to 1 indicate about user preferences?

### 04.5: Word embeddings similarity {data-difficulty="2"}
ToDo

::: {.callout-tip collapse="true" appearance="minimal"}
#### Context
In NLP, words can be represented as vectors called embeddings. By comparing these vectors using different distance metrics, we can determine semantic similarity between words. This is the foundation of modern language models and search engines.
:::

Given the following 2D word embeddings:
- **cheese:** $(1, 2)$
- **mushroom:** $(3, 1)$  
- **tasty:** $(2, 2)$

a) **Euclidean Distance Analysis:**
   - Compute the Euclidean distance between **tasty** and **cheese**
   - Compute the Euclidean distance between **tasty** and **mushroom**
   - Which word is closer to **tasty** based on Euclidean distance?

b) **Cosine Similarity Analysis:**
   - Compute the cosine similarity between **tasty** and **cheese**
   - Compute the cosine similarity between **tasty** and **mushroom**
   - Which word is closer to **tasty** based on cosine similarity?

c) **Discussion:** Compare the outcomes from parts (a) and (b). Why might one metric be preferred over the other in different NLP applications?

::: {.callout-note collapse="true" appearance="minimal"}
**Fun fact:** Check out this [3Blue1Brown video on word vectors](https://youtu.be/wjZofJX0v4M?t=751) for more insights!
:::


### 06: Finding perpendicular vectors {data-difficulty="2"}


Given the vector $\vec{v} = (2, 3)$:

1. Find a non-zero vector $\vec{w} = (x, y)$ such that $\vec{v}$ and $\vec{w}$ are perpendicular.
2. Verify that your chosen vector $\vec{w}$ satisfies $\vec{v} \cdot \vec{w} = 0$.
3. Find a unit vector in the direction of $\vec{w}$ by computing $\frac{\vec{w}}{||\vec{w}||}$.
4. Explain why there are infinitely many vectors perpendicular to $\vec{v}$ and describe the general form of all such vectors.



### 08: Deriving the cosine angle formula {data-difficulty="3"}

Derive the formula for the cosine of the angle between two vectors: $\cos(\theta) = \frac{\vec{a} \cdot \vec{b}}{||\vec{a}|| \cdot ||\vec{b}||}$

::: {.callout-warning collapse="true" appearance="minimal"}
#### Hint
Start with the law of cosines for a triangle: $c^2 = a^2 + b^2 - 2ab\cos(\theta)$. Consider a triangle formed by vectors $\vec{a}$, $\vec{b}$, and $\vec{a} - \vec{b}$. The side lengths are $||\vec{a}||$, $||\vec{b}||$, and $||\vec{a} - \vec{b}||$. Express $||\vec{a} - \vec{b}||^2$ using the dot product and substitute into the law of cosines.
:::

1. Write down the law of cosines for the triangle with sides $||\vec{a}||$, $||\vec{b}||$, and $||\vec{a} - \vec{b}||$
2. Express $||\vec{a} - \vec{b}||^2$ in terms of dot products by expanding $(\vec{a} - \vec{b}) \cdot (\vec{a} - \vec{b})$
3. Substitute your result from part (2) into the law of cosines and solve for $\cos(\theta)$
4. Verify your derived formula using vectors $\vec{u} = (3, 4)$ and $\vec{v} = (1, 0)$


## Geometric Interpretation

### 11: Triangle inequality {data-difficulty="2"}
For vectors $\vec{u} = (3, 4)$ and $\vec{v} = (5, -12)$:

a) Calculate $||\vec{u}||$, $||\vec{v}||$, and $||\vec{u} + \vec{v}||$
b) Verify the triangle inequality: $||\vec{u} + \vec{v}|| \leq ||\vec{u}|| + ||\vec{v}||$
c) When does equality hold in the triangle inequality?


### 18: Model selection with regularization {data-difficulty="3"}
::: {.callout-important collapse="true" appearance="minimal"}
#### Context
In machine learning, we constantly face a dilemma: should we use a complex model that fits our training data perfectly, or a simpler model that captures the general pattern? This is where **regularization** comes in.

Imagine you're Netflix trying to predict movie ratings. You could create an extremely complex formula with thousands of parameters that perfectly predicts every rating in your training data. But when a new user comes along, your model might fail spectacularly - it memorized the training data instead of learning the underlying patterns. This is called **overfitting**. ([Kargin example](https://www.youtube.com/watch?v=723rlQAhXqc))

**Regularization** prevents overfitting by adding a penalty for model complexity to our optimization goal:

$$\text{Total Error} = \text{Prediction Error} + \lambda \cdot \text{Complexity Penalty}$$

where $\lambda$ controls how much we penalize complexity (having large parameter values).

The two most common regularization methods use different norms to measure complexity:

- **L1 Regularization (Lasso)**: Uses the sum of absolute values
   $$\text{L1 penalty} = \lambda \sum_{i=1}^{n} |w_i|$$
   
- **L2 Regularization (Ridge)**: Uses the sum of squares
   $$\text{L2 penalty} = \lambda \sum_{i=1}^{n} w_i^2$$

**Real-world example:** Suppose you're predicting house prices using features like size, location, age, etc. Without regularization, your model might learn that "houses with exactly 2,347 sq ft, built in 1987, with 3.5 bathrooms, facing north-northeast, with blue doors" sell for $523,456. With regularization, it learns more general rules like "larger houses in good neighborhoods cost more."
:::

You're comparing two models that predict house prices:

- Model A: Complex formula with weights (coefficients) $\vec{w_A} = (10, -8)$ and prediction error = 100
- Model B: Simpler formula with weights $\vec{w_B} = (2, -1)$ and prediction error = 120

Model B makes slightly worse predictions, but which model is better when considering both accuracy and simplicity?

a) **L1 Regularization (Œª = 0.5):** Calculate the total error for each model
   - Model A: $\text{Error} + \lambda \cdot ||\vec{w_A}||_1 = ?$
   - Model B: $\text{Error} + \lambda \cdot ||\vec{w_B}||_1 = ?$

b) **L2 Regularization (Œª = 0.5):** Calculate the total error for each model
   - Model A: $\text{Error} + \lambda \cdot ||\vec{w_A}||_2^2 = ?$
   - Model B: $\text{Error} + \lambda \cdot ||\vec{w_B}||_2^2 = ?$

c) **Model Selection:** Which model would you choose under each regularization method? How does the choice of $\lambda$ affect your decision?
d) **Practical Insight:** In production systems, why might we prefer a model with slightly worse accuracy but much simpler weights?

### 21: High-dimensional vector geometry {.bonus-problem data-difficulty="3"}
In high-dimensional spaces (common in ML), our intuition about geometry can be misleading.

Consider the unit sphere in $\mathbb{R}^n$ (all vectors with norm 1):

a) In 2D, what fraction of a unit square $[-1,1] \times [-1,1]$ is occupied by the unit circle?
b) Estimate this fraction for a unit cube in 3D
c) Research: What happens to this fraction as the dimension $n$ increases? This is known as the "curse of dimensionality."

[Video](https://www.youtube.com/watch?v=9Tf-_mJhOkU)

# üõ†Ô∏è ‘≥’∏÷Ä’Æ’∂’°’Ø’°’∂
- [üõ†Ô∏èüì∫ ‘≥’∏÷Ä’Æ’∂’°’Ø’°’∂’´ ’ø’•’Ω’°’£÷Ä’∏÷Ç’©’µ’∏÷Ç’∂’®](https://youtu.be/vectors_practical)
- [üõ†Ô∏èüóÇÔ∏è ‘≥’∏÷Ä’Æ’∂’°’Ø’°’∂’´ PDF-’®](Homeworks/hw_01_vectors_practical.pdf)
:::

# üé≤ 38 (01) TODO
- ‚ñ∂Ô∏è[3Blue1Brown - Essence of Linear Algebra](https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab)
- üîó[Linear Algebra Visualizations](https://www.3blue1brown.com/topics/linear-algebra)
- üá¶üá≤üé∂[’ç’∫’´’ø’°’Ø ‘æ’°’º’•÷Ä (’Ä’°’æ’•’ø)](https://www.youtube.com/watch?v=white_trees_forever)
- üåêüé∂[Radiohead (Everything In Its Right Place)](https://www.youtube.com/watch?v=onRk0sjSgFU)
- ü§å[‘ø’°÷Ä’£’´’∂]()
