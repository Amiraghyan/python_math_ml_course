---
title: "01 Vectors and Linear Algebra Fundamentals"
format:
  html:
    css: homework-styles.css
---

<script src="homework-scripts.js"></script>

![image.png](../background_photos/math_01_vectors.jpg)
[’¨’∏÷Ç’Ω’°’∂’Ø’°÷Ä’´ ’∞’≤’∏÷Ç’¥’®](https://unsplash.com/), ’Ä’•’≤’´’∂’°’Ø’ù [Artist Name](https://unsplash.com/)


# üìö ’Ü’µ’∏÷Ç’©’®

- [üìö ‘±’¥’¢’∏’≤’ª’°’Ø’°’∂ ’∂’µ’∏÷Ç’©’®](01_vectors_linear_algebra.qmd)
- [üì∫ ’è’•’Ω’°’£÷Ä’∏÷Ç’©’µ’∏÷Ç’∂’®](https://youtu.be/vectors_lecture)
- [üéûÔ∏è ’ç’¨’°’µ’§’•÷Ä - Vectors](Lectures/L01_Vectors.pdf)
- [üéûÔ∏è ’ç’¨’°’µ’§’•÷Ä - Geometry](Lectures/L02_Geometry_of_Vectors__Matrices.pdf)
- [üõ†Ô∏èüì∫ ‘≥’∏÷Ä’Æ’∂’°’Ø’°’∂’´ ’ø’•’Ω’°’£÷Ä’∏÷Ç’©’µ’∏÷Ç’∂’®](https://youtu.be/vectors_practical)
- [üõ†Ô∏èüóÇÔ∏è ‘≥’∏÷Ä’Æ’∂’°’Ø’°’∂’´ PDF-’®](Homeworks/hw_01_vectors.pdf)
  
‘±’µ’Ω ’§’°’Ω’´’∂ ’Ø’Æ’°’∂’∏’©’°’∂’°’∂÷Ñ ’æ’•’Ø’ø’∏÷Ä’∂’•÷Ä’´ ÷á ’¥’°’ø÷Ä’´÷Å’∂’•÷Ä’´ ’∞’´’¥’∏÷Ç’∂÷Ñ’∂’•÷Ä’´’∂’ù

1. ’é’•’Ø’ø’∏÷Ä’∂’•÷Ä (’£’∏÷Ä’Æ’∏’≤’∏÷Ç’©’µ’∏÷Ç’∂’∂’•÷Ä, ’Ω’Ø’°’¨’µ’°÷Ä ’°÷Ä’ø’°’§÷Ä’µ’°’¨, ’∂’∏÷Ä’¥’°)
2. ’é’•’Ø’ø’∏÷Ä’∂’•÷Ä’´ ’•÷Ä’Ø÷Ä’°’π’°÷É’∏÷Ç’©’µ’∏÷Ç’∂ (’°’∂’Ø’µ’∏÷Ç’∂’∂’•÷Ä, ’∏÷Ç’≤’≤’°’∞’°’µ’°÷Å’∏÷Ç’©’µ’∏÷Ç’∂)
3. ’Ñ’°’ø÷Ä’´÷Å’∂’•÷Ä ÷á ’£’Æ’°’µ’´’∂ ÷É’∏’≠’°’Ø’•÷Ä’∫’∏÷Ç’¥’∂’•÷Ä
4. ‘≥’Æ’°’µ’´’∂ ’∞’°’¥’°’Ø÷Å’∏÷Ç’©’µ’∏÷Ç’∂’∂’•÷Ä ÷á ’ø’°÷Ä’°’Æ’∏÷Ç’©’µ’∏÷Ç’∂’∂’•÷Ä

# üè° ’è’∂’°’µ’´’∂

::: {.callout-note collapse="false"}
1. ‚ùó‚ùó‚ùó DON'T CHECK THE SOLUTIONS BEFORE TRYING TO DO THE HOMEWORK BY YOURSELF‚ùó‚ùó‚ùó
2. Please don't hesitate to ask questions, never forget about the üçäkaralyoküçä principle!
3. The harder the problem is, the more üßÄcheesesüßÄ it has.
4. Problems with üéÅ are just extra bonuses. It would be good to try to solve them, but also it's not the highest priority task.
5. If the problem involve many boring calculations, feel free to skip them - important part is understanding the concepts.
6. Submit your solutions [here](https://forms.gle/CFEvNqFiTSsDLiFc6) (even if it's unfinished)
:::


## Vector Operations

### 01: RGB color mixing with vectors {data-difficulty="1"}

::: {.callout-tip collapse="true" appearance="minimal"}
#### Context
In computer graphics and image processing, colors are represented as RGB vectors where each component (Red, Green, Blue) ranges from 0 to 255. Vector operations on these RGB values correspond to color mixing and transformations.
:::

Consider these RGB color vectors:
- Red: $\vec{r} = (255, 0, 0)$
- Yellow: $\vec{y} = (255, 255, 0)$
- A custom color: $\vec{c} = (128, 64, 192)$

a) Calculate what color you get by adding red and yellow: $\vec{r} + \vec{y}$. What happens when RGB values exceed 255?

b) Find the "average" color between red and yellow: $\frac{1}{2}(\vec{r} + \vec{y})$

c) Use a color picker (like [Google's color picker](https://g.co/kgs/color-picker) or any online tool) to verify your answers from parts (a) and (b). What colors do you actually see?

d) Calculate $\vec{r} - \frac{1}{2}\vec{y}$. What does this operation represent in terms of color mixing?

### 02: Feature vector normalization {data-difficulty="2"}

::: {.callout-note collapse="true" appearance="minimal"}
#### Context
In ML preprocessing, we often normalize feature vectors to have unit length. This helps algorithms that are sensitive to the scale of input features, like k-nearest neighbors or neural networks.
:::

A customer profile is represented by the vector $\vec{v} = (25, 50000, 3)$ where components represent [age, income in $, number of purchases].

a) Calculate the Euclidean norm (magnitude) $||\vec{v}||_2$
b) Find the unit vector $\hat{v} = \frac{\vec{v}}{||\vec{v}||_2}$
c) Verify that $||\hat{v}||_2 = 1$

### 03: Distance between data points {data-difficulty="2"}
Two data points in a dataset are represented as $\vec{p_1} = (1, 3, -2)$ and $\vec{p_2} = (4, -1, 1)$.

a) Calculate the Euclidean distance between these points
b) Calculate the Manhattan distance (L1 norm of the difference)
c) Which distance metric would be more robust to outliers? Explain briefly.

## Dot Products and Angles

### 04: Similarity measurement {data-difficulty="2"}

::: {.callout-important collapse="true" appearance="minimal"}
#### Context
The dot product is fundamental in measuring similarity between vectors. In recommendation systems, we often use cosine similarity (based on dot products) to find similar users or items.
:::

Two user preference vectors are $\vec{u_1} = (5, 3, 1, 4)$ and $\vec{u_2} = (3, 5, 2, 2)$ where each component represents rating for different movie genres.

a) Calculate the dot product $\vec{u_1} \cdot \vec{u_2}$
b) Calculate the cosine similarity: $\cos(\theta) = \frac{\vec{u_1} \cdot \vec{u_2}}{||\vec{u_1}|| \cdot ||\vec{u_2}||}$
c) What does a cosine similarity close to 1 indicate about user preferences?

### 04.5: Word embeddings similarity {data-difficulty="2"}

::: {.callout-tip collapse="true" appearance="minimal"}
#### Context
In NLP, words can be represented as vectors called embeddings. By comparing these vectors using different distance metrics, we can determine semantic similarity between words. This is the foundation of modern language models and search engines.
:::

Given the following 2D word embeddings:
- **cheese:** $(1, 2)$
- **mushroom:** $(3, 1)$  
- **tasty:** $(2, 2)$

a) **Euclidean Distance Analysis:**
   - Compute the Euclidean distance between **tasty** and **cheese**
   - Compute the Euclidean distance between **tasty** and **mushroom**
   - Which word is closer to **tasty** based on Euclidean distance?

b) **Cosine Similarity Analysis:**
   - Compute the cosine similarity between **tasty** and **cheese**
   - Compute the cosine similarity between **tasty** and **mushroom**
   - Which word is closer to **tasty** based on cosine similarity?

c) **Discussion:** Compare the outcomes from parts (a) and (b). Why might one metric be preferred over the other in different NLP applications?

::: {.callout-note collapse="true" appearance="minimal"}
**Fun fact:** Check out this [3Blue1Brown video on word vectors](https://youtu.be/wjZofJX0v4M?t=751) for more insights!
:::

### 05: Matrix transformations {data-difficulty="2"}

What vectors do you get by applying the matrix $A = \begin{pmatrix} 3 & -3 \\ 3 & 3 \end{pmatrix}$ on the vectors:

a) $\vec{a} = \begin{pmatrix} 1 \\ 0 \end{pmatrix}$

b) $\vec{b} = \begin{pmatrix} 0 \\ 1 \end{pmatrix}$

c) $\vec{c} = \begin{pmatrix} 1 \\ 1 \end{pmatrix}$

d) (Additional) Draw the vectors before and after multiplying with $A$. What can you say visually about the matrix? Can you guess how it will act on the vector $\begin{pmatrix} 2 \\ -2 \end{pmatrix}$?

### 06: Finding perpendicular vectors {data-difficulty="2"}

::: {.callout-note collapse="true" appearance="minimal"}
#### Context
Finding perpendicular vectors is fundamental in many applications, from computer graphics (surface normals) to optimization (gradient descent directions) and data analysis (principal component analysis).
:::

Given the vector $\vec{v} = (2, 3)$:

a) Find a non-zero vector $\vec{w} = (x, y)$ such that $\vec{v}$ and $\vec{w}$ are perpendicular.

b) Verify that your chosen vector $\vec{w}$ satisfies $\vec{v} \cdot \vec{w} = 0$.

c) Find a unit vector in the direction of $\vec{w}$ by computing $\frac{\vec{w}}{||\vec{w}||}$.

d) Explain why there are infinitely many vectors perpendicular to $\vec{v}$ and describe the general form of all such vectors.

### 06.5: Orthogonality check {data-difficulty="1"}
Determine which pairs of vectors are orthogonal (perpendicular):

a) $\vec{a} = (1, 2, -1)$ and $\vec{b} = (2, -1, 0)$
b) $\vec{c} = (3, 4)$ and $\vec{d} = (-4, 3)$
c) $\vec{e} = (1, 1, 1)$ and $\vec{f} = (1, -2, 1)$

### 06: Orthogonality check {data-difficulty="1"}
Determine which pairs of vectors are orthogonal (perpendicular):

a) $\vec{a} = (1, 2, -1)$ and $\vec{b} = (2, -1, 0)$
b) $\vec{c} = (3, 4)$ and $\vec{d} = (-4, 3)$
c) $\vec{e} = (1, 1, 1)$ and $\vec{f} = (1, -2, 1)$

### 07: Matrix products {data-difficulty="2"}

Compute the following products:

a) $AB$, where $A = \begin{pmatrix} 6 & 5 \\ -2 & 7 \end{pmatrix}$, $B = \begin{pmatrix} -5 & 3 \\ 1 & 4 \end{pmatrix}$

b) $(A - B)(A + B)$, where $A = \begin{pmatrix} 2 & 2 & 4 \\ -3 & -2 & 4 \\ -2 & 0 & 2 \end{pmatrix}$, $B = \begin{pmatrix} 2 & 1 & 3 \\ -1 & 2 & 2 \\ 1 & 4 & -1 \end{pmatrix}$

c) $A^2 - B^2$, with the same $A$ and $B$ as in part (b).

### 08: Deriving the cosine angle formula {data-difficulty="3"}

Derive the formula for the cosine of the angle between two vectors: $\cos(\theta) = \frac{\vec{a} \cdot \vec{b}}{||\vec{a}|| \cdot ||\vec{b}||}$

::: {.callout-warning collapse="true" appearance="minimal"}
#### Hint
Start with the law of cosines for a triangle: $c^2 = a^2 + b^2 - 2ab\cos(\theta)$. Consider a triangle formed by vectors $\vec{a}$, $\vec{b}$, and $\vec{a} - \vec{b}$. The side lengths are $||\vec{a}||$, $||\vec{b}||$, and $||\vec{a} - \vec{b}||$. Express $||\vec{a} - \vec{b}||^2$ using the dot product and substitute into the law of cosines.
:::

a) Write down the law of cosines for the triangle with sides $||\vec{a}||$, $||\vec{b}||$, and $||\vec{a} - \vec{b}||$

b) Express $||\vec{a} - \vec{b}||^2$ in terms of dot products by expanding $(\vec{a} - \vec{b}) \cdot (\vec{a} - \vec{b})$

c) Substitute your result from part (b) into the law of cosines and solve for $\cos(\theta)$

d) Verify your derived formula using vectors $\vec{u} = (3, 4)$ and $\vec{v} = (1, 0)$

### 08: Deriving the cosine angle formula {data-difficulty="3"}

Derive the formula for the cosine of the angle between two vectors: $\cos(\theta) = \frac{\vec{a} \cdot \vec{b}}{||\vec{a}|| \cdot ||\vec{b}||}$

::: {.callout-warning collapse="true" appearance="minimal"}
#### Hint
Start with the law of cosines for a triangle: $c^2 = a^2 + b^2 - 2ab\cos(\theta)$. Consider a triangle formed by vectors $\vec{a}$, $\vec{b}$, and $\vec{a} - \vec{b}$. The side lengths are $||\vec{a}||$, $||\vec{b}||$, and $||\vec{a} - \vec{b}||$. Express $||\vec{a} - \vec{b}||^2$ using the dot product and substitute into the law of cosines.
:::

a) Write down the law of cosines for the triangle with sides $||\vec{a}||$, $||\vec{b}||$, and $||\vec{a} - \vec{b}||$

b) Express $||\vec{a} - \vec{b}||^2$ in terms of dot products by expanding $(\vec{a} - \vec{b}) \cdot (\vec{a} - \vec{b})$

c) Substitute your result from part (b) into the law of cosines and solve for $\cos(\theta)$

d) Verify your derived formula using vectors $\vec{u} = (3, 4)$ and $\vec{v} = (1, 0)$

### 09: Shear matrix transformations {data-difficulty="2"}

Consider the following matrix (it is called the shear matrix): $S = \begin{pmatrix} 1 & 1 \\ 0 & 1 \end{pmatrix}$

a) What would you get if you apply $S$ on the vector $\begin{pmatrix} 0 \\ 1 \end{pmatrix}$?

b) What would you get if you apply $S$ again on the result of the previous point?

c) What if you apply $S$ one more time?

d) What do you think happens when we apply $S$ 100 times on that vector?

e) Can you compute $S^{100}$?

### 10: Projection and components {data-difficulty="3"}
Given vectors $\vec{a} = (4, 3)$ and $\vec{b} = (1, 2)$:

a) Find the projection of $\vec{a}$ onto $\vec{b}$: $\text{proj}_{\vec{b}}\vec{a} = \frac{\vec{a} \cdot \vec{b}}{||\vec{b}||^2}\vec{b}$
b) Find the component of $\vec{a}$ perpendicular to $\vec{b}$
c) Verify that $\vec{a} = \text{proj}_{\vec{b}}\vec{a} + \vec{a}_{\perp}$

## Geometric Interpretation

### 08: Triangle inequality {data-difficulty="2"}
For vectors $\vec{u} = (3, 4)$ and $\vec{v} = (5, -12)$:

a) Calculate $||\vec{u}||$, $||\vec{v}||$, and $||\vec{u} + \vec{v}||$
b) Verify the triangle inequality: $||\vec{u} + \vec{v}|| \leq ||\vec{u}|| + ||\vec{v}||$
c) When does equality hold in the triangle inequality?

### 09: Cross product applications {data-difficulty="3"}

::: {.callout-note collapse="true" appearance="minimal"}
#### Context
Cross products are used in computer graphics for calculating surface normals, determining the orientation of objects, and computing areas of parallelograms.
:::

Given vectors $\vec{a} = (2, 1, -1)$ and $\vec{b} = (1, 3, 2)$:

a) Calculate the cross product $\vec{a} \times \vec{b}$
b) Verify that $\vec{a} \times \vec{b}$ is orthogonal to both $\vec{a}$ and $\vec{b}$
c) Find the area of the parallelogram spanned by $\vec{a}$ and $\vec{b}$

## Linear Combinations and Spans

### 10: Linear combinations {data-difficulty="2"}
Given vectors $\vec{v_1} = (1, 2, 1)$, $\vec{v_2} = (2, -1, 3)$, and $\vec{v_3} = (1, -4, 1)$:

a) Express $\vec{w} = (5, 0, 7)$ as a linear combination of $\vec{v_1}$ and $\vec{v_2}$ if possible
b) Can $\vec{v_3}$ be written as a linear combination of $\vec{v_1}$ and $\vec{v_2}$?
c) What does it mean geometrically if three vectors are linearly dependent?

### 11: Basis vectors {data-difficulty="2"}
Consider the vectors $\vec{e_1} = (1, 0, 0)$, $\vec{e_2} = (0, 1, 0)$, and $\vec{e_3} = (0, 0, 1)$:

a) Express the vector $\vec{v} = (7, -3, 5)$ in terms of $\vec{e_1}$, $\vec{e_2}$, and $\vec{e_3}$
b) Why are $\vec{e_1}$, $\vec{e_2}$, and $\vec{e_3}$ called the standard basis vectors for $\mathbb{R}^3$?
c) Can any vector in $\mathbb{R}^3$ be expressed uniquely as a linear combination of these basis vectors?

### 12: Span and linear independence {data-difficulty="3"}
Determine whether the following sets of vectors span $\mathbb{R}^3$ and whether they are linearly independent:

a) $\{(1, 0, 1), (0, 1, 1), (1, 1, 0)\}$
b) $\{(1, 2, 3), (2, 4, 6), (0, 1, 2)\}$
c) $\{(1, 0, 0), (1, 1, 0), (1, 1, 1), (0, 1, 1)\}$

### 13: Principal Component Analysis (PCA) intuition {data-difficulty="3"}

::: {.callout-important collapse="true" appearance="minimal"}
#### Context
PCA is a dimensionality reduction technique that finds the directions (principal components) along which data varies the most. These directions are eigenvectors of the covariance matrix.
:::

Given a 2D dataset with points: $(1, 1)$, $(2, 2)$, $(3, 3)$, $(1, 3)$, $(3, 1)$:

a) Calculate the mean vector $\vec{\mu}$ of the dataset
b) Center the data by subtracting the mean from each point
c) Which direction would you expect the first principal component to point? Explain intuitively.

### 14: Vector in machine learning cost functions {data-difficulty="3"}

::: {.callout-tip collapse="true" appearance="minimal"}
#### Context
In linear regression, we minimize the sum of squared errors. This can be expressed elegantly using vector notation, where the error vector's magnitude represents the total cost.
:::

In a simple linear regression with predictions $\vec{y}_{pred} = (2.1, 4.8, 6.9, 9.2)$ and actual values $\vec{y}_{true} = (2, 5, 7, 9)$:

a) Calculate the error vector $\vec{e} = \vec{y}_{true} - \vec{y}_{pred}$
b) Calculate the mean squared error: $MSE = \frac{1}{n}||\vec{e}||^2$
c) How does minimizing $||\vec{e}||^2$ relate to finding the best-fit line?

### 15: Model selection with regularization {data-difficulty="3"}

::: {.callout-important collapse="true" appearance="minimal"}
#### Context
In machine learning, complex models with large weights can "memorize" the training data instead of learning generalizable patterns. This leads to overfitting - the model performs well on training data but poorly on new data. Regularization helps by penalizing large weight values, encouraging simpler models that generalize better. L1 regularization (Lasso) uses the Manhattan norm and can drive some weights to zero, while L2 regularization (Ridge) uses the Euclidean norm and keeps all weights small. The total loss = prediction error + Œª √ó regularization penalty.
:::

Two models have learned different weight vectors:
- Model A: $\vec{w_A} = (2.5, -1.8, 0.3, 4.1)$
- Model B: $\vec{w_B} = (1.2, -0.9, 0.7, 2.3)$

Both models achieve the same prediction error of $E = 12.5$ on the validation set.

a) Calculate the L1 regularization term for each model: $R_{L1} = ||\vec{w}||_1 = \sum_i |w_i|$

b) Calculate the L2 regularization term for each model: $R_{L2} = ||\vec{w}||_2^2 = \sum_i w_i^2$

c) For regularization parameter $\lambda = 0.1$, calculate the total regularized loss for each model using both L1 and L2 regularization:
   - $Loss_{L1} = E + \lambda \cdot R_{L1}$  
   - $Loss_{L2} = E + \lambda \cdot R_{L2}$

d) Which model would you choose under L1 regularization? Which under L2 regularization? Explain the difference.

### 16: Vector spaces {data-difficulty="2"}

::: {.callout-note collapse="true" appearance="minimal"}
#### Context
A vector space is a set of vectors that is closed under vector addition and scalar multiplication, and satisfies certain axioms. Understanding vector spaces is crucial for linear algebra and provides the foundation for many ML concepts like feature spaces and function spaces.
:::

Consider the set $S = \{(a, 2a, 3a) : a \in \mathbb{R}\}$ (all vectors of the form $(a, 2a, 3a)$ where $a$ is any real number).

a) Show that $S$ is closed under vector addition by taking two arbitrary vectors from $S$ and showing their sum is also in $S$.

b) Show that $S$ is closed under scalar multiplication by taking an arbitrary vector from $S$ and an arbitrary scalar.

c) Does $S$ form a vector space? What about the zero vector requirement?

d) Give a geometric interpretation of what the set $S$ represents in $\mathbb{R}^3$.

### 17: Vector subspaces {data-difficulty="3"}

::: {.callout-important collapse="true" appearance="minimal"}
#### Context
A vector subspace is a subset of a vector space that is itself a vector space. In machine learning, feature subspaces are often used for dimensionality reduction, and understanding subspaces helps with concepts like the null space and column space of matrices.
:::

Determine which of the following sets are vector subspaces of $\mathbb{R}^3$:

a) $V_1 = \{(x, y, z) : x + y + z = 0\}$ (vectors whose components sum to zero)

b) $V_2 = \{(x, y, z) : x + y + z = 1\}$ (vectors whose components sum to one)

c) $V_3 = \{(x, 0, z) : x, z \in \mathbb{R}\}$ (vectors with zero as the middle component)

d) $V_4 = \{(x, y, z) : x^2 + y^2 + z^2 \leq 1\}$ (vectors inside or on the unit sphere)

For each set, check the three subspace requirements: (1) contains zero vector, (2) closed under addition, (3) closed under scalar multiplication.

### 18: High-dimensional vector geometry {.bonus-problem data-difficulty="3"}
In high-dimensional spaces (common in ML), our intuition about geometry can be misleading.

Consider the unit sphere in $\mathbb{R}^n$ (all vectors with norm 1):

a) In 2D, what fraction of a unit square $[-1,1] \times [-1,1]$ is occupied by the unit circle?
b) Estimate this fraction for a unit cube in 3D
c) Research: What happens to this fraction as the dimension $n$ increases? This is known as the "curse of dimensionality."

## Matrix Transformations

### 19: Matrix transformations and geometric interpretation {data-difficulty="2"}

What vectors do you get by applying the matrix $A = \begin{pmatrix} 3 & -3 \\ 3 & 3 \end{pmatrix}$ on the vectors:

a) $\vec{a} = \begin{pmatrix} 1 \\ 0 \end{pmatrix}$

b) $\vec{b} = \begin{pmatrix} 0 \\ 1 \end{pmatrix}$

c) $\vec{c} = \begin{pmatrix} 1 \\ 1 \end{pmatrix}$

**Additional:** Draw the vectors before and after multiplying with $A$. What can you say visually about the matrix? Can you guess how it will act on the vector $\begin{pmatrix} 2 \\ -2 \end{pmatrix}$?

### 20: Matrix operations {data-difficulty="2"}

Compute the following products:

a) $AB$, where $A = \begin{pmatrix} 6 & 5 \\ -2 & 7 \end{pmatrix}$, $B = \begin{pmatrix} -5 & 3 \\ 1 & 4 \end{pmatrix}$

b) $(A - B)(A + B)$, where $A = \begin{pmatrix} 2 & 2 & 4 \\ -3 & -2 & 4 \\ -2 & 0 & 2 \end{pmatrix}$, $B = \begin{pmatrix} 2 & 1 & 3 \\ -1 & 2 & 2 \\ 1 & 4 & -1 \end{pmatrix}$

c) $A^2 - B^2$, with the same $A$ and $B$ as in part (b).

### 21: Shear matrix analysis {.bonus-problem data-difficulty="3"}

::: {.callout-note collapse="true" appearance="minimal"}
#### Context
Shear transformations are commonly used in computer graphics for creating italic text effects, perspective corrections, and geometric distortions. They preserve area but change angles and shapes.
:::

Consider the shear matrix $S = \begin{pmatrix} 1 & 1 \\ 0 & 1 \end{pmatrix}$:

a) What would you get if you apply $S$ on the vector $\begin{pmatrix} 0 \\ 1 \end{pmatrix}$?

b) What would you get if you apply $S$ again on the result of the previous point?

c) What if you apply $S$ one more time?

d) What do you think happens when we apply $S$ 100 times on that vector?

e) Can you compute $S^{100}$?

### 21.5: Diagonal matrix powers {data-difficulty="2"}

::: {.callout-note collapse="true" appearance="minimal"}
#### Context
Diagonal matrices are particularly useful in linear algebra because their powers are easy to compute. This property is extensively used in eigenvalue decomposition and diagonalization of matrices.
:::

Consider the diagonal matrix $A = \begin{pmatrix} 2 & 0 \\ 0 & -1 \end{pmatrix}$.

a) Compute $A^2$, $A^3$, and $A^4$.

b) Find a general formula for $A^n$ where $n$ is any positive integer.

c) What does this transformation represent geometrically? How does it affect the unit circle when applied repeatedly?

d) **Bonus:** What happens when you apply this transformation to the vector $\begin{pmatrix} 1 \\ 1 \end{pmatrix}$ multiple times?

## Vector Spaces and Subspaces

### 22: Identifying vector spaces and non-vector spaces {data-difficulty="3"}

::: {.callout-important collapse="true" appearance="minimal"}
#### Context
Understanding what constitutes a vector space is fundamental in linear algebra. A set must satisfy specific axioms to be considered a vector space: closure under addition and scalar multiplication, existence of zero element, existence of additive inverses, and several other properties.
:::

For each of the following sets, determine whether it is a vector space or not. If it is a vector space, prove it by verifying all the required axioms. If it is not a vector space, identify which axiom(s) fail and provide counterexamples.

a) $A = \left\{\begin{pmatrix} a \\ 0 \end{pmatrix} \mid a \in \mathbb{R}\right\}$ (vectors with second component zero)

b) $B = \left\{\begin{pmatrix} a \\ -a \end{pmatrix} \mid a \in \mathbb{R}\right\}$ (vectors where second component is negative of first)

c) $C = \mathbb{N}$ (the set of natural numbers)

d) $D = \left\{\begin{pmatrix} a \\ 1 \end{pmatrix} \mid a \in \mathbb{R}\right\}$ (vectors with second component always 1)

**Hint:** For the non-vector spaces, show that there are some "bad" elements such that if we add them or multiply with some number (not necessarily positive), the result would not belong to the set.


# üõ†Ô∏è ‘≥’∏÷Ä’Æ’∂’°’Ø’°’∂
- [üõ†Ô∏èüì∫ ‘≥’∏÷Ä’Æ’∂’°’Ø’°’∂’´ ’ø’•’Ω’°’£÷Ä’∏÷Ç’©’µ’∏÷Ç’∂’®](https://youtu.be/vectors_practical)
- [üõ†Ô∏èüóÇÔ∏è ‘≥’∏÷Ä’Æ’∂’°’Ø’°’∂’´ PDF-’®](Homeworks/hw_01_vectors_practical.pdf)

Practice problems include:
- Implementing vector operations in Python/NumPy
- Visualizing vectors and transformations
- Computing similarity matrices for recommendation systems
- PCA implementation on real datasets

# üé≤ 38 (01)
- ‚ñ∂Ô∏è[3Blue1Brown - Essence of Linear Algebra](https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab)
- üîó[Linear Algebra Visualizations](https://www.3blue1brown.com/topics/linear-algebra)
- üá¶üá≤üé∂[’ç’∫’´’ø’°’Ø ‘æ’°’º’•÷Ä (’Ä’°’æ’•’ø)](https://www.youtube.com/watch?v=white_trees_forever)
- üåêüé∂[Radiohead (Everything In Its Right Place)](https://www.youtube.com/watch?v=onRk0sjSgFU)
- ü§å[’é’•’Ø’ø’∏÷Ä’°’Ø’°’∂ ’∫’°÷Ä’ø’•’¶](https://www.youtube.com/watch?v=vector_garden)
