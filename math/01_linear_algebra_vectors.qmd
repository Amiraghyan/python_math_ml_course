---
title: "01 Vectors and Linear Algebra Fundamentals"
format:
  html:
    css: homework-styles.css
---

<script src="homework-scripts.js"></script>

![image.png](../background_photos/math_01_shinararutun.jpg)
[’¨’∏÷Ç’Ω’°’∂’Ø’°÷Ä’´ ’∞’≤’∏÷Ç’¥’®](https://unsplash.com/photos/black-and-yellow-crane-near-building-during-daytime-JcRhkLqvICA), ’Ä’•’≤’´’∂’°’Ø’ù [Suren Sargsyan](https://unsplash.com/@s_u_ren)
      

# üìö ’Ü’µ’∏÷Ç’©’®

- [üìö ‘±’¥’¢’∏’≤’ª’°’Ø’°’∂ ’∂’µ’∏÷Ç’©’®](01_vectors_linear_algebra.qmd)
- [üì∫ ’è’•’Ω’°’£÷Ä’∏÷Ç’©’µ’∏÷Ç’∂’® (ToDo)]()
- [üéûÔ∏è ’ç’¨’°’µ’§’•÷Ä - Vectors ToDo](Lectures/L01_Vectors.pdf)
- [üéûÔ∏è ’ç’¨’°’µ’§’•÷Ä - Geometry ToDo ](Lectures/L02_Geometry_of_Vectors__Matrices.pdf)
- [üõ†Ô∏èüì∫ ‘≥’∏÷Ä’Æ’∂’°’Ø’°’∂’´ ’ø’•’Ω’°’£÷Ä’∏÷Ç’©’µ’∏÷Ç’∂’® ToDo](https://youtu.be/vectors_practical)
- [üõ†Ô∏èüóÇÔ∏è ‘≥’∏÷Ä’Æ’∂’°’Ø’°’∂’´ PDF-’® ToDo](Homeworks/hw_01_vectors.pdf)
  

ToDo

# üè° ’è’∂’°’µ’´’∂

::: {.callout-note collapse="false"}
1. ‚ùó‚ùó‚ùó DON'T CHECK THE SOLUTIONS BEFORE TRYING TO DO THE HOMEWORK BY YOURSELF‚ùó‚ùó‚ùó
2. Please don't hesitate to ask questions, never forget about the üçäkaralyoküçä principle!
3. The harder the problem is, the more üßÄcheesesüßÄ it has.
4. Problems with üéÅ are just extra bonuses. It would be good to try to solve them, but also it's not the highest priority task.
5. If the problem involve many boring calculations, feel free to skip them - important part is understanding the concepts.
6. Submit your solutions [here](https://forms.gle/CFEvNqFiTSsDLiFc6) (even if it's unfinished)
:::


## Vector Operations

### 01 RGB color mixing with vectors {data-difficulty="1"}

::: {.callout-tip collapse="true" appearance="minimal"}
#### Context
In computer graphics and image processing, colors can be represented as RGB vectors where each component (Red, Green, Blue) ranges from 0 to 255. Vector operations on these RGB values correspond to color mixing and transformations.
:::

Consider these RGB color vectors:

- Red: $\vec{r} = (255, 0, 0)$
- Cyan: $\vec{c} = (0, 255, 255)$

1. Calculate what color you get by adding red and cyan: $\vec{r} + \vec{c}$.
2. Find the "average" color between red and cyan: $\frac{1}{2}(\vec{r} + \vec{c})$.
3. Use a [color picker](https://share.google/yadDErXuKGKRwIHnq) to verify your answers from parts (a) and (b). What colors do you actually see?

### 02 Dot product {data-difficulty="1"}
A translation office translated $a = [24, 17, 9, 13]$ documents from English,
French, German and Russian, respectively. For each of those languages, it takes about
$b = [5, 10, 11, 7]$ minutes to translate one page.
How much time did they spend translating in total? How much did each of the translators
spend on average if there are 4 translators in the office? Write an expression for this amount
in terms of the vectors $a$ and $b$.


### 03 Feature vector normalization {data-difficulty="2"}

::: {.callout-note collapse="true" appearance="minimal"}
#### Context
In machine learning, we often work with data that has very different scales - like comparing a person's age (around 20-80) with their salary (around 20,000-100,000). Without normalization (bringing all the values to a similar scale (e.g. having length of 1)), algorithms might think salary is much more important just because the numbers are bigger. Normalizing vectors to unit length helps ensure all features are treated equally.
:::

A customer is represented by the vector $\vec{v} = (25, 50000, 3)$ where components represent [age, income in $, number of purchases].

1. Calculate the Euclidean norm (magnitude) $||\vec{v}||_2$
2. Find the unit vector $\hat{v} = \frac{\vec{v}}{||\vec{v}||_2}$
3. Verify that $||\hat{v}||_2 = 1$

*Note:* No need to carry out the calculations explicitly.



### 04 Triangle inequality {data-difficulty="2"}
For vectors $\vec{u} = (3, 4)$ and $\vec{v} = (5, -12)$:

a) Calculate $||\vec{u}||$, $||\vec{v}||$, and $||\vec{u} + \vec{v}||$
b) Verify the triangle inequality: $||\vec{u} + \vec{v}|| \leq ||\vec{u}|| + ||\vec{v}||$
c) When does equality hold in the triangle inequality?


### 05 Model selection with regularization {data-difficulty="2"}
::: {.callout-important collapse="true" appearance="minimal"}
#### Context
In machine learning, we constantly face a tradeoff: should we use a complex model that fits our training data very well, or a simpler model that captures the general pattern? This is where **regularization** comes in.

Imagine you're Netflix trying to predict movie ratings. You could create an extremely complex formula with thousands of parameters that perfectly predicts every rating in your training data. But when a new user comes along, your model might fail spectacularly - it memorized the training data instead of learning the underlying patterns. This is called **overfitting**. ([Kargin example](https://www.youtube.com/watch?v=723rlQAhXqc))

**Regularization** prevents overfitting by adding a penalty for model complexity to our optimization goal:

$$\text{Total Error} = \text{Prediction Error} + \lambda \cdot \text{Complexity Penalty}$$

where $\lambda$ controls how much we penalize complexity (having large parameter values).

The two most common regularization methods use different norms to measure complexity:

- **L1 Regularization (Lasso)**: Uses the sum of absolute values
   $$\text{L1 penalty} = \lambda \sum_{i=1}^{n} |w_i|$$
   
- **L2 Regularization (Ridge)**: Uses the sum of squares
   $$\text{L2 penalty} = \lambda \sum_{i=1}^{n} w_i^2$$

**Real-world example:** Suppose you're predicting house prices using features like size, location, age, etc. Without regularization, your model might learn that "houses with exactly 2,347 sq ft, built in 1987, with 3.5 bathrooms, facing north-northeast, with blue doors" sell for $523,456. With regularization, it learns more general rules like "larger houses in good neighborhoods cost more."
:::

’é’Ω’ø’°’∞ ’π’•’¥ ’∏÷Ä ’¨’°’æ ’•’¥ ’±÷á’°’Ø’•÷Ä’∫’•’¨ (’∞’°’ø’Ø’°’∫’•’Ω) ’ß’Ω ’≠’∂’§’´÷Ä’® , ’•’©’• ’∞’°÷Ä÷Å’•÷Ä ’¨’´’∂’•’∂’ù ’≠’°’¢’°÷Ä ’°÷Ä’•÷Ñ÷â


You're comparing two models that predict house prices:

- Model A: Complex formula with weights (coefficients) $\vec{w_A} = (10, -8, 4)$ (this can correspond to equation ($10x^2 - 8x + 4$ (quadratic))) and prediction error = 100
- Model B: Simpler formula with weights $\vec{w_B} = (0.1, -3, 1)$ $(0.1x^2 - 3x + 1)$ (almost just a linear function) and prediction error = 120

Model B makes slightly worse predictions, but which model is better when considering both error and simplicity?

a) **L1 Regularization (Œª = 0.5):** Calculate the total error for each model
   - Model A: $\text{Error} + \lambda \cdot ||\vec{w_A}||_1 = ?$
   - Model B: $\text{Error} + \lambda \cdot ||\vec{w_B}||_1 = ?$

b) **L2 Regularization (Œª = 0.5):** Calculate the total error for each model
   - Model A: $\text{Error} + \lambda \cdot ||\vec{w_A}||_2^2 = ?$
   - Model B: $\text{Error} + \lambda \cdot ||\vec{w_B}||_2^2 = ?$

c) **Model Selection:** Which model would you choose under each regularization method? How does the choice of $\lambda$ affect your decision?
d) **Practical Insight:** In production systems, why might we prefer a model with slightly worse accuracy but much simpler weights?



### 06 k-Nearest Neighbors Classification {data-difficulty="3"}


‘ø÷Å’æ’°’Æ ’Ø’£’ø’∂’•÷Ñ [csv ÷Ü’°’µ’¨](assets/knn.csv) ’•÷Ä’•÷Ñ ’Ω’µ’∏÷Ç’∂’∏’æ’ù feature_1, feature_2, label÷â 
‘ø’°÷Ä’∏’≤ ’•÷Ñ ’∫’°’ø’Ø’•÷Ä’°÷Å’∂’•’¨  ’∏÷Ä feature_1-’® ’´÷Ä’°’∂’´÷Å ’∂’•÷Ä’Ø’°’µ’°÷Å’∂’∏÷Ç’¥ ’° ’Æ’°’≤’Ø’´ ’¢’°÷Ä’±÷Ä’∏÷Ç’©’µ’∏÷Ç’∂’®, feature_2-’®’ù ’¨’°’µ’∂’∏÷Ç’©’µ’∏÷Ç’∂’® ’∏÷Ç label (’∫’´’ø’°’Ø’®)  ’∂’•÷Ä’Ø’°’µ’°÷Å’∂’∏÷Ç’¥ ’° ’©’• 4 ’Æ’°’≤’Ø’´ ’ø’•’Ω’°’Ø’∂’•÷Ä’´÷Å (0,1,2,3) ’∏÷Ä ’¥’•’Ø’∂ ’°÷â

 ’ä’•’ø÷Ñ ’° ’Ω’ø’•’≤’Æ’•’¨ ’¥’∏’§’•’¨ (’°’¨’£’∏÷Ä’´’©’¥) ’∏÷Ä’® ’Ω’ø’°’∂’°’¨’∏’æ feature_1, feature_2 ’°÷Ä’™’•÷Ñ’∂’•÷Ä’® ’Ø’£’∏÷Ç’∑’°’Ø’´ ’Æ’°’≤’Ø’´ ’ø’•’Ω’°’Ø’®÷â 

’Ä’•’ø÷á’µ’°’¨ ’Ø’•÷Ä’∫’∏’æ’ù ’∂’∏÷Ä ’Æ’°’≤’Ø’´ ’∞’°’¥’°÷Ä ’£’ø’∂’•’¨ K ’∞’°’ø ’°’¥’•’∂’°’¥’∏’ø’´’Ø ’Æ’°’≤’´’Ø’∂’•÷Ä’® ’¥’•÷Ä ’∏÷Ç’∂’•÷Å’°’Æ ’ø’æ’µ’°’¨’∂’•÷Ä’´÷Å ’∏÷Ç ’∂’°’µ’•’¨ ’©’• ’ß’§ k ’∞’°÷Ä÷á’°’∂’∂’•÷Ä’´÷Å ’∏÷Ä ’ø’•’Ω’°’Ø’´ ’Æ’°’≤’´’Ø’∂ ’° ’£’•÷Ä’°’Ø’∑’º’∏÷Ç’¥’ù ’∏÷Ç ’§’° ÷Ö’£’ø’°’£’∏÷Ä’Æ’•’¨ ’∏÷Ä’∫’•’Ω ’£’∏÷Ç’∑’°’Ø’∏÷Ç’©’µ’∏÷Ç’∂, 

’Ä’•’º’°’æ’∏÷Ä’∏÷Ç’©’µ’∏÷Ç’∂ ’∏÷Ä’∫’•’Ω ÷Ö’£’ø’°’£’∏÷Ä’Æ’•÷Ñ ’¥’´ ’§’•’∫÷Ñ’∏÷Ç’¥ L1-’® (Manhattan), ’¥’´ ’§’•’∫÷Ñ’∏÷Ç’¥ L2-’® (Euclidean): K-’´ ’∞’°’¥’°÷Ä ’ß’¨ ’ø’°÷Ä’¢’•÷Ä ’°÷Ä’™’•÷Ñ’∂’•÷Ä ’¢’¶’¢’°÷Å’•÷Ñ’ù 2,3, 5, 10 .

‘π’•’©÷á ’∞’°’æ’•’¨’µ’°’¨ ’∂’∑’∏÷Ç’¥’∂’•÷Ä
1. ‘±’¨’£’∏÷Ä’´’¥’©’´ ’°’∂’∏÷Ç’∂’∂ ’° K Nearest Neighbors ’∏÷Ç ’¶’∏÷Ç’ø "’°’Ω’° ’´’∂’± ’∏’æ÷Ñ’•÷Ä ’•’∂ ÷Ñ’∏ ’®’∂’Ø’•÷Ä’∂’•÷Ä’®, ’•’Ω ’Ø’°’Ω’•’¥ ’∏’æ ’•’Ω ’§’∏÷Ç" ’Ω’Ø’¶’¢’∏÷Ç’¥÷Ñ’∏’æ ’° ’°’∑’≠’°’ø’∏÷Ç’¥, ’∫÷Ä’°’Ø’ø’´’Ø’°’µ’∏÷Ç’¥ ’∞’°’¥’°÷Ä’µ’° ’•÷Ä’¢’•÷Ñ ’π’´ ÷Ö’£’ø’°’£’∏÷Ä’Æ’æ’∏÷Ç’¥ ’¢’°’µ÷Å ’ø’∂’°’µ’´’∂’´ ’∞’°’¥’°÷Ä ’Ø’°÷Ä’° ’∞’°’æ’•’Ω ’¨’´’∂’´ 
2. ’ä’°’ø’≥’°’º’∂’•÷Ä’´÷Å ’¥’•’Ø’® ’©’• ’´’∂’π’´ ’π’´ ÷Ö’£’ø’°’£’∏÷Ä’Æ’æ’∏÷Ç’¥ ’§’° "’â’°÷É’∏’≤’°’Ø’°’∂’∏÷Ç’©’µ’°’∂ ’°’∂’•’Æ÷Ñ’∂" ’° (Curse of dimenionality), ’∑’°’ø ’∞’°’æ’•’Ω ’ß÷Ü’•’Ø’ø ’° ’®’Ω’ø ’∏÷Ä’´ ’•÷Ä’¢ ’£’∏÷Ä’Æ ’•’∂÷Ñ ’∏÷Ç’∂’•’∂’∏÷Ç’¥ ’¢’°÷Ä’±÷Ä ’π’°÷É’°’∂’´ ’ø’°÷Ä’°’Æ’∏÷Ç’©’µ’∏÷Ç’∂’∂’•÷Ä’´ ’∞’•’ø, ’ø’æ’µ’°’¨’∂’•÷Ä’® ’∞’´’¥’∂’°’Ø’°’∂’∏÷Ç’¥ ’´÷Ä’°÷Ä’´÷Å ’∞’°’¥’°÷Ä’µ’° ’∞’°’æ’°’Ω’°÷Ä’°’∞’•’º ’•’∂ ’§’°’º’∂’∏÷Ç’¥ ’∏÷Ç ’°’∂’Ø’µ’∏÷Ç’∂’∂’•÷Ä’∏÷Ç’¥ ’•’∂ ’Ø’∏÷Ç’ø’°’Ø’æ’∏÷Ç’¥ (’°’µ’¨ ’Ø’•÷Ä’∫ ’°’Ω’°’Æ’ù ’•’©’• ’¢’°÷Ä’±÷Ä’°’π’°÷É ’∂’°÷Ä’´’∂’ª’® ’Ø’¨’∫’•’∂÷Ñ’ù ’ø’°’Ø’® ’¢’°’∂ ’π’´ ’¥’∂’°)÷â ‘±’≤’¢’µ’∏÷Ç÷Ä (https://slds-lmu.github.io/i2ml/chapters/14_cod/)

## ’Ñ’∂’°÷Å’°’Æ’® ’Ø’°÷Ä’°÷Ñ ’°’∂’ø’•’Ω’•÷Ñ ’§’•’º

### 04: Similarity measurement {data-difficulty="2"}

::: {.callout-important collapse="true" appearance="minimal"}
#### Context
The dot product is fundamental in measuring similarity between vectors. In recommendation systems, we often use cosine similarity (based on dot products) to find similar users or items.
:::

Two user preference vectors are $\vec{u_1} = (5, 3, 1, 4)$ and $\vec{u_2} = (3, 5, 2, 2)$ where each component represents rating for different movie genres.

a) Calculate the dot product $\vec{u_1} \cdot \vec{u_2}$
b) Calculate the cosine similarity: $\cos(\theta) = \frac{\vec{u_1} \cdot \vec{u_2}}{||\vec{u_1}|| \cdot ||\vec{u_2}||}$
c) What does a cosine similarity close to 1 indicate about user preferences?

### 04.5: Word embeddings similarity {data-difficulty="2"}
ToDo

::: {.callout-note collapse="true" appearance="minimal"}
 Check out this [3Blue1Brown video on word vectors](https://youtu.be/wjZofJX0v4M?t=751) for more insights!
:::


### 06: Finding perpendicular vectors {data-difficulty="2"}


Given the vector $\vec{v} = (2, 3)$:

1. Find a non-zero vector $\vec{w} = (x, y)$ such that $\vec{v}$ and $\vec{w}$ are perpendicular.
2. Verify that your chosen vector $\vec{w}$ satisfies $\vec{v} \cdot \vec{w} = 0$.
3. Find a unit vector in the direction of $\vec{w}$ by computing $\frac{\vec{w}}{||\vec{w}||}$.
4. Explain why there are infinitely many vectors perpendicular to $\vec{v}$ and describe the general form of all such vectors.



### 08: Deriving the cosine angle formula {data-difficulty="3"}

Derive the formula for the cosine of the angle between two vectors: $\cos(\theta) = \frac{\vec{a} \cdot \vec{b}}{||\vec{a}|| \cdot ||\vec{b}||}$

::: {.callout-warning collapse="true" appearance="minimal"}
#### Hint
Start with the law of cosines for a triangle: $c^2 = a^2 + b^2 - 2ab\cos(\theta)$. Consider a triangle formed by vectors $\vec{a}$, $\vec{b}$, and $\vec{a} - \vec{b}$. The side lengths are $||\vec{a}||$, $||\vec{b}||$, and $||\vec{a} - \vec{b}||$. Express $||\vec{a} - \vec{b}||^2$ using the dot product and substitute into the law of cosines.
:::

1. Write down the law of cosines for the triangle with sides $||\vec{a}||$, $||\vec{b}||$, and $||\vec{a} - \vec{b}||$
2. Express $||\vec{a} - \vec{b}||^2$ in terms of dot products by expanding $(\vec{a} - \vec{b}) \cdot (\vec{a} - \vec{b})$
3. Substitute your result from part (2) into the law of cosines and solve for $\cos(\theta)$
4. Verify your derived formula using vectors $\vec{u} = (3, 4)$ and $\vec{v} = (1, 0)$


## Geometric Interpretation


### 21: High-dimensional vector geometry {.bonus-problem data-difficulty="3"}
In high-dimensional spaces (common in ML), our intuition about geometry can be misleading.

Consider the unit sphere in $\mathbb{R}^n$ (all vectors with norm 1):

a) In 2D, what fraction of a unit square $[-1,1] \times [-1,1]$ is occupied by the unit circle?
b) Estimate this fraction for a unit cube in 3D
c) Research: What happens to this fraction as the dimension $n$ increases? This is known as the "curse of dimensionality."

[Video](https://www.youtube.com/watch?v=9Tf-_mJhOkU)

# üõ†Ô∏è ‘≥’∏÷Ä’Æ’∂’°’Ø’°’∂ ToDo
- [üõ†Ô∏èüì∫ ‘≥’∏÷Ä’Æ’∂’°’Ø’°’∂’´ ’ø’•’Ω’°’£÷Ä’∏÷Ç’©’µ’∏÷Ç’∂’®]()
- [üõ†Ô∏èüóÇÔ∏è ‘≥’∏÷Ä’Æ’∂’°’Ø’°’∂’´ PDF-’®]()
:::

# üé≤ 38 (01) TODO
- ‚ñ∂Ô∏è[ToDo]()
- üîó[Random link]()
- üá¶üá≤üé∂[ToDo]()
- üåêüé∂[ToDo]()
- ü§å[‘ø’°÷Ä’£’´’∂]()
