---
title: "03 Linear Independence, Basis, and Systems"
format:
  html:
    css: homework-styles.css
---

<script src="homework-scripts.js"></script>

<!-- 
[image.png](../background_photos/)
[’¨’∏÷Ç’Ω5. Verify the rank-nullity theorem: $\dim(\text{domain}) = \text{rank}(T) + \dim(\text{kernel}(T))$.



# üõ†Ô∏è ‘≥’∏÷Ä’Æ’∂’°’Ø’°’∂’∂’Ø’°÷Ä’´ ’∞’≤’∏÷Ç’¥’®](https://unsplash.com/photos/a-tall-building-with-lots-of-windows-and-balconies-AowELlZmpZM), ’Ä’•’≤’´’∂’°’Ø’ù [Gor Davtyan](https://unsplash.com/@gor918) -->

[image.png](../background_photos/math_03_gazananoc.jpg)
[’¨’∏÷Ç’Ω’°’∂’Ø’°÷Ä’´ ’∞’≤’∏÷Ç’¥’®](https://unsplash.com/photos/a-close-up-of-a-monkey-in-a-cage-mQF2vmyV0Zc), ‘≥’°’¶’°’∂’°’∂’∏÷Å, ’Ä’•’≤’´’∂’°’Ø’ù [Elmira Gokoryan](https://unsplash.com/@elmira)

      

# üìö ’Ü’µ’∏÷Ç’©’® ToDo

- [üìö ‘±’¥’¢’∏’≤’ª’°’Ø’°’∂ ’∂’µ’∏÷Ç’©’®]()
- [üì∫ ’è’•’Ω’°’£÷Ä’∏÷Ç’©’µ’∏÷Ç’∂’®]()
- [üéûÔ∏è ’ç’¨’°’µ’§’•÷Ä - ToDo](Lectures/L01_Vectors.pdf)
- [üéûÔ∏è ’ç’¨’°’µ’§’•÷Ä - Geometry](Lectures/L02_Geometry_of_Vectors__Matrices.pdf)
- [üõ†Ô∏èüì∫ ‘≥’∏÷Ä’Æ’∂’°’Ø’°’∂’´ ’ø’•’Ω’°’£÷Ä’∏÷Ç’©’µ’∏÷Ç’∂’®](https://youtu.be/vectors_practical)
- [üõ†Ô∏èüóÇÔ∏è ‘≥’∏÷Ä’Æ’∂’°’Ø’°’∂’´ PDF-’®](Homeworks/hw_01_vectors.pdf)
  


# üè° ’è’∂’°’µ’´’∂

::: {.callout-note collapse="false"}
1. ‚ùó‚ùó‚ùó DON'T CHECK THE SOLUTIONS BEFORE TRYING TO DO THE HOMEWORK BY YOURSELF‚ùó‚ùó‚ùó
2. Please don't hesitate to ask questions, never forget about the üçäkaralyoküçä principle!
3. The harder the problem is, the more üßÄcheesesüßÄ it has.
4. Problems with üéÅ are just extra bonuses. It would be good to try to solve them, but also it's not the highest priority task.
5. If the problem involve many boring calculations, feel free to skip them - important part is understanding the concepts.
6. Submit your solutions [here](https://forms.gle/CFEvNqFiTSsDLiFc6) (even if it's unfinished)
:::


## Systems of Linear Equations

### 01: GPS positioning - from 2D to 3D {data-difficulty="2"}

::: {.callout-note collapse="true" appearance="minimal"}
#### Context
GPS systems solve systems of equations to determine location. Understanding this process helps grasp how linear systems work in practice and why we need the right number of equations.
:::

**Part A: 2D Positioning (Easier warm-up)**

Imagine you're lost in a 2D world and receive distance signals from cell towers:
- Tower A at (0, 0): You are 5 units away
- Tower B at (6, 0): You are 3 units away

1. Write the system of equations for your position (x, y).
2. Solve it step by step using substitution or elimination.
3. Plot the circles and find their intersection point(s).

**Part B: 3D GPS Challenge**

Now for real GPS with 4 satellites in 3D space:
- Why do you need exactly 4 satellites for 3D positioning when you only need 2 towers for 2D?
- What happens if you only have 3 satellites? 
- What happens if you have 5 satellites and they give slightly different measurements?

### 02: Linear regression with normal equations {data-difficulty="2"}

::: {.callout-note collapse="true" appearance="minimal"}
#### Context
The normal equation provides a direct way to solve linear regression problems, connecting linear algebra to machine learning prediction tasks.
:::

You have the following data points for house size (x) vs price (y):
- (50, 100), (100, 180), (150, 280)

1. Set up the design matrix X (including the intercept column) and target vector y.
2. Solve the normal equation $\vec{\theta} = (X^T X)^{-1} X^T \vec{y}$ using Gaussian elimination.
3. Find the line equation $y = \theta_0 + \theta_1 x$.
4. Plot the data points and your fitted line.
5. Predict the price for a 120 square meter house.

### 03: The cheese shop multicollinearity problem {data-difficulty="2"}

::: {.callout-note collapse="true" appearance="minimal"}
#### Context
Real datasets often contain redundant features that are linear combinations of each other, causing problems in machine learning models.
:::

A cheese shop tracks the following features for each cheese:
- Price in AMD: $p_1$
- Price in USD: $p_2$ 
- Weight in kilograms: $w_1$
- Weight in pounds: $w_2$

Given the conversion rates: 1 USD = 400 AMD and 1 kg = 2.2 pounds.

1. **Linear dependence detection**: Which features are linearly dependent? Write the exact relationships.

2. **Matrix rank problem**: If you create a data matrix with these 4 features for 100 cheeses, what would be the maximum possible rank? Why?

3. **Feature selection**: Which 2 features should you keep to avoid multicollinearity? Explain your choice.

4. **System solvability**: If you try to fit a model using all 4 features, what problems might arise?

### 04: The image compression rank revelation {data-difficulty="3"}

::: {.callout-note collapse="true" appearance="minimal"}
#### Context
Understanding matrix rank helps explain how image compression works and why we can store images more efficiently without losing much visual quality.
:::

Consider a grayscale image represented as a matrix:
$$A = \begin{pmatrix} 100 & 150 & 200 \\ 200 & 300 & 400 \\ 50 & 75 & 100 \end{pmatrix}$$

1. **Rank analysis**: Calculate the rank of this image matrix. What does this tell you about the image structure?

2. **Compression potential**: If this represents a 3√ó3 pixel image, how many numbers do you actually need to store it losslessly? 

3. **Real-world connection**: Why do most natural images have much lower rank than their dimensions suggest? What does this mean for compression algorithms like JPEG?

4. **The storage paradox**: A 1000√ó1000 image should need 1,000,000 numbers, but often can be compressed to 50,000 numbers with minimal quality loss. How does rank theory explain this?

### 05: The social network influence model {data-difficulty="2"}

::: {.callout-note collapse="true" appearance="minimal"}
#### Context
Social networks can be modeled using linear systems where influence flows between users, helping understand how information spreads and how to detect influential users.
:::

In a small social network, user influence follows these relationships:
- Alice's influence = 0.5 √ó Bob's influence + 0.3 √ó Carol's influence + 2
- Bob's influence = 0.4 √ó Alice's influence + 0.2 √ó Carol's influence + 1  
- Carol's influence = 0.1 √ó Alice's influence + 0.6 √ó Bob's influence + 3

1. **System setup**: Write this as a matrix equation $A\vec{x} = \vec{b}$ where $\vec{x}$ represents the influence scores.

2. **Solution method**: Solve using Gaussian elimination to find each person's influence score.

3. **Stability analysis**: What happens if we change the external influence factors (the constants 2, 1, 3)? Does the system always have a unique solution?

4. **Network insights**: Who is the most influential person in this network? How do the coefficients relate to the network structure?

### 06: System consistency analysis {data-difficulty="2"}

::: {.callout-note collapse="true" appearance="minimal"}
#### Context
Understanding when systems have different numbers of solutions is crucial for determining model solvability and parameter sensitivity in machine learning applications.
:::

For which values of $a$ does the following system have 0, 1, or infinitely many solutions?

$$\begin{cases}
x + 2y + z = 1 \\
2x + 4y + az = 2 \\
-x - y + (a-1)z = 0
\end{cases}$$

1. **Setup**: Write the augmented matrix and begin row reduction.
2. **Parameter analysis**: Identify the critical values of $a$ where the system behavior changes.
3. **Classification**: For each case, determine whether the system has:
   - No solution (inconsistent)
   - Unique solution  
   - Infinitely many solutions
4. **Interpretation**: What do these different cases mean in a machine learning context?

### 07: Linear independence in polynomial space {data-difficulty="2"}

::: {.callout-note collapse="true" appearance="minimal"}
#### Context
Polynomial spaces are fundamental in feature engineering and basis function methods. Understanding linear independence in these spaces helps with feature selection and avoiding redundancy.
:::

Determine if the set $\{1 + t, 1 + 2t, 1 + t + t^2\} \subset P_2$ is linearly independent.

1. **Setup**: Write the linear combination equation $c_1(1 + t) + c_2(1 + 2t) + c_3(1 + t + t^2) = 0$.
2. **Coefficient matching**: Collect terms and set up the system for coefficients of $1$, $t$, and $t^2$.
3. **Solution analysis**: Solve the resulting system to determine if only the trivial solution exists.
4. **Dependency case**: If dependent, express one polynomial as a linear combination of the others.

### 08: Matrix subspaces and trace {data-difficulty="3"}

::: {.callout-note collapse="true" appearance="minimal"}
#### Context
Matrix subspaces appear frequently in machine learning, particularly in regularization techniques and constraint optimization. The trace operation is fundamental in many ML applications.
:::

Let $V = \{M \in M_{2 \times 2} : \text{tr}(M) = 0\}$ be the set of $2 \times 2$ matrices with zero trace.

**Part (a)**: Prove that $V$ is a subspace of $M_{2 \times 2}$.
1. Show that the zero matrix is in $V$.
2. Prove closure under addition: if $A, B \in V$, then $A + B \in V$.
3. Prove closure under scalar multiplication: if $A \in V$ and $c$ is a scalar, then $cA \in V$.

**Part (b)**: Find a basis and determine the dimension of $V$.
1. Write a general matrix in $V$ and identify the free parameters.
2. Construct a basis using simple matrices.
3. Verify linear independence and spanning.

### 09: Change of basis and coordinates {data-difficulty="3"}

::: {.callout-note collapse="true" appearance="minimal"}
#### Context
Change of basis is essential in machine learning for feature transformations, principal component analysis, and coordinate system rotations that can simplify problems.
:::

Let $B = \{(1,0,1), (0,1,1), (1,1,0)\}$ and $C = \{(1,0,0), (1,1,0), (1,1,1)\}$ be bases of $\mathbb{R}^3$.

**Part (a)**: Find the change-of-basis matrix $P_{B \to C}$.
1. Express each vector in basis $B$ as a linear combination of vectors in basis $C$.
2. Set up and solve the necessary systems of equations.
3. Construct the change-of-basis matrix from the coefficients.

**Part (b)**: Coordinate transformation.
If $[v]_B = (2, -1, 3)^T$, compute $[v]_C$.
1. Use the relationship $[v]_C = P_{B \to C} [v]_B$.
2. Verify your answer by computing the actual vector $v$ and expressing it in basis $C$.



# üõ†Ô∏è ‘≥’∏÷Ä’Æ’∂’°’Ø’°’∂
- [üõ†Ô∏èüì∫ ‘≥’∏÷Ä’Æ’∂’°’Ø’°’∂’´ ’ø’•’Ω’°’£÷Ä’∏÷Ç’©’µ’∏÷Ç’∂’®]()
- [üõ†Ô∏èüóÇÔ∏è ‘≥’∏÷Ä’Æ’∂’°’Ø’°’∂’´ PDF-’®]()

# üé≤ 40 (03)
- ‚ñ∂Ô∏è[]()
- üîó[Random link](https://www.youtube.com/watch?v=qTkpyUHBGDA)
- üá¶üá≤üé∂[]()
- üåêüé∂[Chet Baker (Almost blue)](https://www.youtube.com/watch?v=z4PKzz81m5c)
- ü§å[‘ø’°÷Ä’£’´’∂]()