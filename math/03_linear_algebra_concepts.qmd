---
title: "03 Linear Independence, Basis, SLE, Eigen-staff "
format:
  html:
    css: homework-styles.css
---

<script src="homework-scripts.js"></script>

<!-- 
[image.png](../background_photos/)
[’¨’∏÷Ç’Ω5. Verify the rank-nullity theorem: $\dim(\text{domain}) = \text{rank}(T) + \dim(\text{kernel}(T))$.



# üõ†Ô∏è ‘≥’∏÷Ä’Æ’∂’°’Ø’°’∂’∂’Ø’°÷Ä’´ ’∞’≤’∏÷Ç’¥’®](https://unsplash.com/photos/a-tall-building-with-lots-of-windows-and-balconies-AowELlZmpZM), ’Ä’•’≤’´’∂’°’Ø’ù [Gor Davtyan](https://unsplash.com/@gor918) -->

![image.png](../background_photos/math_03_gazananoc.jpg)
[’¨’∏÷Ç’Ω’°’∂’Ø’°÷Ä’´ ’∞’≤’∏÷Ç’¥’®](https://unsplash.com/photos/a-close-up-of-a-monkey-in-a-cage-mQF2vmyV0Zc), ‘≥’°’¶’°’∂’°’∂’∏÷Å, ’Ä’•’≤’´’∂’°’Ø’ù [Elmira Gokoryan](https://unsplash.com/@elmira)

      

# üìö ’Ü’µ’∏÷Ç’©’® ToDo

- [üìö ‘±’¥’¢’∏’≤’ª’°’Ø’°’∂ ’∂’µ’∏÷Ç’©’®]()
- [üì∫ ’è’•’Ω’°’£÷Ä’∏÷Ç’©’µ’∏÷Ç’∂’®]()
- [üéûÔ∏è ’ç’¨’°’µ’§’•÷Ä - ToDo](Lectures/L01_Vectors.pdf)
- [üéûÔ∏è ’ç’¨’°’µ’§’•÷Ä - Geometry](Lectures/L02_Geometry_of_Vectors__Matrices.pdf)
- [üõ†Ô∏èüì∫ ‘≥’∏÷Ä’Æ’∂’°’Ø’°’∂’´ ’ø’•’Ω’°’£÷Ä’∏÷Ç’©’µ’∏÷Ç’∂’®](https://youtu.be/vectors_practical)
- [üõ†Ô∏èüóÇÔ∏è ‘≥’∏÷Ä’Æ’∂’°’Ø’°’∂’´ PDF-’®](Homeworks/hw_01_vectors.pdf)
  


# üè° ’è’∂’°’µ’´’∂

::: {.callout-note collapse="false"}
1. ‚ùó‚ùó‚ùó DON'T CHECK THE SOLUTIONS BEFORE TRYING TO DO THE HOMEWORK BY YOURSELF‚ùó‚ùó‚ùó
2. Please don't hesitate to ask questions, never forget about the üçäkaralyoküçä principle!
3. The harder the problem is, the more üßÄcheesesüßÄ it has.
4. Problems with üéÅ are just extra bonuses. It would be good to try to solve them, but also it's not the highest priority task.
5. If the problem involve many boring calculations, feel free to skip them - important part is understanding the concepts.
6. Submit your solutions [here](https://forms.gle/CFEvNqFiTSsDLiFc6) (even if it's unfinished)
:::


## Systems of Linear Equations

### 01: GPS positioning - from 2D to 3D {data-difficulty="2"}

::: {.callout-note collapse="true" appearance="minimal"}
#### Context
GPS systems solve systems of equations to determine location. Understanding this process helps grasp how linear systems work in practice and why we need the right number of equations.
:::

**Part A: 2D Positioning (Easier warm-up)**

Imagine you're lost in a 2D world and receive distance signals from cell towers:
- Tower A at (0, 0): You are 5 units away
- Tower B at (6, 0): You are 3 units away

1. Write the system of equations for your position (x, y).
2. Solve it step by step using substitution or elimination.
3. Plot the circles and find their intersection point(s).

**Part B: 3D GPS Challenge**

Now for real GPS with 4 satellites in 3D space:
- Why do you need exactly 4 satellites for 3D positioning when you only need 2 towers for 2D?
- What happens if you only have 3 satellites? 


### 02: Linear regression with normal equations {data-difficulty="2"}

::: {.callout-note collapse="true" appearance="minimal"}
#### Context
The normal equation provides a direct way to solve linear regression problems, connecting linear algebra to machine learning prediction tasks.
:::

You have the following data points for house size (x) vs price (y):
- (50, 100), (100, 180), (150, 280)

1. Set up the design matrix X (including the intercept column) and target vector y.
2. Solve the normal equation $\vec{\theta} = (X^T X)^{-1} X^T \vec{y}$ (set up the system of linear equations and you can use [Gaussian elimination](https://www.youtube.com/watch?v=eDb6iugi6Uk)).
3. Find the line equation $y = \theta_0 + \theta_1 x$.
4. Plot the data points and your fitted line.
5. Predict the price for a 120 square meter house.

Hint for 1.:
We want to express calculating y (the price) as a matrix multiplication of X and theta. 

$$\vec{y} = X \vec{\theta}$$

Where $\vec{\theta} = \begin{pmatrix} \theta_0 \\ \theta_1 \end{pmatrix}$. In order for each $y_i = \theta_0 + \theta_1 x_i$, we set up the design matrix (X) as follows:
$$X = \begin{pmatrix} 1 & 50 \\ 1 & 100 \\ 1 & 150 \end{pmatrix}$$ 

Calculate the product $X \vec{\theta}$ to verify that each $y_i$ corresponds to plugging in $x_i$ into the line equation.

::: {.callout-tip collapse="true"}
#### Solution

**Step 1: Set up the design matrix X and target vector y**

For the data points (50, 100), (100, 180), (150, 280), we need to include an intercept column:

$$X = \begin{pmatrix} 1 & 50 \\ 1 & 100 \\ 1 & 150 \end{pmatrix}, \quad \vec{y} = \begin{pmatrix} 100 \\ 180 \\ 280 \end{pmatrix}$$

**Step 2: Solve the normal equation**

First, compute $X^T X$:
$$X^T = \begin{pmatrix} 1 & 1 & 1 \\ 50 & 100 & 150 \end{pmatrix}$$

$$X^T X = \begin{pmatrix} 1 & 1 & 1 \\ 50 & 100 & 150 \end{pmatrix} \begin{pmatrix} 1 & 50 \\ 1 & 100 \\ 1 & 150 \end{pmatrix} = \begin{pmatrix} 3 & 300 \\ 300 & 35000 \end{pmatrix}$$

Next, compute $X^T \vec{y}$:
$$X^T \vec{y} = \begin{pmatrix} 1 & 1 & 1 \\ 50 & 100 & 150 \end{pmatrix} \begin{pmatrix} 100 \\ 180 \\ 280 \end{pmatrix} = \begin{pmatrix} 560 \\ 59000 \end{pmatrix}$$

Now solve $(X^T X) \vec{\theta} = X^T \vec{y}$:
$$\begin{pmatrix} 3 & 300 \\ 300 & 35000 \end{pmatrix} \begin{pmatrix} \theta_0 \\ \theta_1 \end{pmatrix} = \begin{pmatrix} 560 \\ 59000 \end{pmatrix}$$

Using Gaussian elimination:
- From the first equation: $3\theta_0 + 300\theta_1 = 560$
- From the second equation: $300\theta_0 + 35000\theta_1 = 59000$

Multiply the first equation by 100: $300\theta_0 + 30000\theta_1 = 56000$

Subtract from the second equation: $5000\theta_1 = 3000$, so $\theta_1 = 0.6$

Substitute back: $3\theta_0 + 300(0.6) = 560$, so $3\theta_0 = 380$, giving $\theta_0 = 126.67$

**Step 3: Line equation**
$$y = 126.67 + 0.6x$$

**Step 4: Verification**
Check our solution with the data points:
- For x = 50: $y = 126.67 + 0.6(50) = 156.67$ (close to 100)
- For x = 100: $y = 126.67 + 0.6(100) = 186.67$ (close to 180)
- For x = 150: $y = 126.67 + 0.6(150) = 216.67$ (close to 280)

**Step 5: Prediction for 120 square meters**
$$y = 126.67 + 0.6(120) = 198.67$$

The predicted price for a 120 square meter house is approximately 198.67 (thousand units).
:::

### 03: The cheese shop multicollinearity problem {data-difficulty="2"}

::: {.callout-note collapse="true" appearance="minimal"}
#### Context
Real datasets often contain redundant features that are linear combinations of each other, causing problems in machine learning models.
:::

A cheese shop tracks the following features for each cheese:
- Price in AMD: $p_1$
- Price in USD: $p_2$ 
- Weight in kilograms: $w_1$
- Weight in pounds: $w_2$

Given the conversion rates: 1 USD = 400 AMD and 1 kg = 2.2 pounds.

1. **Linear dependence detection**: Which features are linearly dependent? Write the exact relationships.

2. **Matrix rank problem**: If you create a data matrix with these 4 features for 100 cheeses, what would be the maximum possible rank? Why?

3. **Feature selection**: Which 2 features should you keep to avoid multicollinearity? Explain your choice.

4. **System solvability**: If you try to fit a model using all 4 features, what problems might arise? *Hint:* Think about the normal equation and matrix invertibility.


### 04: System consistency analysis {data-difficulty="2"}


For which values of $a$ does the following system have 0, 1, or infinitely many solutions?

$$\begin{cases}
x + 2y + z = 1 \\
2x + 4y + az = 2 \\
-x - y + (a-1)z = 0
\end{cases}$$

### 05: Linear independence in polynomial space {data-difficulty="2"}

Determine if the set $\{1 + t, 1 + 2t, 1 + t + t^2\} \subset P_2$ is linearly independent. If dependent, express one polynomial as a linear combination of the others.

### 06: Matrix subspaces and trace {data-difficulty="3"}

Let $V = \{M \in M_{2 \times 2} : \text{tr}(M) = 0\}$ be the set of $2 \times 2$ matrices with zero trace.

1. Prove that $V$ is a subspace of $M_{2 \times 2}$.
2. Find a basis and determine the dimension of $V$.

### 07: Change of basis and coordinates {data-difficulty="3"}


Let $B = \{(3,1), (2,1)\}$ and $C = \{(1,1), (1,0)\}$ be bases of $\mathbb{R}^2$.

1. Find the change-of-basis matrix $P_{C \leftarrow B}$.
2. If $[v]_B = (1, 2)^T$, compute $[v]_C$. 

Hint: $[v]_B$ notation means that the coordinates of vector $v$ in basis $B$ are $(1, 2)^T$ e.g. $v = 1 \cdot (3,1) + 2 \cdot (2,1)$.

::: {.callout-tip collapse="true"}
#### Solution

**Part 1: Find the change-of-basis matrix $P_{C \leftarrow B}$**

The columns of the change-of-basis matrix $P_{C \leftarrow B}$ are the coordinate vectors of the basis vectors in $B$ relative to the basis $C$. Let $\vec{b_1} = (3,1)$, $\vec{b_2} = (2,1)$, and $\vec{c_1} = (1,1)$, $\vec{c_2} = (1,0)$.

We need to express $\vec{b_1}$ and $\vec{b_2}$ as linear combinations of $\vec{c_1}$ and $\vec{c_2}$.

For $\vec{b_1} = (3,1)$:
We need to find scalars $x_1, y_1$ such that $(3,1) = x_1(1,1) + y_1(1,0)$. This gives the system:
$$ \begin{cases} x_1 + y_1 = 3 \\ x_1 = 1 \end{cases} $$
Solving this system yields $x_1 = 1$ and $y_1 = 2$. So, $[\vec{b_1}]_C = \begin{pmatrix} 1 \\ 2 \end{pmatrix}$.

For $\vec{b_2} = (2,1)$:
We need to find scalars $x_2, y_2$ such that $(2,1) = x_2(1,1) + y_2(1,0)$. This gives the system:
$$ \begin{cases} x_2 + y_2 = 2 \\ x_2 = 1 \end{cases} $$
Solving this system yields $x_2 = 1$ and $y_2 = 1$. So, $[\vec{b_2}]_C = \begin{pmatrix} 1 \\ 1 \end{pmatrix}$.

Combining these column vectors gives the change-of-basis matrix:
$$ P_{C \leftarrow B} = \begin{pmatrix} 1 & 1 \\ 2 & 1 \end{pmatrix} $$

**Part 2: Compute $[v]_C$**

To find the coordinates of $v$ with respect to basis $C$, we use the formula:
$$ [v]_C = P_{C \leftarrow B} [v]_B $$

Given $[v]_B = \begin{pmatrix} 1 \\ 2 \end{pmatrix}$, we can compute:
$$ [v]_C = \begin{pmatrix} 1 & 1 \\ 2 & 1 \end{pmatrix} \begin{pmatrix} 1 \\ 2 \end{pmatrix} = \begin{pmatrix} 1 \cdot 1 + 1 \cdot 2 \\ 2 \cdot 1 + 1 \cdot 2 \end{pmatrix} = \begin{pmatrix} 1 + 2 \\ 2 + 2 \end{pmatrix} = \begin{pmatrix} 3 \\ 4 \end{pmatrix} $$

So, the coordinates of the vector $v$ in the basis $C$ are $(3, 4)^T$.
:::

### 08: Basis verification in different vector spaces {data-difficulty="2"}

In each case, determine whether $S$ is a basis for $V$:

**a)** $V = P_3$, $S = \{0, x, x^2, x^3\}$

**b)** $V = M_{2 \times 2}$, $S = \left\{I_2, \begin{pmatrix} 1 & 3 \\ 2 & 4 \end{pmatrix}, \begin{pmatrix} 1 & 2 \\ 3 & 4 \end{pmatrix}\right\}$

**c)** $V = \mathbb{R}^3$, $S = \{e_1, e_2, e_1 + 3e_2\}$

**d)** $V = P_1$, $S = \{1 - 2x, 2 - x\}$

### 09: Linear transformation from coordinates {data-difficulty="3"}

::: {.callout-note collapse="true" appearance="minimal"}
#### Context
Linear transformations can be uniquely determined by their values on a basis. This principle is fundamental in neural networks where transformations are defined by weight matrices.
:::

Suppose $T$ is a linear transformation from $\mathbb{R}^2$ to $P_2$ such that:
$$T\begin{pmatrix} -1 \\ 1 \end{pmatrix} = x + 3, \quad T\begin{pmatrix} 2 \\ 3 \end{pmatrix} = 2x^2 - x$$

Find $T\begin{pmatrix} a \\ b \end{pmatrix}$ for arbitrary $a, b \in \mathbb{R}$.

### 10: Geometric eigenvalue intuition {data-difficulty="2"}

::: {.callout-note collapse="true" appearance="minimal"}
#### Context
Understanding eigenvalues geometrically helps build intuition for principal component analysis, matrix factorizations, and understanding how transformations affect data.
:::

**Without doing any calculations**, determine the eigenvalues and eigenvectors for the following transformations by thinking about their geometric effects:

- **a) Shear matrix:** $S = \begin{pmatrix} 1 & 1 \\ 0 & 1 \end{pmatrix}$

- - What vectors remain on the same line after this transformation?
- - What vectors get stretched or shrunk (and by what factor)?

- **b) Rotation matrix:** $R = \begin{pmatrix} 0 & -1 \\ 1 & 0 \end{pmatrix}$ (90¬∞ counterclockwise)

- **c) Reflection matrix:** $F = \begin{pmatrix} 1 & 0 \\ 0 & -1 \end{pmatrix}$ (reflection across x-axis)

- **d) Scaling matrix:** $D = \begin{pmatrix} 3 & 0 \\ 0 & 2 \end{pmatrix}$

### 11: Matrix powers using eigenvalues {data-difficulty="3"}


Consider the matrix $A = \begin{pmatrix} 5 & 8 \\ 2 & 5 \end{pmatrix}$. We want to compute $A^{10}$ efficiently.

1. **Find eigenvalues and eigenvectors** of matrix $A$.

2. **Diagonalize the matrix**: Express $A = PDP^{-1}$ where $D$ is diagonal.

3. **Compute high powers efficiently**: Use diagonalization to find $A^{10}$ without multiplying the matrix 10 times.

4. **Pattern recognition**: What happens to $A^n$ as $n \to \infty$? Which eigenvalue dominates?

<!-- ### 12: Cayley-Hamilton theorem exploration {data-difficulty="3"}

::: {.callout-note collapse="true" appearance="minimal"}
#### Context
The Cayley-Hamilton theorem states that every matrix satisfies its own characteristic equation. This has profound implications for matrix computations and understanding polynomial relationships in linear algebra.
:::

Consider the matrix $B = \begin{pmatrix} 3 & 1 \\ 2 & 4 \end{pmatrix}$.

1. **Find the characteristic polynomial**: Compute $p(\lambda) = \det(B - \lambda I)$.

2. **The magic of Cayley-Hamilton**: According to the theorem, $p(B) = 0$ (the zero matrix). Verify this by substituting the matrix $B$ into its characteristic polynomial.

3. **Practical application**: Use the Cayley-Hamilton theorem to express $B^3$ in terms of lower powers of $B$.

4. **Matrix inverse shortcut**: If $B$ is invertible, use Cayley-Hamilton to find a formula for $B^{-1}$ without computing it directly.

5. **Theoretical insight**: Why does this theorem work? What does it tell us about the relationship between matrices and polynomials? -->

<!-- ### 13: Stationary distribution as an eigenvector {data-difficulty="3"}

::: {.callout-note collapse="true" appearance="minimal"}
#### Context
Markov chains are fundamental in machine learning (PageRank, hidden Markov models, MCMC sampling). The stationary distribution is the long-term behavior of the system and is found as the eigenvector corresponding to eigenvalue 1.
:::

A simplified weather model has two states: Sunny (S) and Rainy (R). The transition probability matrix is:

$$P = \begin{pmatrix} 0.9 & 0.1 \\ 0.2 & 0.8 \end{pmatrix}$$

where $P_{ij}$ is the probability of transitioning from state $i$ to state $j$.

**Part A: Understanding the stationary distribution**

1. **Set up the eigenvalue equation**: The stationary distribution $\vec{\pi}$ satisfies $\vec{\pi}^T P = \vec{\pi}^T$ (left eigenvector with eigenvalue 1).

2. **Solve the system**: Write this as $\vec{\pi}^T (P - I) = \vec{0}^T$ and solve using the constraint $\pi_1 + \pi_2 = 1$.

3. **Verify the given solution**: Check that $\vec{\pi} = [2/3, 1/3]$ indeed satisfies both conditions.

**Part B: Interpretation and long-term behavior**

4. **Physical meaning**: What does this stationary distribution tell you about the long-term weather pattern?

5. **Convergence**: Start with initial distribution $\vec{\pi}_0 = [1, 0]$ (100% sunny). Compute $\vec{\pi}_1 = \vec{\pi}_0 P$, $\vec{\pi}_2 = \vec{\pi}_1 P$, etc. How many steps does it take to get close to the stationary distribution?

6. **PageRank connection**: How does this relate to Google's PageRank algorithm? What do the "states" and "transitions" represent in that context?

**Part C: Matrix powers and dominant eigenvalue**

7. **Powers of P**: Compute $P^n$ for large $n$. What pattern do you observe in the rows?

8. **Eigenvalue dominance**: Find all eigenvalues of $P$. Why does the eigenvalue 1 dominate the long-term behavior? -->

<!-- ### 14: Optimization and positive definite matrices {data-difficulty="3"}

::: {.callout-note collapse="true" appearance="minimal"}
#### Context
Positive definite matrices are fundamental in optimization theory, machine learning, and statistics. They determine whether a critical point is a minimum, maximum, or saddle point, and appear in covariance matrices and neural network loss functions.
:::

Consider the quadratic function $f(x, y) = ax^2 + bxy + cy^2$ where the Hessian matrix is $H = \begin{pmatrix} 2a & b \\ b & 2c \end{pmatrix}$.

**Part A: Classification by eigenvalues**

1. **Positive definite case**: For $H = \begin{pmatrix} 4 & 2 \\ 2 & 6 \end{pmatrix}$:
   - Find the eigenvalues without full calculation (use trace and determinant)
   - Verify that both eigenvalues are positive
   - What does this tell you about the shape of the function $f(x,y)$?

2. **Negative definite case**: For $H = \begin{pmatrix} -4 & -1 \\ -1 & -2 \end{pmatrix}$:
   - Determine the signs of eigenvalues
   - Describe the shape of the corresponding quadratic function

3. **Indefinite case**: For $H = \begin{pmatrix} 2 & 3 \\ 3 & -1 \end{pmatrix}$:
   - Show that eigenvalues have different signs
   - What type of critical point does this represent?

**Part B: Machine learning applications**

4. **Covariance matrix**: A dataset has covariance matrix $C = \begin{pmatrix} 4 & 1 \\ 1 & 3 \end{pmatrix}$. 
   - Why must covariance matrices always be positive semidefinite?
   - Verify this property for matrix $C$

5. **Gradient descent convergence**: The loss function $L(\theta) = \theta^T A \theta - 2b^T\theta$ has Hessian $H = 2A$. For what conditions on $A$ will gradient descent converge to the global minimum? -->

### 15: From ellipse equation to eigenanalysis {data-difficulty="3"}

’ß’Ω ’¥’•’Ø’® ’®’∂’©’°÷Å’´’Ø ’° ’∏÷Ç’≤’≤’°’Ø’´, ’°’æ’•’¨’´ ’∑’∏÷Ç’ø ÷Ö’∫’ø’´’¥ ’°’∂’•’¨’∏÷Ç÷Å ’•’∂÷Ñ ’∂’°’µ’•’¨’∏÷Ç, ’¢’°’µ÷Å ’§’• ’Ω’ø’•’≤’´÷Å ’π’•’¥ ’ª’∂’ª’∏÷Ç’¥ ’∏÷Ä’∏’æ’∞’•’ø÷á ’£’∏÷Ç÷Å’• ’´’∂’π-’∏÷Ä ’¥’•’Ø’® ’∏÷Ç’¶’´ ’∞’´’¥’´’Ø’æ’°’∂’´÷Å ’ß’¨ ’¢’¶’¢’¶’°


You are given the equation of an ellipsoid

$$3x^2 + 2y^2 + 2xy$$

**Part A: Extract the matrix representation**

1. **Rewrite as quadratic form**: Express the equation in the form $\mathbf{v}^T A \mathbf{v}$ where $\mathbf{v} = \begin{pmatrix} x \\ y \end{pmatrix}$.

2. **Build the symmetric matrix**: What is the matrix $A$?

**Part B: Eigenanalysis**

3. **Find eigenvalues and eigenvectors**: Compute the eigenvalues $\lambda_1, \lambda_2$ and their corresponding eigenvectors $\mathbf{v}_1, \mathbf{v}_2$.

4. **Verify orthogonality**: Check that the eigenvectors are orthogonal to each other.

**Part C: Geometric interpretation**

5. **Principal axes**: The eigenvectors represent the principal axes of the ellipse. What are the directions of these axes in the plane?

<!-- 6. **Semi-axis lengths**: The semi-axis lengths are given by $a_i = \sqrt{\frac{6}{\lambda_i}}$. Calculate the two semi-axis lengths. -->

7. **Sketch and describe**: 
   - Which axis is the major axis (longer)? Which is the minor axis (shorter)?
   - How is the ellipse oriented relative to the standard x-y coordinate axes?
   - What's the angle of rotation from the standard axes?

<!-- **Part D: Coordinate transformation**

8. **Diagonalization**: Express the ellipse equation in the principal coordinate system where the matrix becomes diagonal.

9. **Standard form**: In the new coordinates $\mathbf{u} = P^T \mathbf{x}$ (where $P$ is the matrix of eigenvectors), the equation becomes:
   $$\lambda_1 u_1^2 + \lambda_2 u_2^2 = 6$$
   
   Verify this transformation and explain why this form is much simpler. -->

<!-- **Part E: Plotting and visualization**

10. **Plot the ellipsoid**: 
    - Sketch the original ellipsoid $3x^2 + 2y^2 + 2xy$
    - Mark the principal axes (eigenvector directions)
    - Compare with the "standard" ellipse $\frac{u_1^2}{a_1^2} + \frac{u_2^2}{a_2^2} = 1$ in the rotated coordinates -->

# üõ†Ô∏è ‘≥’∏÷Ä’Æ’∂’°’Ø’°’∂ ToDo
- [üõ†Ô∏èüì∫ ‘≥’∏÷Ä’Æ’∂’°’Ø’°’∂’´ ’ø’•’Ω’°’£÷Ä’∏÷Ç’©’µ’∏÷Ç’∂’®]()
- [üõ†Ô∏èüóÇÔ∏è ‘≥’∏÷Ä’Æ’∂’°’Ø’°’∂’´ PDF-’®]()

# üé≤ 40 (03)
- ‚ñ∂Ô∏è[]()
- üîó[Random link](https://www.youtube.com/watch?v=qTkpyUHBGDA)
- üá¶üá≤üé∂[Rozen Tal (‘∫’°’¥’°’∂’°’Ø’∂ ’ß)](https://www.youtube.com/watch?v=ngcI8vx64f0)
- üåêüé∂[Chet Baker (Almost blue)](https://www.youtube.com/watch?v=z4PKzz81m5c)
- ü§å[‘ø’°÷Ä’£’´’∂ ToDo]()