### 05: Matrix transformations {data-difficulty="2"}

What vectors do you get by applying the matrix $A = \begin{pmatrix} 3 & -3 \\ 3 & 3 \end{pmatrix}$ on the vectors:

a) $\vec{a} = \begin{pmatrix} 1 \\ 0 \end{pmatrix}$

b) $\vec{b} = \begin{pmatrix} 0 \\ 1 \end{pmatrix}$

c) $\vec{c} = \begin{pmatrix} 1 \\ 1 \end{pmatrix}$

d) (Additional) Draw the vectors before and after multiplying with $A$. What can you say visually about the matrix? Can you guess how it will act on the vector $\begin{pmatrix} 2 \\ -2 \end{pmatrix}$?


### 07: Matrix products {data-difficulty="2"}

Compute the following products:

a) $AB$, where $A = \begin{pmatrix} 6 & 5 \\ -2 & 7 \end{pmatrix}$, $B = \begin{pmatrix} -5 & 3 \\ 1 & 4 \end{pmatrix}$

b) $(A - B)(A + B)$, where $A = \begin{pmatrix} 2 & 2 & 4 \\ -3 & -2 & 4 \\ -2 & 0 & 2 \end{pmatrix}$, $B = \begin{pmatrix} 2 & 1 & 3 \\ -1 & 2 & 2 \\ 1 & 4 & -1 \end{pmatrix}$

c) $A^2 - B^2$, with the same $A$ and $B$ as in part (b).



### 09: Shear matrix transformations {data-difficulty="2"}

Consider the following matrix (it is called the shear matrix): $S = \begin{pmatrix} 1 & 1 \\ 0 & 1 \end{pmatrix}$

a) What would you get if you apply $S$ on the vector $\begin{pmatrix} 0 \\ 1 \end{pmatrix}$?

b) What would you get if you apply $S$ again on the result of the previous point?

c) What if you apply $S$ one more time?

d) What do you think happens when we apply $S$ 100 times on that vector?

e) Can you compute $S^{100}$?



## Linear Combinations and Spans

### 13: Linear combinations {data-difficulty="2"}
Given vectors $\vec{v_1} = (1, 2, 1)$, $\vec{v_2} = (2, -1, 3)$, and $\vec{v_3} = (1, -4, 1)$:

a) Express $\vec{w} = (5, 0, 7)$ as a linear combination of $\vec{v_1}$ and $\vec{v_2}$ if possible
b) Can $\vec{v_3}$ be written as a linear combination of $\vec{v_1}$ and $\vec{v_2}$?
c) What does it mean geometrically if three vectors are linearly dependent?

### 14: Basis vectors {data-difficulty="2"}
Consider the vectors $\vec{e_1} = (1, 0, 0)$, $\vec{e_2} = (0, 1, 0)$, and $\vec{e_3} = (0, 0, 1)$:

a) Express the vector $\vec{v} = (7, -3, 5)$ in terms of $\vec{e_1}$, $\vec{e_2}$, and $\vec{e_3}$
b) Why are $\vec{e_1}$, $\vec{e_2}$, and $\vec{e_3}$ called the standard basis vectors for $\mathbb{R}^3$?
c) Can any vector in $\mathbb{R}^3$ be expressed uniquely as a linear combination of these basis vectors?

### 15: Span and linear independence {data-difficulty="3"}
Determine whether the following sets of vectors span $\mathbb{R}^3$ and whether they are linearly independent:

a) $\{(1, 0, 1), (0, 1, 1), (1, 1, 0)\}$
b) $\{(1, 2, 3), (2, 4, 6), (0, 1, 2)\}$
c) $\{(1, 0, 0), (1, 1, 0), (1, 1, 1), (0, 1, 1)\}$

### 16: Principal Component Analysis (PCA) intuition {data-difficulty="3"}

::: {.callout-important collapse="true" appearance="minimal"}
#### Context
PCA is a dimensionality reduction technique that finds the directions (principal components) along which data varies the most. These directions are eigenvectors of the covariance matrix.
:::

Given a 2D dataset with points: $(1, 1)$, $(2, 2)$, $(3, 3)$, $(1, 3)$, $(3, 1)$:

a) Calculate the mean vector $\vec{\mu}$ of the dataset
b) Center the data by subtracting the mean from each point
c) Which direction would you expect the first principal component to point? Explain intuitively.

### 17: Vector in machine learning cost functions {data-difficulty="3"}

::: {.callout-tip collapse="true" appearance="minimal"}
#### Context
In linear regression, we minimize the sum of squared errors. This can be expressed elegantly using vector notation, where the error vector's magnitude represents the total cost.
:::

In a simple linear regression with predictions $\vec{y}_{pred} = (2.1, 4.8, 6.9, 9.2)$ and actual values $\vec{y}_{true} = (2, 5, 7, 9)$:

a) Calculate the error vector $\vec{e} = \vec{y}_{true} - \vec{y}_{pred}$
b) Calculate the mean squared error: $MSE = \frac{1}{n}||\vec{e}||^2$
c) How does minimizing $||\vec{e}||^2$ relate to finding the best-fit line?



### 19: Vector spaces {data-difficulty="2"}

::: {.callout-note collapse="true" appearance="minimal"}
#### Context
A vector space is a set of vectors that is closed under vector addition and scalar multiplication, and satisfies certain axioms. Understanding vector spaces is crucial for linear algebra and provides the foundation for many ML concepts like feature spaces and function spaces.
:::

Consider the set $S = \{(a, 2a, 3a) : a \in \mathbb{R}\}$ (all vectors of the form $(a, 2a, 3a)$ where $a$ is any real number).

a) Show that $S$ is closed under vector addition by taking two arbitrary vectors from $S$ and showing their sum is also in $S$.

b) Show that $S$ is closed under scalar multiplication by taking an arbitrary vector from $S$ and an arbitrary scalar.

c) Does $S$ form a vector space? What about the zero vector requirement?

d) Give a geometric interpretation of what the set $S$ represents in $\mathbb{R}^3$.

### 20: Vector subspaces {data-difficulty="3"}


Determine which of the following sets are vector subspaces of $\mathbb{R}^3$:

a) $V_1 = \{(x, y, z) : x + y + z = 0\}$ (vectors whose components sum to zero)

b) $V_2 = \{(x, y, z) : x + y + z = 1\}$ (vectors whose components sum to one)

c) $V_3 = \{(x, 0, z) : x, z \in \mathbb{R}\}$ (vectors with zero as the middle component)

d) $V_4 = \{(x, y, z) : x^2 + y^2 + z^2 \leq 1\}$ (vectors inside or on the unit sphere)

For each set, check the three subspace requirements: (1) contains zero vector, (2) closed under addition, (3) closed under scalar multiplication.


## Matrix Transformations

### 22: Matrix transformations and geometric interpretation {data-difficulty="2"}

What vectors do you get by applying the matrix $A = \begin{pmatrix} 3 & -3 \\ 3 & 3 \end{pmatrix}$ on the vectors:

a) $\vec{a} = \begin{pmatrix} 1 \\ 0 \end{pmatrix}$

b) $\vec{b} = \begin{pmatrix} 0 \\ 1 \end{pmatrix}$

c) $\vec{c} = \begin{pmatrix} 1 \\ 1 \end{pmatrix}$

**Additional:** Draw the vectors before and after multiplying with $A$. What can you say visually about the matrix? Can you guess how it will act on the vector $\begin{pmatrix} 2 \\ -2 \end{pmatrix}$?

### 23: Matrix operations {data-difficulty="2"}

Compute the following products:

a) $AB$, where $A = \begin{pmatrix} 6 & 5 \\ -2 & 7 \end{pmatrix}$, $B = \begin{pmatrix} -5 & 3 \\ 1 & 4 \end{pmatrix}$

b) $(A - B)(A + B)$, where $A = \begin{pmatrix} 2 & 2 & 4 \\ -3 & -2 & 4 \\ -2 & 0 & 2 \end{pmatrix}$, $B = \begin{pmatrix} 2 & 1 & 3 \\ -1 & 2 & 2 \\ 1 & 4 & -1 \end{pmatrix}$

c) $A^2 - B^2$, with the same $A$ and $B$ as in part (b).

### 24: Shear matrix analysis {.bonus-problem data-difficulty="3"}

::: {.callout-note collapse="true" appearance="minimal"}
#### Context
Shear transformations are commonly used in computer graphics for creating italic text effects, perspective corrections, and geometric distortions. They preserve area but change angles and shapes.
:::

Consider the shear matrix $S = \begin{pmatrix} 1 & 1 \\ 0 & 1 \end{pmatrix}$:

a) What would you get if you apply $S$ on the vector $\begin{pmatrix} 0 \\ 1 \end{pmatrix}$?

b) What would you get if you apply $S$ again on the result of the previous point?

c) What if you apply $S$ one more time?

d) What do you think happens when we apply $S$ 100 times on that vector?

e) Can you compute $S^{100}$?

### 24.5: Diagonal matrix powers {data-difficulty="2"}

::: {.callout-note collapse="true" appearance="minimal"}
#### Context
Diagonal matrices are particularly useful in linear algebra because their powers are easy to compute. This property is extensively used in eigenvalue decomposition and diagonalization of matrices.
:::

Consider the diagonal matrix $A = \begin{pmatrix} 2 & 0 \\ 0 & -1 \end{pmatrix}$.

a) Compute $A^2$, $A^3$, and $A^4$.

b) Find a general formula for $A^n$ where $n$ is any positive integer.

c) What does this transformation represent geometrically? How does it affect the unit circle when applied repeatedly?

d) **Bonus:** What happens when you apply this transformation to the vector $\begin{pmatrix} 1 \\ 1 \end{pmatrix}$ multiple times?

## Vector Spaces and Subspaces

### 25: Identifying vector spaces and non-vector spaces {data-difficulty="3"}

::: {.callout-important collapse="true" appearance="minimal"}
#### Context
Understanding what constitutes a vector space is fundamental in linear algebra. A set must satisfy specific axioms to be considered a vector space: closure under addition and scalar multiplication, existence of zero element, existence of additive inverses, and several other properties.
:::

For each of the following sets, determine whether it is a vector space or not. If it is a vector space, prove it by verifying all the required axioms. If it is not a vector space, identify which axiom(s) fail and provide counterexamples.

a) $A = \left\{\begin{pmatrix} a \\ 0 \end{pmatrix} \mid a \in \mathbb{R}\right\}$ (vectors with second component zero)

b) $B = \left\{\begin{pmatrix} a \\ -a \end{pmatrix} \mid a \in \mathbb{R}\right\}$ (vectors where second component is negative of first)

c) $C = \mathbb{N}$ (the set of natural numbers)

d) $D = \left\{\begin{pmatrix} a \\ 1 \end{pmatrix} \mid a \in \mathbb{R}\right\}$ (vectors with second component always 1)

**Hint:** For the non-vector spaces, show that there are some "bad" elements such that if we add them or multiply with some number (not necessarily positive), the result would not belong to the set.