---
title: "02 Linear Algebra - Matrices"
format:
  html:
    css: homework-styles.css
---

<script src="homework-scripts.js"></script>

![image.png](../background_photos/math_02_old_yerevan.jpg)
Հանրապետության Հրապարակ, [լուսանկարի հղումը](https://www.facebook.com/photo/?fbid=2458079004566211&set=g.576489656651693), Նկարը facebook-ում հրապարակող՝ [Marine Tovmasyan](https://www.facebook.com/marine.tovmasyan.921?__cft__[0]=AZUiFtxhpNRX8mqfh4IPPyKdAeWHXpH_HYctA5scvjH2Q9TF8CbeXA-UH2n0-yIfhqK_JXZwUEyuv6H-g7QlmpobqU9-frIHuQvw_KBsJXGBF_AE0F3_r5pbpa5U6xnb8_Mt13vXCY8AmWgoUD-PP_aE&__tn__=-UC%2CP-R)
      

# 📚 Նյութը

- [📚 Ամբողջական նյութը](02_linear_algebra_matrices.qmd)
- [📺 Մատրիցի երկրաչափական իմաստը, գծային ձևափոխություններ](https://youtu.be/AjPnQIe-BSo), [🎞️ Սլայդեր](Lectures/L02_Angles__Vector_Spaces__Matrices.pdf)
- [📺 Lecture 2, ToDo](), [🎞️ Սլայդեր]()
- [🛠️📺 Գործնականի տեսագրությունը ToDo]()
- [🛠️🗂️ Գործնականի PDF-ը ToDo](Homeworks/)
  
📚 Տանը կարդում ենք՝
**Մատրիցի երկրաչափական իմաստը, գծային ձևափոխություններ**

- [Johnston](bibliography/Poole - Linear Algebra-1-400.pdf), 20-25 (մատրիցներ), 35-38 (գծային ձևափոխություն) էջերը
- [Poole](bibliography/Nathaniel Johnston - Introduction to Linear and Matrix Algebra-Springer (2021).pdf), 219-221 էջերը (մատրիցների արտադրյալ/համադրույթ)

և դիտում 3b1b-ի 3-րդ տեսադասը գծային հանրահաշվից՝
https://youtu.be/kYB8IZa5AuE

## 🧮 Հարմար գործիքներ
մատրիցները վիզուալ պատկերացնելու համար օգտակար գործիքներ՝

- 2x2 չափանի [եռանկյունովը](https://melbapplets.ms.unimelb.edu.au/2024/03/25/visualising-linear-transformations-in-r2/)
- 2x2 չափանի [դեղին սլաքովը](https://visualize-it.github.io/linear_transformations/simulation.html)
- 2x2 չափանի [ձեռքով կարգավորվող](https://shad.io/MatVis/)
- 3x3 չափանի [երեքաչափ](https://melbapplets.ms.unimelb.edu.au/2024/03/25/visualising-linear-transformations-in-r3/)



# 🏡 Տնային

::: {.callout-note collapse="false"}
1. ❗❗❗ DON'T CHECK THE SOLUTIONS BEFORE TRYING TO DO THE HOMEWORK BY YOURSELF❗❗❗
2. Please don't hesitate to ask questions, never forget about the 🍊karalyok🍊 principle!
3. The harder the problem is, the more 🧀cheeses🧀 it has.
4. Problems with 🎁 are just extra bonuses. It would be good to try to solve them, but also it's not the highest priority task.
5. If the problem involve many boring calculations, feel free to skip them - important part is understanding the concepts.
6. Submit your solutions [here](https://forms.gle/CFEvNqFiTSsDLiFc6) (even if it's unfinished)
:::


### 01: Matrix transformations {data-difficulty="2"}

What vectors do you get by applying the matrix $A = \begin{pmatrix} 3 & -3 \\ 3 & 3 \end{pmatrix}$ on the vectors:

1. $\vec{a} = \begin{pmatrix} 1 \\ 0 \end{pmatrix}$
2. $\vec{b} = \begin{pmatrix} 0 \\ 1 \end{pmatrix}$
3. $\vec{c} = \begin{pmatrix} 1 \\ 1 \end{pmatrix}$
4. Draw the vectors before and after multiplying with $A$. What can you say visually about the matrix? Can you guess how it will act on the vector $\begin{pmatrix} 2 \\ -2 \end{pmatrix}$?


### 02: Matrix products {data-difficulty="2"}
Compute the following products:

1. $(A - B)(A + B)$, where $A = \begin{pmatrix} 2 & 3 \\ -1 & 2 \end{pmatrix}$, $B = \begin{pmatrix} 1 & 2 \\ 2 & -1 \end{pmatrix}$
2. $A^2 - B^2$, with the same $A$ and $B$ as in part (b).
3. Any comments on the results?

### 03: Shear matrix transformations {data-difficulty="2"}
::: {.callout-note collapse="true" appearance="minimal"}
#### Context
Shear transformations are commonly used in computer graphics for creating italic text effects, perspective corrections, and geometric distortions. They preserve area but change angles and shapes.
:::
Consider the following matrix (it is called the shear matrix): $S = \begin{pmatrix} 1 & 1 \\ 0 & 1 \end{pmatrix}$

1. What would you get if you apply $S$ on the vector $\begin{pmatrix} 0 \\ 1 \end{pmatrix}$?
2. What would you get if you apply $S$ again on the result of the previous point?
3. What if you apply $S$ one more time?
4. What do you think happens when we apply $S$ 100 times on that vector?
5. Can you compute $S^{100}$?

### 04: Diagonal matrix powers {data-difficulty="2"}

::: {.callout-note collapse="true" appearance="minimal"}
#### Context
Diagonal matrices are particularly useful in linear algebra because their powers are easy to compute. This property is extensively used in eigenvalue decomposition and diagonalization of matrices (more on this later).
:::

Consider the diagonal matrix $A = \begin{pmatrix} 2 & 0 \\ 0 & -1 \end{pmatrix}$.

1. Compute $A^2$, $A^3$, and $A^4$.
2. Find a general formula for $A^n$ where $n$ is any positive integer.
3. What does this transformation represent geometrically? How does it affect the unit circle when applied repeatedly?
4. What happens when you apply this transformation to the vector $\begin{pmatrix} 1 \\ 1 \end{pmatrix}$ multiple times?

### 05: Determinant properties {data-difficulty="1"}

1. Prove that $\det(B^{-1}AB) = \det(A)$ if $B$ is invertible.

2. Suppose $Q$ is a $3 \times 3$ real matrix such that $Q^T Q = I$. What values can $\det(Q)$ take?

### 06: Normal equation for linear regression {data-difficulty="2"}

::: {.callout-note collapse="true" appearance="minimal"}
#### Context
The normal equation is a closed-form solution to linear regression problems. It directly computes the optimal parameters using matrix operations, avoiding the need for iterative optimization algorithms like gradient descent.
:::

Consider a simple linear regression problem where you want to fit a line $y = \theta_0 + \theta_1 x$ to the following data points:

| $x$ | $y$ |
|-----|-----|
| 1   | 2   |
| 2   | 4   |

1. Set up the design matrix $X$ (including the intercept column) and the target vector $\vec{y}$.

2. Use the normal equation $\vec{\theta} = (X^T X)^{-1} X^T \vec{y}$ to find the optimal parameters $\theta_0$ and $\theta_1$.

3. What line equation did you get? Does it make sense given the data?

4. Verify your result by checking that this line passes through the given data points.

::: {.content-visible when-profile="solution"}

### Solution {.solution-header}

**Part 1:** Setting up the matrices

Design matrix $X$ (with intercept column):
$$X = \begin{pmatrix} 1 & 1 \\ 1 & 2 \end{pmatrix}$$

Target vector:
$$\vec{y} = \begin{pmatrix} 2 \\ 4 \end{pmatrix}$$

**Part 2:** Computing the normal equation

First, compute $X^T$:
$$X^T = \begin{pmatrix} 1 & 1 \\ 1 & 2 \end{pmatrix}$$

Next, compute $X^T X$:
$$X^T X = \begin{pmatrix} 1 & 1 \\ 1 & 2 \end{pmatrix} \begin{pmatrix} 1 & 1 \\ 1 & 2 \end{pmatrix} = \begin{pmatrix} 2 & 3 \\ 3 & 5 \end{pmatrix}$$

Find $(X^T X)^{-1}$:
$$\det(X^T X) = 2 \cdot 5 - 3 \cdot 3 = 10 - 9 = 1$$
$$(X^T X)^{-1} = \frac{1}{1} \begin{pmatrix} 5 & -3 \\ -3 & 2 \end{pmatrix} = \begin{pmatrix} 5 & -3 \\ -3 & 2 \end{pmatrix}$$

Compute $X^T \vec{y}$:
$$X^T \vec{y} = \begin{pmatrix} 1 & 1 \\ 1 & 2 \end{pmatrix} \begin{pmatrix} 2 \\ 4 \end{pmatrix} = \begin{pmatrix} 6 \\ 10 \end{pmatrix}$$

Finally, compute $\vec{\theta}$:
$$\vec{\theta} = (X^T X)^{-1} X^T \vec{y} = \begin{pmatrix} 5 & -3 \\ -3 & 2 \end{pmatrix} \begin{pmatrix} 6 \\ 10 \end{pmatrix} = \begin{pmatrix} 0 \\ 2 \end{pmatrix}$$

**Part 3:** The line equation is $y = 0 + 2x = 2x$. This makes perfect sense since both data points lie exactly on this line.

**Part 4:** Verification:
- For $x = 1$: $y = 2(1) = 2$ ✓
- For $x = 2$: $y = 2(2) = 4$ ✓

:::




# 🎲 39 (02)
- ▶️[Մենք ենք մեր սարերը](https://www.youtube.com/watch?v=9CbMaPXC8SI)
- 🔗[Random link](https://www.youtube.com/watch?v=-33IXM8gC4g)
- 🇦🇲🎶[Ալեքսանդր Աճեմյան (Սիրո հասակ)](https://www.youtube.com/watch?v=2lt23ofSku0)
- 🌐🎶[Armin Küpper(Pipelinefunk)](https://youtu.be/p8GcHoSIPDg?si=kR7B9WfHmQ_-HRqi)
- 🤌[Կարգին](https://www.youtube.com/watch?v=HCTKe3SwUHU)

<a href="http://s01.flagcounter.com/more/1oO"><img src="https://s01.flagcounter.com/count2/1oO/bg_FFFFFF/txt_000000/border_CCCCCC/columns_2/maxflags_10/viewers_0/labels_0/pageviews_1/flags_0/percent_0/" alt="Flag Counter"></a>