---
title: "Introduction + Linear Regression"
---

# Topics
- Types of ML problems (Supervised, unsupervised, reinforcement learning)
- Intuitive example of supervised learning (trees in a forest)
- Line / Hypothesis / Model
- Hypothesis space
- Loss function
- Risk function
- Gradient descent
  
# ๐ Resources
## Clean PDF
```{=html}
<object data="PDF/L01 Linear Regression.pdf" type="application/pdf" width="100%" height="100%" style="min-height:100vh;">
    <p>Failed to display in your browser.
    Just click here <a href="PDF/L01 Linear Regression.pdf"> and download the file</a></p>
</object>   
```

## PDF With notes
```{=html}	
<object data="PDF\2025\L01 Linear Regression_notes.pdf" type="application/pdf" width="100%" height="100%" style="min-height:100vh;">
    <p>Failed to display in your browser.
    Just click here <a href="PDF\2025\L01 Linear Regression_notes.pdf"> and download the file</a></p>
</object>
```

# ๐กHomework
[data_lin_reg.csv](Homeworks/HW01 Linear Regression + Main Concepts/data_lin_reg.csv) ึีกีตีฌีธึีด ีฅึีฏีธึ ีฝีตีธึีถ ีฏีกี x ีธึ yึ 

ีีฅีฟึ ีก ีฃีฟีถีฅีถึ `theta_1 * x` ีธึีฒีซีฒีจ ีธึ ีฌีกีพีกีฃีธึีตีถีจ ีถีฏีกึีกีฃึีธึีด ีก ีฟีพีตีกีฌีถีฅึีจึ

ิฑีดีฅีถีซีถีน ีฑีฅีผึีธีพ ีก ีบีฅีฟึ ีฃึีฅีฌี ีฏีกึีฅีฌีซ ีก ึีฃีฟีพีฅีฌ ีดีฅีถีกีฏ ีดีซีถีนึ ีงีฝ ีบีกีฐีจ ีกีถึีกีฎ ีฃึีกีคีกึีกีถีถีฅึีซึึ

1. ิดีกีฟีกีถ ึีกีทีฅึ, ีฏีกึีคีกึีฅึ ีบีกีถีคีกีฝีธีพ, ีพีซีฆีธึีกีฌีซีฆีกึึีฅึ
2. Gradient descent-ีจ ีซีดีบีฌีฅีดีฅีถีฟีกึีซีก ีกึีฅึึ ิฒีฆีขีฆีกีฌีธีพ ีฃีฟีฅึ ีฌีกีพีกีฃีธึีตีถ alpha-ีถ (step size / learning rate)
3. ีีกึีขีฅึ alpha-ีถีฅึีซ ีฐีกีดีกึ ีฃีฎีฅึ ีฃึีกึีซีฏ ีธึีจ ึีธึีตึ ีฏีฟีก ีซีฟีฅึีกึีซีกีตีซ ึีกีถีกีฏีซ ีธึ ีีซีฝีฏีซ (ีดีซีปีซีถ ีฝีญีกีฌีกีถึีซ) ีฏีกีบีจึ
4. ีีฅึีปีธึีด ีฃีฎีฅึ ีคีกีฟีกีถ - ีฌีกีพีกีฃีธึีตีถ ีดีธีคีฅีฌีซ ีฃีธึีทีกีฏีกีฎ ีฏีฅีฟีฅึีจ, ีฃีธึีทีกีฏีกีฎ ีธึีฒีซีฒีจ ีธึ ีซีฝีฏีกีฏีกีถ ีกึีชีฅึีถีฅึีจึ
5 4 3 2 1
**ิผึีกึีธึึีซีนี
-** ีฃีฎีฅึ ีฝีญีกีฌีกีถึีถีฅึีซ ีนีกึีฅึีซ ีฐีซีฝีฟีธีฃึีกีดีจึ
- ีฃีฎีฅึ `Theta_1`-ีซ ีธึ `Risk`-ีซ ีฏีกีบีจ ีกึีฟีกีฐีกีตีฟีธีฒ ีฃึีกึีซีฏีจึ 
  - ิผึีกึีธึึีซีน^2 ีงีค ีฃึีกึีซีฏีซ ีพึีก ีถีทีฅึ ีฃึีกีคีซีฅีถีฟีซ ีฐีฅีฟีกีฃีซีฎีจึ


ิตีฉีฅ ีดีกึีฟีกีฐึีกีพีฅึีซ ีบีกีฟึีกีฝีฟ ีฅึ ีงีฝ ีกีดีฅีถีซีถีนีจ ีกึีฅึ ีธีนีฉีฅ `theta_1*x` ีดีธีคีฅีฌีซ ีฐีกีดีกึ ีกีตีฌ `theta_0 + theta_1*x`-ีซ ีฐีกีดีกึ

ิตึีข ีฝีฟีกีถีกึ ีฃีธึีทีกีฏีกีฎ ีฃีซีฎีจ, ีธึีฒีกึีฏีฅึ ีขีกีถีกีฑึีจี ีธึึีกีญีกีถีกีดึ

**ิผึีกึีธึึีซีน ีฟีถีกีตีซีถี **

ีคีธึึีฝ ีขีฅึีฅีฌ L2 (Mean Square error) Loss-ีธีพ ีฌีกีพีกีฃีธึีตีถ ีฌีธึีฎีดีกีถ ีกีถีกีฌีซีฆีซีฟ ีฟีฅีฝึีจ (normal equation-ีก ีฏีธีนีพีธึีด)

ิณีธึีฎีถีกีฏีกีถีซ ีชีกีดีกีถีกีฏ ีกีดีฅีถีกีตีถ ีฐีกีพีกีถีกีฏีถีธึีฉีตีกีดีข ีนีฅีถึ ีถีกีตีซ ีคึีก ีฐีกีดีกึ ีดีซีกีถีฃีกีดีซึ ีธึีฒีกึีฏีฅีด ีฌึีกึีธึึีซีน ีถีตีธึีฉีฅึ, ีถีกึ ีฌีธึีฎีธึีดีจึ ิฒีกีตึ ีกีผีกีถึ ีดีฟีกีฎีฅีฌีธึ ีฌีธึีฎีธึีดีจ ีถีกีตีฅีฌีจ ีทีกีฟ ีพีกีฟ ีดีซีฟึ ีกึ

- [video](https://www.youtube.com/watch?v=NN7mBupK-8o) it's quite old but I think it does a good job explaining it. Here J(theta) is just the same as R_emp in our notation
- And this is also a nice [post](http://mlwiki.org/index.php/Normal_Equation)
- And if you want to see the derivation of the formula, [this forum](https://math.stackexchange.com/questions/4177039/deriving-the-normal-equation-for-linear-regression) is quite helpful  



    