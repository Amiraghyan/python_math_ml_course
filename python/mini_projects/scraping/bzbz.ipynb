{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "347e7b89",
   "metadata": {},
   "source": [
    "# Web Scraping Armenian Poems from grqamol.am\n",
    "\n",
    "This notebook scrapes poems from the Armenian poetry website grqamol.am. It extracts poems from multiple pages and collects the text content from each poem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba35db83",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1dc912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "BASE_URL = \"https://grqamol.am\"\n",
    "STARTING_URL = \"https://grqamol.am/category/poems/?tag=180213113413410888\"\n",
    "\n",
    "# Headers to mimic a real browser\n",
    "HEADERS = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "}\n",
    "\n",
    "# Storage for scraped poems\n",
    "poems_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc03b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_poem_content(poem_url):\n",
    "    \"\"\"\n",
    "    Extract the full poem content from a poem's individual page\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(poem_url, headers=HEADERS, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Try to find the poem content - looking for various possible selectors\n",
    "        poem_content = None\n",
    "        \n",
    "        # Common selectors for poem content\n",
    "        selectors = [\n",
    "            '.poem-content',\n",
    "            '.content',\n",
    "            '.post-content',\n",
    "            '.entry-content',\n",
    "            'article .content',\n",
    "            '.poem-text',\n",
    "            '.text'\n",
    "        ]\n",
    "        \n",
    "        for selector in selectors:\n",
    "            content_element = soup.select_one(selector)\n",
    "            if content_element:\n",
    "                poem_content = content_element.get_text(strip=True)\n",
    "                break\n",
    "        \n",
    "        # If no specific selector works, try to find the main content area\n",
    "        if not poem_content:\n",
    "            # Look for the largest text block that might contain the poem\n",
    "            possible_content = soup.find_all(['div', 'p', 'section'], string=re.compile(r'.{50,}'))\n",
    "            if possible_content:\n",
    "                poem_content = max(possible_content, key=lambda x: len(x.get_text())).get_text(strip=True)\n",
    "        \n",
    "        # Extract title\n",
    "        title = \"\"\n",
    "        title_selectors = ['h1', '.title', '.post-title', '.entry-title']\n",
    "        for selector in title_selectors:\n",
    "            title_element = soup.select_one(selector)\n",
    "            if title_element:\n",
    "                title = title_element.get_text(strip=True)\n",
    "                break\n",
    "        \n",
    "        return {\n",
    "            'url': poem_url,\n",
    "            'title': title,\n",
    "            'content': poem_content or \"Content not found\"\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {poem_url}: {str(e)}\")\n",
    "        return {\n",
    "            'url': poem_url,\n",
    "            'title': \"Error\",\n",
    "            'content': f\"Error: {str(e)}\"\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0f955f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_poems_from_page(page_url):\n",
    "    \"\"\"\n",
    "    Scrape poem links from a category page\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(page_url, headers=HEADERS, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Find all poem links on the page\n",
    "        poem_links = []\n",
    "        \n",
    "        # Common selectors for poem links\n",
    "        link_selectors = [\n",
    "            'a[href*=\"/poem/\"]',\n",
    "            'a[href*=\"/poetry/\"]',\n",
    "            '.poem-link a',\n",
    "            '.post-title a',\n",
    "            '.entry-title a',\n",
    "            'h2 a',\n",
    "            'h3 a',\n",
    "            '.title a'\n",
    "        ]\n",
    "        \n",
    "        for selector in link_selectors:\n",
    "            links = soup.select(selector)\n",
    "            for link in links:\n",
    "                href = link.get('href')\n",
    "                if href:\n",
    "                    full_url = urljoin(BASE_URL, href)\n",
    "                    if full_url not in poem_links:\n",
    "                        poem_links.append(full_url)\n",
    "        \n",
    "        # If no specific selectors work, try to find all links that might be poems\n",
    "        if not poem_links:\n",
    "            all_links = soup.find_all('a', href=True)\n",
    "            for link in all_links:\n",
    "                href = link.get('href')\n",
    "                # Look for patterns that suggest this is a poem link\n",
    "                if href and any(pattern in href.lower() for pattern in ['poem', 'poetry', 'post']):\n",
    "                    full_url = urljoin(BASE_URL, href)\n",
    "                    if full_url not in poem_links:\n",
    "                        poem_links.append(full_url)\n",
    "        \n",
    "        print(f\"Found {len(poem_links)} poem links on {page_url}\")\n",
    "        return poem_links\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping page {page_url}: {str(e)}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625678a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_page_url(soup, current_url):\n",
    "    \"\"\"\n",
    "    Find the URL of the next page\n",
    "    \"\"\"\n",
    "    next_page_selectors = [\n",
    "        'a[rel=\"next\"]',\n",
    "        '.next-page a',\n",
    "        '.pagination .next a',\n",
    "        'a:contains(\"Հաջորդ\")',  # \"Next\" in Armenian\n",
    "        'a:contains(\"այլ\")',     # \"More\" in Armenian\n",
    "        'a[href*=\"page=\"]'\n",
    "    ]\n",
    "    \n",
    "    for selector in next_page_selectors:\n",
    "        next_link = soup.select_one(selector)\n",
    "        if next_link and next_link.get('href'):\n",
    "            return urljoin(BASE_URL, next_link.get('href'))\n",
    "    \n",
    "    # Try to find pagination links and get the next one\n",
    "    pagination_links = soup.find_all('a', href=re.compile(r'page=\\d+'))\n",
    "    if pagination_links:\n",
    "        # Extract page numbers and find the next one\n",
    "        current_page = 1\n",
    "        try:\n",
    "            current_page = int(re.search(r'page=(\\d+)', current_url).group(1))\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        for link in pagination_links:\n",
    "            href = link.get('href')\n",
    "            try:\n",
    "                page_num = int(re.search(r'page=(\\d+)', href).group(1))\n",
    "                if page_num == current_page + 1:\n",
    "                    return urljoin(BASE_URL, href)\n",
    "            except:\n",
    "                continue\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40be7c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_all_poems(starting_url, max_pages=10):\n",
    "    \"\"\"\n",
    "    Main function to scrape all poems from multiple pages\n",
    "    \"\"\"\n",
    "    current_url = starting_url\n",
    "    page_count = 0\n",
    "    all_poem_links = []\n",
    "    \n",
    "    while current_url and page_count < max_pages:\n",
    "        print(f\"\\nScraping page {page_count + 1}: {current_url}\")\n",
    "        \n",
    "        # Get the page content\n",
    "        try:\n",
    "            response = requests.get(current_url, headers=HEADERS, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        except Exception as e:\n",
    "            print(f\"Error accessing page {current_url}: {str(e)}\")\n",
    "            break\n",
    "        \n",
    "        # Get poem links from this page\n",
    "        poem_links = scrape_poems_from_page(current_url)\n",
    "        all_poem_links.extend(poem_links)\n",
    "        \n",
    "        # Find next page\n",
    "        next_url = get_next_page_url(soup, current_url)\n",
    "        current_url = next_url\n",
    "        page_count += 1\n",
    "        \n",
    "        # Be respectful to the server\n",
    "        time.sleep(1)\n",
    "    \n",
    "    print(f\"\\nFound total {len(all_poem_links)} poem links across {page_count} pages\")\n",
    "    \n",
    "    # Now scrape each poem\n",
    "    print(\"Starting to scrape individual poems...\")\n",
    "    for i, poem_url in enumerate(all_poem_links):\n",
    "        print(f\"Scraping poem {i+1}/{len(all_poem_links)}: {poem_url}\")\n",
    "        poem_data = get_poem_content(poem_url)\n",
    "        poems_data.append(poem_data)\n",
    "        \n",
    "        # Be respectful to the server\n",
    "        time.sleep(0.5)\n",
    "    \n",
    "    print(f\"\\nCompleted scraping {len(poems_data)} poems!\")\n",
    "    return poems_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215ff15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start scraping - you can adjust max_pages as needed\n",
    "print(\"Starting to scrape Armenian poems from grqamol.am...\")\n",
    "scraped_poems = scrape_all_poems(STARTING_URL, max_pages=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a949e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with the scraped poems\n",
    "df = pd.DataFrame(poems_data)\n",
    "print(f\"Scraped {len(df)} poems\")\n",
    "print(\"\\nFirst few poems:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca858e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display some statistics about the scraped poems\n",
    "print(f\"Total poems scraped: {len(df)}\")\n",
    "print(f\"Poems with content: {len(df[df['content'] != 'Content not found'])}\")\n",
    "print(f\"Average content length: {df['content'].str.len().mean():.1f} characters\")\n",
    "\n",
    "# Show a sample poem\n",
    "if len(df) > 0:\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"SAMPLE POEM:\")\n",
    "    print(\"=\"*50)\n",
    "    sample_poem = df.iloc[0]\n",
    "    print(f\"Title: {sample_poem['title']}\")\n",
    "    print(f\"URL: {sample_poem['url']}\")\n",
    "    print(f\"Content preview: {sample_poem['content'][:300]}...\")\n",
    "    print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f84d4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the scraped poems to a CSV file\n",
    "if len(df) > 0:\n",
    "    filename = \"armenian_poems_grqamol.csv\"\n",
    "    df.to_csv(filename, index=False, encoding='utf-8')\n",
    "    print(f\"Saved {len(df)} poems to {filename}\")\n",
    "    \n",
    "    # Also save as JSON for better text handling\n",
    "    import json\n",
    "    json_filename = \"armenian_poems_grqamol.json\"\n",
    "    df.to_json(json_filename, orient='records', force_ascii=False, indent=2)\n",
    "    print(f\"Also saved as {json_filename}\")\n",
    "else:\n",
    "    print(\"No poems were scraped successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9668fe1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requests library imported successfully!\n",
      "Testing connection to: https://grqamol.am/category/poems/?tag=180213113413410888\n"
     ]
    }
   ],
   "source": [
    "# Let's start with a simple test first\n",
    "import requests\n",
    "print(\"Requests library imported successfully!\")\n",
    "\n",
    "# Test connection\n",
    "url = \"https://grqamol.am/category/poems/?tag=180213113413410888\"\n",
    "print(f\"Testing connection to: {url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8193d1e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status code: 200\n",
      "Page title: Բանաստեղծություններ\n",
      "Found 141 links on the page\n",
      "Link: javascript: - Text: \n",
      "Link: #mob_menu - Text: Toggle navigation\n",
      "Link: /pulitzer/ - Text: Պուլիցերյան մրցանակաբաշխություն\n",
      "Link: /category/harcazruycner/ - Text: Հարցազրույցներ\n",
      "Link: /category/press/ - Text: Արձագանքներ\n",
      "Link: /view/about-us/ - Text: Մեր Մասին\n",
      "Link: /feedback/ - Text: Հետադարձ կապ\n",
      "Link: / - Text: \n",
      "Link: / - Text: \n",
      "Link: /category/quote-book/ - Text: Մեջբերումներ Գրքերից\n"
     ]
    }
   ],
   "source": [
    "# Let's first examine the page structure\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "# Test the URL\n",
    "response = requests.get(url)\n",
    "print(f\"Status code: {response.status_code}\")\n",
    "\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "print(f\"Page title: {soup.title.string if soup.title else 'No title found'}\")\n",
    "\n",
    "# Let's see what links are on the page\n",
    "all_links = soup.find_all('a', href=True)\n",
    "print(f\"Found {len(all_links)} links on the page\")\n",
    "\n",
    "# Look for poem links specifically\n",
    "poem_links = []\n",
    "for link in all_links[:10]:  # Just look at first 10 for now\n",
    "    href = link.get('href')\n",
    "    text = link.get_text(strip=True)\n",
    "    print(f\"Link: {href} - Text: {text[:50]}\")\n",
    "    if href and ('poem' in href.lower() or 'post' in href.lower()):\n",
    "        poem_links.append(href)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec8b67eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 96 content divs\n",
      "Found 0 areas with substantial text\n",
      "\n",
      "Found 78 potential poem links:\n",
      "1. Մեր Մասին - https://grqamol.am/view/about-us/\n",
      "2.  - https://grqamol.am/article/paruyr-sevak-yerku-siro-aranqum/\n",
      "3. Պարույր Սևակ «Երկու սիրո արանքում» - https://grqamol.am/article/paruyr-sevak-yerku-siro-aranqum/\n",
      "4.  - https://grqamol.am/article/paruyr-sevak-mor-dzerqer/\n",
      "5. Պարույր Սևակ «Մոր ձեռքերը» - https://grqamol.am/article/paruyr-sevak-mor-dzerqer/\n"
     ]
    }
   ],
   "source": [
    "# Let's look for more specific patterns and content divs\n",
    "import re\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "# Check for specific content areas\n",
    "content_divs = soup.find_all('div', class_=re.compile(r'content|post|poem|article'))\n",
    "print(f\"Found {len(content_divs)} content divs\")\n",
    "\n",
    "# Look for actual poem content on this page\n",
    "poem_content_areas = soup.find_all(['div', 'section', 'article'], string=re.compile(r'.{100,}'))\n",
    "print(f\"Found {len(poem_content_areas)} areas with substantial text\")\n",
    "\n",
    "# Let's check if there are links to individual poems\n",
    "base_url = \"https://grqamol.am\"\n",
    "individual_poem_links = []\n",
    "\n",
    "for link in all_links:\n",
    "    href = link.get('href')\n",
    "    if href:\n",
    "        # Look for links that might lead to individual poems\n",
    "        if any(pattern in href for pattern in ['/view/', '/post/', '/poem/', '/article/']):\n",
    "            full_url = urljoin(base_url, href)\n",
    "            individual_poem_links.append((full_url, link.get_text(strip=True)))\n",
    "\n",
    "print(f\"\\nFound {len(individual_poem_links)} potential poem links:\")\n",
    "for i, (url, text) in enumerate(individual_poem_links[:5]):  # Show first 5\n",
    "    print(f\"{i+1}. {text[:50]} - {url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4831aa2e",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Now let's create a comprehensive scraping function\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_poem_content(poem_url):\n",
    "    \"\"\"Scrape the actual poem content from an individual poem page\"\"\"\n",
    "    try:\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        }\n",
    "        response = requests.get(poem_url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Get the title\n",
    "        title = \"\"\n",
    "        title_selectors = ['h1', 'h2', '.title', '.post-title', '.entry-title']\n",
    "        for selector in title_selectors:\n",
    "            title_element = soup.select_one(selector)\n",
    "            if title_element:\n",
    "                title = title_element.get_text(strip=True)\n",
    "                break\n",
    "        \n",
    "        # Get the poem content\n",
    "        poem_content = \"\"\n",
    "        \n",
    "        # Look for content areas\n",
    "        content_selectors = [\n",
    "            '.article-content',\n",
    "            '.post-content', \n",
    "            '.content',\n",
    "            '.entry-content',\n",
    "            '#content',\n",
    "            '.main-content'\n",
    "        ]\n",
    "        \n",
    "        for selector in content_selectors:\n",
    "            content_element = soup.select_one(selector)\n",
    "            if content_element:\n",
    "                # Get text and clean it up\n",
    "                poem_content = content_element.get_text(separator='\\n', strip=True)\n",
    "                break\n",
    "        \n",
    "        # If no specific content selector works, try to find the main text\n",
    "        if not poem_content:\n",
    "            # Look for paragraphs or divs with substantial text\n",
    "            paragraphs = soup.find_all(['p', 'div'], string=re.compile(r'.{20,}'))\n",
    "            if paragraphs:\n",
    "                poem_content = '\\n'.join([p.get_text(strip=True) for p in paragraphs[:5]])\n",
    "        \n",
    "        return {\n",
    "            'url': poem_url,\n",
    "            'title': title,\n",
    "            'content': poem_content or \"Content not found\"\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {poem_url}: {str(e)}\")\n",
    "        return {\n",
    "            'url': poem_url,\n",
    "            'title': \"Error\",\n",
    "            'content': f\"Error: {str(e)}\"\n",
    "        }\n",
    "\n",
    "def get_all_poem_links_from_page(page_url):\n",
    "    \"\"\"Get all poem links from a category page\"\"\"\n",
    "    try:\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        }\n",
    "        response = requests.get(page_url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        poem_links = []\n",
    "        all_links = soup.find_all('a', href=True)\n",
    "        \n",
    "        for link in all_links:\n",
    "            href = link.get('href')\n",
    "            if href and '/article/' in href:\n",
    "                full_url = urljoin(\"https://grqamol.am\", href)\n",
    "                if full_url not in poem_links:\n",
    "                    poem_links.append(full_url)\n",
    "        \n",
    "        return poem_links\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error getting links from {page_url}: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "print(\"Scraping functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7679f230",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Armenian Poems Scraper - Simple Approach\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
