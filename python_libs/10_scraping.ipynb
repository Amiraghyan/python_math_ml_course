{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"10 Scraping\"\n",
    "description: \"Beautiful Soup, Scrapy, and Selenium\"\n",
    "lightbox: true\n",
    "format: \n",
    "  html:\n",
    "    code-fold: false\n",
    "number-offset: 1\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "![image.png](../background_photos/)\n",
    "[’¨’∏÷Ç’Ω’°’∂’Ø’°÷Ä’´ ’∞’≤’∏÷Ç’¥’®](https://unsplash.com/photos/a-large-mountain-with-a-very-tall-cliff-UiP9KfVe3aQ), ’Ä’•’≤’´’∂’°’Ø’ù []()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "<a href=\"ToDo\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a> (ToDo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "> Song reference - ToDo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# üìå ’Ü’Ø’°÷Ä’°’£’´÷Ä\n",
    "\n",
    "[üìö ‘±’¥’¢’∏’≤’ª’°’Ø’°’∂ ’∂’µ’∏÷Ç’©’®]()\n",
    "\n",
    "#### üì∫ ’è’•’Ω’°’∂’µ’∏÷Ç’©’•÷Ä\n",
    "#### üè° ’è’∂’°’µ’´’∂"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# üìö ’Ü’µ’∏÷Ç’©’®"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üåê HTML Basics - Understanding Web Structure\n",
    "\n",
    "Before diving into web scraping, it's essential to understand the structure of web pages. HTML (HyperText Markup Language) provides the structure of web pages.\n",
    "\n",
    "### What is HTML?\n",
    "\n",
    "HTML uses **tags** to define elements. Tags are enclosed in angle brackets `< >` and usually come in pairs:\n",
    "\n",
    "```html\n",
    "<tagname>Content goes here</tagname>\n",
    "```\n",
    "\n",
    "### Basic HTML Document Structure:\n",
    "\n",
    "```html\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "    <title>Page Title</title>\n",
    "</head>\n",
    "<body>\n",
    "    <h1>Main Heading</h1>\n",
    "    <p>This is a paragraph.</p>\n",
    "</body>\n",
    "</html>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common HTML Tags for Scraping:\n",
    "\n",
    "#### Document Structure:\n",
    "- `<html>` - Root element\n",
    "- `<head>` - Contains metadata\n",
    "- `<title>` - Page title\n",
    "- `<body>` - Visible page content\n",
    "\n",
    "#### Text Content:\n",
    "- `<h1>`, `<h2>`, `<h3>` - Headers (most important to least)\n",
    "- `<p>` - Paragraphs\n",
    "- `<span>` - Inline text container\n",
    "\n",
    "#### Containers:\n",
    "- `<div>` - Block-level container (most common)\n",
    "- `<section>` - Semantic section\n",
    "- `<article>` - Independent content\n",
    "\n",
    "#### Lists and Links:\n",
    "- `<ul>`, `<ol>`, `<li>` - Unordered/ordered lists and list items\n",
    "- `<a>` - Links\n",
    "- `<img>` - Images\n",
    "\n",
    "#### Data Tables:\n",
    "- `<table>`, `<tr>`, `<td>`, `<th>` - Tables, rows, cells, headers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HTML Attributes - The Key to Scraping\n",
    "\n",
    "Attributes provide additional information about elements and are **crucial** for web scraping:\n",
    "\n",
    "```html\n",
    "<div id=\"content\" class=\"main-section\">\n",
    "<a href=\"https://example.com\" target=\"_blank\">Link</a>\n",
    "<img src=\"image.jpg\" alt=\"Description\">\n",
    "<div data-price=\"29.99\" data-category=\"electronics\">Product</div>\n",
    "```\n",
    "\n",
    "**Most Important Attributes for Scraping:**\n",
    "- `id` - Unique identifier (use with `#` in CSS selectors)\n",
    "- `class` - CSS class name(s) (use with `.` in CSS selectors)\n",
    "- `href` - Link destination\n",
    "- `src` - Source for images/scripts\n",
    "- `data-*` - Custom data attributes (very common in modern websites)\n",
    "\n",
    "**Why Attributes Matter:**\n",
    "- They help us target specific elements\n",
    "- They often contain valuable data\n",
    "- They make our scrapers more precise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ CSS Selectors - Your Scraping Toolkit\n",
    "\n",
    "CSS selectors are **THE MOST IMPORTANT** concept in web scraping. They tell your scraper exactly which elements to extract.\n",
    "\n",
    "### Basic Selectors:\n",
    "\n",
    "#### 1. Element Selector:\n",
    "```css\n",
    "p          /* Selects all <p> elements */\n",
    "div        /* Selects all <div> elements */\n",
    "h1         /* Selects all <h1> elements */\n",
    "```\n",
    "\n",
    "#### 2. Class Selector (starts with .):\n",
    "```css\n",
    ".classname     /* Selects elements with class=\"classname\" */\n",
    ".post-title    /* Selects elements with class=\"post-title\" */\n",
    ".btn-primary   /* Selects elements with class=\"btn-primary\" */\n",
    "```\n",
    "\n",
    "#### 3. ID Selector (starts with #):\n",
    "```css\n",
    "#idname        /* Selects element with id=\"idname\" */\n",
    "#main-content  /* Selects element with id=\"main-content\" */\n",
    "#header        /* Selects element with id=\"header\" */\n",
    "```\n",
    "\n",
    "#### 4. Attribute Selector:\n",
    "```css\n",
    "[href]                    /* Elements with href attribute */\n",
    "[class=\"post\"]           /* Elements with class=\"post\" */\n",
    "[data-price=\"29.99\"]     /* Elements with data-price=\"29.99\" */\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced CSS Selectors:\n",
    "\n",
    "#### Combination Selectors:\n",
    "```css\n",
    "div p              /* All <p> inside <div> (descendant) */\n",
    "div > p            /* Direct <p> children of <div> */\n",
    "h1 + p             /* First <p> immediately after <h1> */\n",
    ".post .title       /* Elements with class \"title\" inside elements with class \"post\" */\n",
    "```\n",
    "\n",
    "#### Multiple Classes:\n",
    "```css\n",
    ".post.featured     /* Elements with BOTH classes \"post\" AND \"featured\" */\n",
    ".btn.btn-primary   /* Elements with BOTH classes \"btn\" AND \"btn-primary\" */\n",
    "```\n",
    "\n",
    "#### Pseudo-selectors:\n",
    "```css\n",
    "p:first-child      /* First <p> element of its parent */\n",
    "p:last-child       /* Last <p> element of its parent */\n",
    "p:nth-child(2)     /* Second <p> element of its parent */\n",
    "a:contains(\"Next\") /* Links containing text \"Next\" */\n",
    "```\n",
    "\n",
    "#### Complex Examples:\n",
    "```css\n",
    "div.post-content p.highlight    /* <p> with class \"highlight\" inside <div> with class \"post-content\" */\n",
    "#main-content .sidebar a[href]  /* Links inside sidebar inside main content */\n",
    "table tr:nth-child(odd)         /* Odd rows in a table */\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSS Selector Practice with Sample HTML\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Sample HTML for practicing CSS selectors\n",
    "practice_html = \"\"\"\n",
    "<html>\n",
    "<body>\n",
    "    <div id=\"header\" class=\"top-section\">\n",
    "        <h1 class=\"main-title\">Welcome to Our Store</h1>\n",
    "        <nav class=\"navigation\">\n",
    "            <a href=\"/home\">Home</a>\n",
    "            <a href=\"/products\">Products</a>\n",
    "            <a href=\"/contact\">Contact</a>\n",
    "        </nav>\n",
    "    </div>\n",
    "    \n",
    "    <div id=\"main-content\">\n",
    "        <div class=\"product featured\" data-price=\"199.99\">\n",
    "            <h2 class=\"product-title\">iPhone 15</h2>\n",
    "            <p class=\"description\">Latest smartphone with amazing features</p>\n",
    "            <span class=\"price\">$199.99</span>\n",
    "        </div>\n",
    "        \n",
    "        <div class=\"product\" data-price=\"89.99\">\n",
    "            <h2 class=\"product-title\">Headphones</h2>\n",
    "            <p class=\"description\">High-quality wireless headphones</p>\n",
    "            <span class=\"price\">$89.99</span>\n",
    "        </div>\n",
    "    </div>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "soup = BeautifulSoup(practice_html, 'html.parser')\n",
    "\n",
    "print(\"üéØ CSS Selector Practice:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# 1. Basic selectors\n",
    "print(\"\\n1Ô∏è‚É£ Basic Selectors:\")\n",
    "print(f\"All h2 elements: {len(soup.select('h2'))} found\")\n",
    "print(f\"Elements with class 'product': {len(soup.select('.product'))} found\")\n",
    "print(f\"Element with id 'header': {len(soup.select('#header'))} found\")\n",
    "\n",
    "# 2. Find specific content\n",
    "print(\"\\n2Ô∏è‚É£ Finding Specific Content:\")\n",
    "main_title = soup.select_one('h1.main-title')\n",
    "if main_title:\n",
    "    print(f\"Main title: {main_title.text}\")\n",
    "\n",
    "navigation_links = soup.select('nav.navigation a')\n",
    "print(f\"Navigation links: {[link.text for link in navigation_links]}\")\n",
    "\n",
    "# 3. Product information\n",
    "print(\"\\n3Ô∏è‚É£ Extract Product Information:\")\n",
    "products = soup.select('div.product')\n",
    "for i, product in enumerate(products, 1):\n",
    "    title = product.select_one('.product-title').text\n",
    "    price = product.select_one('.price').text\n",
    "    is_featured = 'featured' in product.get('class', [])\n",
    "    \n",
    "    print(f\"Product {i}: {title}\")\n",
    "    print(f\"  Price: {price}\")\n",
    "    print(f\"  Featured: {'‚úÖ' if is_featured else '‚ùå'}\")\n",
    "\n",
    "# 4. Advanced selectors\n",
    "print(\"\\n4Ô∏è‚É£ Advanced Selectors:\")\n",
    "featured_product = soup.select_one('.product.featured .product-title')\n",
    "if featured_product:\n",
    "    print(f\"Featured product: {featured_product.text}\")\n",
    "\n",
    "expensive_products = soup.select('[data-price]')\n",
    "print(f\"Products with price data: {len(expensive_products)}\")\n",
    "\n",
    "first_product = soup.select_one('.product:first-child .product-title')\n",
    "if first_product:\n",
    "    print(f\"First product: {first_product.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü•Ñ Beautiful Soup - Your HTML Parsing Companion\n",
    "\n",
    "Beautiful Soup is perfect for beginners and handles most scraping tasks effectively. It makes parsing HTML as easy as navigating a family tree!\n",
    "\n",
    "### Why Beautiful Soup?\n",
    "- **Easy to learn**: Intuitive syntax\n",
    "- **Powerful**: Handles messy HTML gracefully\n",
    "- **Flexible**: Multiple ways to find elements\n",
    "- **Robust**: Handles encoding issues automatically\n",
    "\n",
    "### Core Concepts:\n",
    "1. **Parsing**: Convert HTML text into a navigable object\n",
    "2. **Searching**: Find specific elements using tags, attributes, or CSS selectors\n",
    "3. **Extracting**: Get text, attributes, or sub-elements\n",
    "4. **Navigating**: Move between parent, children, and sibling elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation and Setup:\n",
    "\n",
    "Beautiful Soup doesn't work alone - it needs a parser. Here are the most common combinations:\n",
    "\n",
    "```bash\n",
    "# Basic installation\n",
    "pip install beautifulsoup4 requests\n",
    "\n",
    "# With faster parsers\n",
    "pip install lxml html5lib\n",
    "```\n",
    "\n",
    "**Parser Comparison:**\n",
    "- `html.parser` - Built-in Python, decent speed, good enough for most tasks\n",
    "- `lxml` - Very fast, requires C libraries\n",
    "- `html5lib` - Most accurate, handles broken HTML best, but slower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\hayk_\\.conda\\envs\\lectures\\lib\\site-packages (4.13.4)\n",
      "Requirement already satisfied: requests in c:\\users\\hayk_\\.conda\\envs\\lectures\\lib\\site-packages (2.32.4)\n",
      "Requirement already satisfied: lxml in c:\\users\\hayk_\\.conda\\envs\\lectures\\lib\\site-packages (6.0.0)\n",
      "Collecting html5lib\n",
      "  Using cached html5lib-1.1-py2.py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\hayk_\\.conda\\envs\\lectures\\lib\\site-packages (from beautifulsoup4) (2.7)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\hayk_\\.conda\\envs\\lectures\\lib\\site-packages (from beautifulsoup4) (4.14.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\hayk_\\.conda\\envs\\lectures\\lib\\site-packages (from requests) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hayk_\\.conda\\envs\\lectures\\lib\\site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hayk_\\.conda\\envs\\lectures\\lib\\site-packages (from requests) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hayk_\\.conda\\envs\\lectures\\lib\\site-packages (from requests) (2025.6.15)\n",
      "Requirement already satisfied: six>=1.9 in c:\\users\\hayk_\\.conda\\envs\\lectures\\lib\\site-packages (from html5lib) (1.17.0)\n",
      "Collecting webencodings (from html5lib)\n",
      "  Using cached webencodings-0.5.1-py2.py3-none-any.whl.metadata (2.1 kB)\n",
      "Using cached html5lib-1.1-py2.py3-none-any.whl (112 kB)\n",
      "Using cached webencodings-0.5.1-py2.py3-none-any.whl (11 kB)\n",
      "Installing collected packages: webencodings, html5lib\n",
      "\n",
      "   ---------------------------------------- 0/2 [webencodings]\n",
      "   -------------------- ------------------- 1/2 [html5lib]\n",
      "   -------------------- ------------------- 1/2 [html5lib]\n",
      "   -------------------- ------------------- 1/2 [html5lib]\n",
      "   -------------------- ------------------- 1/2 [html5lib]\n",
      "   -------------------- ------------------- 1/2 [html5lib]\n",
      "   -------------------- ------------------- 1/2 [html5lib]\n",
      "   ---------------------------------------- 2/2 [html5lib]\n",
      "\n",
      "Successfully installed html5lib-1.1 webencodings-0.5.1\n"
     ]
    }
   ],
   "source": [
    "# Install required packages for web scraping\n",
    "!pip install beautifulsoup4 requests lxml html5lib pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beautiful Soup Basic Usage - Step by Step\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Step 1: Create a BeautifulSoup object\n",
    "html_content = \"\"\"\n",
    "<html>\n",
    "<head>\n",
    "    <title>My First Scraping Example</title>\n",
    "</head>\n",
    "<body>\n",
    "    <h1 id=\"main-title\">Welcome to Web Scraping!</h1>\n",
    "    <p class=\"intro\">This is a sample paragraph.</p>\n",
    "    <p class=\"content\">This is another paragraph with <a href=\"/link\">a link</a>.</p>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "# Parse the HTML\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "print(\"‚úÖ BeautifulSoup object created successfully!\")\n",
    "print(f\"Type: {type(soup)}\")\n",
    "print(f\"Parser used: html.parser\")\n",
    "\n",
    "# Step 2: Basic navigation\n",
    "print(\"\\nüîç Basic Element Access:\")\n",
    "print(f\"Page title: {soup.title.text}\")\n",
    "print(f\"First h1: {soup.h1.text}\")\n",
    "print(f\"First paragraph: {soup.p.text}\")\n",
    "\n",
    "# Step 3: Pretty print the parsed HTML\n",
    "print(\"\\nüìã Formatted HTML structure:\")\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beautiful Soup Finding Methods - The Core of Scraping\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# More complex HTML for demonstration\n",
    "complex_html = \"\"\"\n",
    "<div class=\"container\">\n",
    "    <article class=\"post\" id=\"post-1\" data-category=\"tech\">\n",
    "        <h2 class=\"post-title\">First Tech Article</h2>\n",
    "        <p class=\"post-content\">Content about technology...</p>\n",
    "        <div class=\"post-meta\">\n",
    "            <span class=\"author\">John Doe</span>\n",
    "            <span class=\"date\">2025-01-15</span>\n",
    "            <a href=\"/tech/article-1\" class=\"read-more\">Read More</a>\n",
    "        </div>\n",
    "    </article>\n",
    "    \n",
    "    <article class=\"post\" id=\"post-2\" data-category=\"science\">\n",
    "        <h2 class=\"post-title\">Science Discovery</h2>\n",
    "        <p class=\"post-content\">Amazing scientific breakthrough...</p>\n",
    "        <div class=\"post-meta\">\n",
    "            <span class=\"author\">Jane Smith</span>\n",
    "            <span class=\"date\">2025-01-16</span>\n",
    "            <a href=\"/science/article-2\" class=\"read-more\">Read More</a>\n",
    "        </div>\n",
    "    </article>\n",
    "</div>\n",
    "\"\"\"\n",
    "\n",
    "soup = BeautifulSoup(complex_html, 'html.parser')\n",
    "\n",
    "print(\"üîç Beautiful Soup Finding Methods:\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Method 1: find() - Returns first match\n",
    "print(\"\\n1Ô∏è‚É£ find() - Get the FIRST matching element:\")\n",
    "first_article = soup.find('article')\n",
    "if first_article:\n",
    "    title = first_article.find('h2').text\n",
    "    print(f\"   First article title: {title}\")\n",
    "\n",
    "# Method 2: find_all() - Returns all matches\n",
    "print(\"\\n2Ô∏è‚É£ find_all() - Get ALL matching elements:\")\n",
    "all_articles = soup.find_all('article')\n",
    "print(f\"   Found {len(all_articles)} articles\")\n",
    "for i, article in enumerate(all_articles, 1):\n",
    "    title = article.find('h2').text\n",
    "    print(f\"   Article {i}: {title}\")\n",
    "\n",
    "# Method 3: Finding by attributes\n",
    "print(\"\\n3Ô∏è‚É£ Finding by attributes:\")\n",
    "tech_article = soup.find('article', {'data-category': 'tech'})\n",
    "if tech_article:\n",
    "    print(f\"   Tech article: {tech_article.find('h2').text}\")\n",
    "\n",
    "science_article = soup.find('article', attrs={'data-category': 'science'})\n",
    "if science_article:\n",
    "    print(f\"   Science article: {science_article.find('h2').text}\")\n",
    "\n",
    "# Method 4: Finding by class (note: class_ because class is a Python keyword)\n",
    "print(\"\\n4Ô∏è‚É£ Finding by class:\")\n",
    "authors = soup.find_all('span', class_='author')\n",
    "print(f\"   Authors found: {[author.text for author in authors]}\")\n",
    "\n",
    "# Method 5: Finding by id\n",
    "print(\"\\n5Ô∏è‚É£ Finding by id:\")\n",
    "post1 = soup.find('article', id='post-1')\n",
    "if post1:\n",
    "    print(f\"   Post 1 title: {post1.find('h2').text}\")\n",
    "\n",
    "print(\"\\nüí° Key Takeaway: find() vs find_all()\")\n",
    "print(\"   find() ‚Üí Returns first match or None\")\n",
    "print(\"   find_all() ‚Üí Returns list of all matches (can be empty)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beautiful Soup with CSS Selectors - The Modern Way\n",
    "# Using the same HTML as previous cell\n",
    "\n",
    "print(\"üéØ CSS Selectors with Beautiful Soup:\")\n",
    "print(\"=\" * 42)\n",
    "\n",
    "# Method 1: select() - CSS selectors (returns list)\n",
    "print(\"\\n1Ô∏è‚É£ select() - CSS selectors (returns list):\")\n",
    "post_titles = soup.select('.post-title')\n",
    "print(f\"   Found {len(post_titles)} titles using '.post-title'\")\n",
    "for title in post_titles:\n",
    "    print(f\"   ‚Ä¢ {title.text}\")\n",
    "\n",
    "# Method 2: select_one() - First match only\n",
    "print(\"\\n2Ô∏è‚É£ select_one() - First match only:\")\n",
    "first_author = soup.select_one('.author')\n",
    "if first_author:\n",
    "    print(f\"   First author: {first_author.text}\")\n",
    "\n",
    "# Method 3: Complex CSS selectors\n",
    "print(\"\\n3Ô∏è‚É£ Complex CSS selectors:\")\n",
    "tech_title = soup.select_one('article[data-category=\"tech\"] .post-title')\n",
    "if tech_title:\n",
    "    print(f\"   Tech article title: {tech_title.text}\")\n",
    "\n",
    "read_more_links = soup.select('.post-meta .read-more')\n",
    "print(f\"   Read more links: {[link.get('href') for link in read_more_links]}\")\n",
    "\n",
    "# Method 4: Combining selectors\n",
    "print(\"\\n4Ô∏è‚É£ Advanced combinations:\")\n",
    "all_meta_spans = soup.select('.post-meta span')\n",
    "print(f\"   Meta spans: {[span.text for span in all_meta_spans]}\")\n",
    "\n",
    "# Method 5: Pseudo-selectors\n",
    "print(\"\\n5Ô∏è‚É£ Pseudo-selectors:\")\n",
    "first_post = soup.select_one('article:first-child .post-title')\n",
    "if first_post:\n",
    "    print(f\"   First post: {first_post.text}\")\n",
    "\n",
    "last_post = soup.select_one('article:last-child .post-title')\n",
    "if last_post:\n",
    "    print(f\"   Last post: {last_post.text}\")\n",
    "\n",
    "print(\"\\nüìù Summary of Selector Methods:\")\n",
    "print(\"   find/find_all + attributes ‚Üí Traditional Beautiful Soup\")\n",
    "print(\"   select/select_one + CSS     ‚Üí Modern, CSS-like approach\")\n",
    "print(\"   üí° Use CSS selectors for complex queries!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beautiful Soup Complete Example - Part 1: Setup and Parsing\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# Sample HTML for demonstration\n",
    "sample_html = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "    <title>Sample Blog</title>\n",
    "</head>\n",
    "<body>\n",
    "    <header id=\"main-header\">\n",
    "        <h1>My Amazing Blog</h1>\n",
    "        <nav class=\"navigation\">\n",
    "            <ul>\n",
    "                <li><a href=\"#home\">Home</a></li>\n",
    "                <li><a href=\"#about\">About</a></li>\n",
    "                <li><a href=\"#contact\">Contact</a></li>\n",
    "            </ul>\n",
    "        </nav>\n",
    "    </header>\n",
    "    \n",
    "    <main class=\"content\">\n",
    "        <article class=\"post featured\" data-id=\"123\">\n",
    "            <h2 class=\"post-title\">How to Learn Web Scraping</h2>\n",
    "            <p class=\"post-content\">Web scraping is an essential skill for data scientists...</p>\n",
    "            <div class=\"post-meta\">\n",
    "                <span class=\"author\">Alice Johnson</span>\n",
    "                <span class=\"date\">2025-01-15</span>\n",
    "                <span class=\"category\">Technology</span>\n",
    "            </div>\n",
    "        </article>\n",
    "        \n",
    "        <article class=\"post\" data-id=\"124\">\n",
    "            <h2 class=\"post-title\">Python Data Analysis Tips</h2>\n",
    "            <p class=\"post-content\">Here are some advanced tips for analyzing data with Python...</p>\n",
    "            <div class=\"post-meta\">\n",
    "                <span class=\"author\">Bob Smith</span>\n",
    "                <span class=\"date\">2025-01-16</span>\n",
    "                <span class=\"category\">Data Science</span>\n",
    "            </div>\n",
    "        </article>\n",
    "    </main>\n",
    "    \n",
    "    <footer>\n",
    "        <p>&copy; 2025 My Blog. All rights reserved.</p>\n",
    "    </footer>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "# Create BeautifulSoup object\n",
    "soup = BeautifulSoup(sample_html, 'html.parser')\n",
    "\n",
    "print(\"ü•Ñ Beautiful Soup Complete Example - Part 1\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"\\n‚úÖ Step 1: Parse HTML into BeautifulSoup object\")\n",
    "print(f\"   Object type: {type(soup)}\")\n",
    "print(f\"   Parser used: html.parser\")\n",
    "\n",
    "print(\"\\nüìã Step 2: Examine the structure\")\n",
    "print(f\"   Page title: {soup.title.text}\")\n",
    "print(f\"   Number of articles: {len(soup.find_all('article'))}\")\n",
    "print(f\"   Number of links: {len(soup.find_all('a'))}\")\n",
    "\n",
    "print(\"\\n\udd0d Step 3: Basic element access\")\n",
    "# Direct access to first element of each type\n",
    "print(f\"   First h1: {soup.h1.text}\")\n",
    "print(f\"   First article title: {soup.find('h2').text}\")\n",
    "print(f\"   Footer text: {soup.footer.p.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beautiful Soup Complete Example - Part 2: Finding Elements\n",
    "# Using the same soup object from Part 1\n",
    "\n",
    "print(\"ü•Ñ Beautiful Soup Complete Example - Part 2\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"\\nüîç Different Ways to Find Elements:\")\n",
    "\n",
    "# Method 1: By tag name\n",
    "print(\"\\n1Ô∏è‚É£ Find by tag name:\")\n",
    "titles = soup.find_all('h2')\n",
    "print(f\"   Found {len(titles)} h2 elements:\")\n",
    "for i, title in enumerate(titles, 1):\n",
    "    print(f\"     {i}. {title.text}\")\n",
    "\n",
    "# Method 2: By class\n",
    "print(\"\\n2Ô∏è‚É£ Find by class:\")\n",
    "posts = soup.find_all('article', class_='post')\n",
    "print(f\"   Found {len(posts)} articles with class 'post'\")\n",
    "\n",
    "# Method 3: By ID\n",
    "print(\"\\n3Ô∏è‚É£ Find by ID:\")\n",
    "header = soup.find('header', id='main-header')\n",
    "if header:\n",
    "    print(f\"   Header found: {header.h1.text}\")\n",
    "\n",
    "# Method 4: By multiple attributes\n",
    "print(\"\\n4Ô∏è‚É£ Find by multiple attributes:\")\n",
    "featured_post = soup.find('article', {'class': 'post featured'})\n",
    "if featured_post:\n",
    "    print(f\"   Featured post: {featured_post.find('h2').text}\")\n",
    "\n",
    "# Method 5: By custom attributes\n",
    "print(\"\\n5Ô∏è‚É£ Find by custom attributes:\")\n",
    "post_123 = soup.find('article', {'data-id': '123'})\n",
    "if post_123:\n",
    "    author = post_123.find('span', class_='author').text\n",
    "    print(f\"   Post 123 author: {author}\")\n",
    "\n",
    "print(\"\\nüí° Key Takeaway:\")\n",
    "print(\"   find() ‚Üí First match or None\")\n",
    "print(\"   find_all() ‚Üí List of all matches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beautiful Soup Complete Example - Part 3: Data Extraction & Navigation\n",
    "# Using the same soup object from previous parts\n",
    "\n",
    "print(\"ü•Ñ Beautiful Soup Complete Example - Part 3\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"\\nüìä Extract Structured Data from All Articles:\")\n",
    "\n",
    "articles = soup.find_all('article', class_='post')\n",
    "blog_data = []\n",
    "\n",
    "for i, article in enumerate(articles, 1):\n",
    "    # Extract data from each article\n",
    "    title = article.find('h2', class_='post-title').text\n",
    "    content = article.find('p', class_='post-content').text\n",
    "    author = article.find('span', class_='author').text\n",
    "    date = article.find('span', class_='date').text\n",
    "    category = article.find('span', class_='category').text\n",
    "    data_id = article.get('data-id')  # Get attribute value\n",
    "    is_featured = 'featured' in article.get('class', [])\n",
    "    \n",
    "    # Store in dictionary\n",
    "    article_data = {\n",
    "        'id': data_id,\n",
    "        'title': title,\n",
    "        'author': author,\n",
    "        'date': date,\n",
    "        'category': category,\n",
    "        'content_preview': content[:50] + \"...\",\n",
    "        'is_featured': is_featured\n",
    "    }\n",
    "    \n",
    "    blog_data.append(article_data)\n",
    "    \n",
    "    print(f\"\\nüìÑ Article {i}:\")\n",
    "    print(f\"   ID: {article_data['id']}\")\n",
    "    print(f\"   Title: {article_data['title']}\")\n",
    "    print(f\"   Author: {article_data['author']}\")\n",
    "    print(f\"   Category: {article_data['category']}\")\n",
    "    print(f\"   Featured: {'‚úÖ' if article_data['is_featured'] else '‚ùå'}\")\n",
    "\n",
    "print(\"\\nüß≠ Navigation Examples:\")\n",
    "\n",
    "# Parent-child navigation\n",
    "first_article = articles[0]\n",
    "post_meta = first_article.find('div', class_='post-meta')\n",
    "print(f\"\\n   Parent of author span: {post_meta.name}\")\n",
    "print(f\"   Children of post-meta: {[child.name for child in post_meta.children if child.name]}\")\n",
    "\n",
    "# Sibling navigation  \n",
    "author_span = first_article.find('span', class_='author')\n",
    "next_sibling = author_span.find_next_sibling('span')\n",
    "if next_sibling:\n",
    "    print(f\"   Next sibling of author: {next_sibling.text} ({next_sibling.get('class')})\")\n",
    "\n",
    "print(f\"\\nüìà Summary Statistics:\")\n",
    "print(f\"   Total articles processed: {len(blog_data)}\")\n",
    "print(f\"   Featured articles: {sum(1 for article in blog_data if article['is_featured'])}\")\n",
    "print(f\"   Unique authors: {len(set(article['author'] for article in blog_data))}\")\n",
    "print(f\"   Categories: {list(set(article['category'] for article in blog_data))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üåê Real Website Scraping Example\n",
    "\n",
    "Let's scrape some real data from a website. We'll use `httpbin.org` which provides testing endpoints:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import required libraries for web scraping\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import json\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Define the scraping function\n",
    "def scrape_quotes():\n",
    "    \"\"\"\n",
    "    Scrape quotes from quotes.toscrape.com\n",
    "    Returns a list of dictionaries containing quote data\n",
    "    \"\"\"\n",
    "    url = \"http://quotes.toscrape.com/\"\n",
    "    \n",
    "    try:\n",
    "        print(f\"üåê Sending request to: {url}\")\n",
    "        \n",
    "        # Send GET request to the website\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raise exception for bad status codes\n",
    "        \n",
    "        print(f\"‚úÖ Request successful! Status code: {response.status_code}\")\n",
    "        \n",
    "        # Parse HTML content with Beautiful Soup\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Find all quote containers\n",
    "        quotes = soup.find_all('div', class_='quote')\n",
    "        print(f\"üìä Found {len(quotes)} quotes on the page\")\n",
    "        \n",
    "        scraped_data = []\n",
    "        \n",
    "        # Extract data from each quote\n",
    "        for i, quote in enumerate(quotes, 1):\n",
    "            # Extract quote text (remove quotes and whitespace)\n",
    "            text = quote.find('span', class_='text').text\n",
    "            \n",
    "            # Extract author name\n",
    "            author = quote.find('small', class_='author').text\n",
    "            \n",
    "            # Extract tags (multiple tags per quote)\n",
    "            tags = [tag.text for tag in quote.find_all('a', class_='tag')]\n",
    "            \n",
    "            # Store data in dictionary\n",
    "            quote_data = {\n",
    "                'text': text,\n",
    "                'author': author,\n",
    "                'tags': tags\n",
    "            }\n",
    "            \n",
    "            scraped_data.append(quote_data)\n",
    "            print(f\"  üìù Processed quote {i}: {author}\")\n",
    "        \n",
    "        return scraped_data\n",
    "    \n",
    "    except requests.RequestException as e:\n",
    "        print(f\"‚ùå Error fetching the webpage: {e}\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error processing data: {e}\")\n",
    "        return []\n",
    "\n",
    "print(\"‚úÖ Function defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Run the scraper and collect data\n",
    "print(\"üï∑Ô∏è Starting the scraping process...\")\n",
    "quotes_data = scrape_quotes()\n",
    "\n",
    "print(f\"\\nüìä Scraping completed!\")\n",
    "print(f\"Total quotes collected: {len(quotes_data)}\")\n",
    "\n",
    "if quotes_data:\n",
    "    print(\"\\nüéØ Sample of scraped data:\")\n",
    "    print(\"=\" * 60)\n",
    "else:\n",
    "    print(\"‚ùå No quotes were scraped. Check your internet connection.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Display the first few quotes to see our results\n",
    "if quotes_data:\n",
    "    print(\"üìù First 3 quotes from our scraping:\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for i, quote in enumerate(quotes_data[:3], 1):\n",
    "        print(f\"\\nüí¨ Quote {i}:\")\n",
    "        print(f\"   Text: {quote['text']}\")\n",
    "        print(f\"   Author: {quote['author']}\")\n",
    "        print(f\"   Tags: {', '.join(quote['tags'])}\")\n",
    "        print(\"-\" * 60)\n",
    "    \n",
    "    # Show some statistics\n",
    "    print(f\"\\nüìà Quick Statistics:\")\n",
    "    print(f\"   Total quotes: {len(quotes_data)}\")\n",
    "    \n",
    "    # Find unique authors\n",
    "    authors = set(quote['author'] for quote in quotes_data)\n",
    "    print(f\"   Unique authors: {len(authors)}\")\n",
    "    \n",
    "    # Find all unique tags\n",
    "    all_tags = set()\n",
    "    for quote in quotes_data:\n",
    "        all_tags.update(quote['tags'])\n",
    "    print(f\"   Unique tags: {len(all_tags)}\")\n",
    "    print(f\"   Some tags: {', '.join(list(all_tags)[:5])}...\")\n",
    "else:\n",
    "    print(\"‚ùå No data to display\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Save the scraped data to a file\n",
    "if quotes_data:\n",
    "    # Save to JSON file with proper formatting\n",
    "    filename = 'quotes_scraped.json'\n",
    "    \n",
    "    try:\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(quotes_data, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        print(f\"üíæ Data saved successfully to '{filename}'\")\n",
    "        print(f\"üìÅ File contains {len(quotes_data)} quotes\")\n",
    "        \n",
    "        # Show file size\n",
    "        import os\n",
    "        file_size = os.path.getsize(filename)\n",
    "        print(f\"üìä File size: {file_size:,} bytes\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error saving file: {e}\")\n",
    "        \n",
    "    # Also demonstrate saving specific data\n",
    "    print(f\"\\nüîç Example of accessing specific quote data:\")\n",
    "    if len(quotes_data) > 0:\n",
    "        first_quote = quotes_data[0]\n",
    "        print(f\"   First quote text: {first_quote['text'][:50]}...\")\n",
    "        print(f\"   First quote author: {first_quote['author']}\")\n",
    "        print(f\"   First quote tags: {first_quote['tags']}\")\n",
    "else:\n",
    "    print(\"‚ùå No data to save\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üéì What We Just Did - Step by Step:\n",
    "\n",
    "1. **üì¶ Imported Libraries**: We imported the essential tools:\n",
    "   - `requests` for making HTTP requests\n",
    "   - `BeautifulSoup` for parsing HTML\n",
    "   - `json` for saving data\n",
    "   - `time` for adding delays (good practice)\n",
    "\n",
    "2. **üîß Created Function**: We defined `scrape_quotes()` that:\n",
    "   - Sends a GET request to the website\n",
    "   - Handles errors gracefully\n",
    "   - Parses HTML with Beautiful Soup\n",
    "   - Extracts specific data using CSS selectors\n",
    "\n",
    "3. **üöÄ Executed Scraper**: We ran the function and collected data\n",
    "\n",
    "4. **üëÄ Viewed Results**: We displayed the scraped quotes to verify success\n",
    "\n",
    "5. **üíæ Saved Data**: We saved the results to a JSON file for future use\n",
    "\n",
    "**Key Learning Points:**\n",
    "- Always check `response.status_code` to ensure successful requests\n",
    "- Use `.find()` for single elements and `.find_all()` for multiple elements\n",
    "- Handle exceptions to make your scraper robust\n",
    "- Save data in structured formats like JSON or CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üè† Real Website Scraping Example\n",
    "\n",
    "Let's create a more complex scraper that demonstrates real-world techniques. This example shows how to:\n",
    "- Handle multiple pages\n",
    "- Extract structured data\n",
    "- Process and analyze results\n",
    "- Implement basic error handling\n",
    "\n",
    "**Key Learning Points:**\n",
    "- Always check a website's robots.txt before scraping\n",
    "- Add appropriate delays between requests\n",
    "- Handle errors gracefully\n",
    "- Structure your extracted data properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List.am Real Estate Scraper\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import json\n",
    "\n",
    "def scrape_listam_listings(base_url=\"https://www.list.am/category/62\", max_pages=2, delay=2):\n",
    "    \"\"\"\n",
    "    Scrape real estate listings from list.am\n",
    "    \n",
    "    Args:\n",
    "        base_url (str): Base URL for the category\n",
    "        max_pages (int): Maximum number of pages to scrape\n",
    "        delay (int): Delay between requests in seconds\n",
    "    \n",
    "    Returns:\n",
    "        list: List of dictionaries containing listing data\n",
    "    \"\"\"\n",
    "    \n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "        'Accept-Language': 'en-US,en;q=0.5',\n",
    "        'Accept-Encoding': 'gzip, deflate',\n",
    "        'Connection': 'keep-alive',\n",
    "        'Upgrade-Insecure-Requests': '1',\n",
    "    }\n",
    "    \n",
    "    all_listings = []\n",
    "    \n",
    "    for page in range(1, max_pages + 1):\n",
    "        try:\n",
    "            # Construct page URL\n",
    "            if page == 1:\n",
    "                page_url = base_url\n",
    "            else:\n",
    "                page_url = f\"{base_url}/{page}\"\n",
    "            \n",
    "            print(f\"üîç Scraping page {page}: {page_url}\")\n",
    "            \n",
    "            # Add delay to be respectful\n",
    "            if page > 1:\n",
    "                time.sleep(delay)\n",
    "            \n",
    "            # Send request\n",
    "            response = requests.get(page_url, headers=headers, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            # Parse HTML\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            # Find listing containers (adjust selectors based on actual HTML structure)\n",
    "            listings = soup.find_all('a', href=True)\n",
    "            \n",
    "            page_listings = []\n",
    "            \n",
    "            for listing in listings:\n",
    "                href = listing.get('href', '')\n",
    "                \n",
    "                # Filter for item links\n",
    "                if '/item/' in href and href.startswith('/item/'):\n",
    "                    # Extract item ID\n",
    "                    item_match = re.search(r'/item/(\\d+)', href)\n",
    "                    if not item_match:\n",
    "                        continue\n",
    "                    \n",
    "                    item_id = item_match.group(1)\n",
    "                    full_url = urljoin(base_url, href)\n",
    "                    \n",
    "                    # Extract text content from the link\n",
    "                    text_content = listing.get_text(strip=True)\n",
    "                    \n",
    "                    # Parse listing information from text\n",
    "                    listing_data = parse_listing_text(text_content, item_id, full_url)\n",
    "                    \n",
    "                    if listing_data:\n",
    "                        page_listings.append(listing_data)\n",
    "            \n",
    "            print(f\"   ‚úÖ Found {len(page_listings)} listings on page {page}\")\n",
    "            all_listings.extend(page_listings)\n",
    "            \n",
    "            # Check if there's a next page\n",
    "            next_link = soup.find('a', string='’Ä’°’ª’∏÷Ä’§’® >')\n",
    "            if not next_link and page == max_pages:\n",
    "                print(\"üìÑ Reached last page or max pages limit\")\n",
    "                break\n",
    "                \n",
    "        except requests.RequestException as e:\n",
    "            print(f\"‚ùå Error fetching page {page}: {e}\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error parsing page {page}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return all_listings\n",
    "\n",
    "def parse_listing_text(text, item_id, url):\n",
    "    \"\"\"\n",
    "    Parse listing information from text content\n",
    "    \n",
    "    Args:\n",
    "        text (str): Text content of the listing\n",
    "        item_id (str): Item ID\n",
    "        url (str): Full URL to the listing\n",
    "    \n",
    "    Returns:\n",
    "        dict: Parsed listing data\n",
    "    \"\"\"\n",
    "    \n",
    "    if not text or len(text.strip()) < 10:\n",
    "        return None\n",
    "    \n",
    "    # Initialize listing data\n",
    "    listing = {\n",
    "        'id': item_id,\n",
    "        'url': url,\n",
    "        'raw_text': text.strip(),\n",
    "        'price': None,\n",
    "        'price_currency': None,\n",
    "        'location': None,\n",
    "        'property_type': None,\n",
    "        'area_sqm': None,\n",
    "        'rooms': None,\n",
    "        'floor': None,\n",
    "        'description': None\n",
    "    }\n",
    "    \n",
    "    # Extract price (handles both USD and AMD)\n",
    "    price_usd_match = re.search(r'\\$([0-9,]+(?:\\.[0-9]+)?)', text)\n",
    "    price_amd_match = re.search(r'([0-9,]+(?:\\.[0-9]+)?)\\s*÷è', text)\n",
    "    \n",
    "    if price_usd_match:\n",
    "        listing['price'] = price_usd_match.group(1).replace(',', '')\n",
    "        listing['price_currency'] = 'USD'\n",
    "    elif price_amd_match:\n",
    "        listing['price'] = price_amd_match.group(1).replace(',', '')\n",
    "        listing['price_currency'] = 'AMD'\n",
    "    \n",
    "    # Extract area (square meters)\n",
    "    area_match = re.search(r'(\\d+)\\s*÷Ñ’¥', text)\n",
    "    if area_match:\n",
    "        listing['area_sqm'] = area_match.group(1)\n",
    "    \n",
    "    # Extract number of rooms\n",
    "    rooms_match = re.search(r'(\\d+)\\s*’Ω’•’∂', text)\n",
    "    if rooms_match:\n",
    "        listing['rooms'] = rooms_match.group(1)\n",
    "    \n",
    "    # Extract floor information\n",
    "    floor_match = re.search(r'(\\d+)/(\\d+)\\s*’∞’°÷Ä’Ø', text)\n",
    "    if floor_match:\n",
    "        listing['floor'] = f\"{floor_match.group(1)}/{floor_match.group(2)}\"\n",
    "    \n",
    "    # Extract location (common locations in Yerevan)\n",
    "    locations = [\n",
    "        '‘ø’•’∂’ø÷Ä’∏’∂', '‘±÷Ä’°’¢’Ø’´÷Ä', '‘¥’°’æ’©’°’∑’•’∂', '’Ñ’°’¨’°’©’´’°-’ç’•’¢’°’Ω’ø’´’°', \n",
    "        '’á’•’∂’£’°’æ’´’©', '’Ü’∏÷Ä ’Ü’∏÷Ä÷Ñ', '‘±’ª’°÷É’∂’µ’°’Ø', '‘±’æ’°’∂', '‘∑÷Ä’•’¢’∏÷Ç’∂’´',\n",
    "        '‘≥’µ’∏÷Ç’¥÷Ä’´', '’é’°’∂’°’±’∏÷Ä', '‘±’¢’∏’æ’µ’°’∂', '‘±÷Ä’ø’°’∑’°’ø', '‘≥÷á’°÷Ä÷Ñ',\n",
    "        '‘æ’°’≤’Ø’°’±’∏÷Ä', '‘¥’´’¨’´’ª’°’∂', '‘ª’ª÷á’°’∂', '‘≥’∏÷Ä’´’Ω', '‘ø’°’∫’°’∂'\n",
    "    ]\n",
    "    \n",
    "    for location in locations:\n",
    "        if location in text:\n",
    "            listing['location'] = location\n",
    "            break\n",
    "    \n",
    "    # Determine property type based on keywords\n",
    "    if '’¢’∂’°’Ø’°÷Ä’°’∂' in text:\n",
    "        listing['property_type'] = 'Apartment'\n",
    "    elif '’ø’∏÷Ç’∂' in text or '’©’°’∏÷Ç’∂’∞’°’∏÷Ç’¶' in text:\n",
    "        listing['property_type'] = 'House'\n",
    "    elif '’∞’∏’≤’°’ø’°÷Ä’°’Æ÷Ñ' in text:\n",
    "        listing['property_type'] = 'Land'\n",
    "    elif '’°’æ’ø’∏’ø’∂’°’Ø' in text:\n",
    "        listing['property_type'] = 'Garage'\n",
    "    elif '’£÷Ä’°’Ω’•’∂’µ’°’Ø' in text:\n",
    "        listing['property_type'] = 'Office'\n",
    "    else:\n",
    "        listing['property_type'] = 'Other'\n",
    "    \n",
    "    # Clean description (remove price and location)\n",
    "    description = text\n",
    "    if listing['price'] and listing['price_currency']:\n",
    "        price_pattern = rf\"\\${listing['price']}|{listing['price']}\\s*÷è\"\n",
    "        description = re.sub(price_pattern, '', description)\n",
    "    \n",
    "    if listing['location']:\n",
    "        description = description.replace(listing['location'], '')\n",
    "    \n",
    "    listing['description'] = description.strip()\n",
    "    \n",
    "    return listing\n",
    "\n",
    "def analyze_listings(listings):\n",
    "    \"\"\"\n",
    "    Analyze scraped listings and provide statistics\n",
    "    \n",
    "    Args:\n",
    "        listings (list): List of listing dictionaries\n",
    "    \n",
    "    Returns:\n",
    "        dict: Analysis results\n",
    "    \"\"\"\n",
    "    \n",
    "    if not listings:\n",
    "        return {}\n",
    "    \n",
    "    df = pd.DataFrame(listings)\n",
    "    \n",
    "    # Convert price to numeric for analysis\n",
    "    df['price_numeric'] = pd.to_numeric(df['price'].str.replace(',', ''), errors='coerce')\n",
    "    df['area_numeric'] = pd.to_numeric(df['area_sqm'], errors='coerce')\n",
    "    df['rooms_numeric'] = pd.to_numeric(df['rooms'], errors='coerce')\n",
    "    \n",
    "    analysis = {\n",
    "        'total_listings': len(listings),\n",
    "        'unique_locations': df['location'].nunique(),\n",
    "        'property_types': df['property_type'].value_counts().to_dict(),\n",
    "        'currency_distribution': df['price_currency'].value_counts().to_dict(),\n",
    "        'price_stats': {},\n",
    "        'area_stats': {},\n",
    "        'location_stats': df['location'].value_counts().head(10).to_dict()\n",
    "    }\n",
    "    \n",
    "    # Price statistics (for USD listings)\n",
    "    usd_prices = df[df['price_currency'] == 'USD']['price_numeric'].dropna()\n",
    "    if len(usd_prices) > 0:\n",
    "        analysis['price_stats']['USD'] = {\n",
    "            'count': len(usd_prices),\n",
    "            'mean': round(usd_prices.mean(), 2),\n",
    "            'median': round(usd_prices.median(), 2),\n",
    "            'min': usd_prices.min(),\n",
    "            'max': usd_prices.max()\n",
    "        }\n",
    "    \n",
    "    # Area statistics\n",
    "    areas = df['area_numeric'].dropna()\n",
    "    if len(areas) > 0:\n",
    "        analysis['area_stats'] = {\n",
    "            'count': len(areas),\n",
    "            'mean': round(areas.mean(), 2),\n",
    "            'median': round(areas.median(), 2),\n",
    "            'min': areas.min(),\n",
    "            'max': areas.max()\n",
    "        }\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "# Example usage\n",
    "print(\"üè† Starting List.am Real Estate Scraper...\")\n",
    "print(\"‚ö†Ô∏è  Remember: This is for educational purposes only!\")\n",
    "print(\"üïê Adding delays between requests to be respectful...\")\n",
    "\n",
    "# Scrape listings (limiting to 2 pages for demo)\n",
    "listings = scrape_listam_listings(max_pages=2, delay=3)\n",
    "\n",
    "print(f\"\\nüìä Scraping completed! Total listings found: {len(listings)}\")\n",
    "\n",
    "if listings:\n",
    "    print(\"\\nüè† Sample listings:\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for i, listing in enumerate(listings[:5], 1):\n",
    "        print(f\"\\n{i}. ID: {listing['id']}\")\n",
    "        print(f\"   Type: {listing['property_type']}\")\n",
    "        print(f\"   Price: {listing['price']} {listing['price_currency'] or 'N/A'}\")\n",
    "        print(f\"   Location: {listing['location'] or 'N/A'}\")\n",
    "        print(f\"   Area: {listing['area_sqm']} sqm\" if listing['area_sqm'] else \"   Area: N/A\")\n",
    "        print(f\"   Rooms: {listing['rooms']}\" if listing['rooms'] else \"   Rooms: N/A\")\n",
    "        print(f\"   Description: {listing['description'][:60]}...\")\n",
    "        print(f\"   URL: {listing['url']}\")\n",
    "else:\n",
    "    print(\"‚ùå No listings found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Analysis and Visualization\n",
    "if listings:\n",
    "    print(\"\\nüìà Analyzing scraped data...\")\n",
    "    \n",
    "    # Perform analysis\n",
    "    analysis = analyze_listings(listings)\n",
    "    \n",
    "    print(f\"\\nüìä Analysis Results:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"üìã Total listings: {analysis['total_listings']}\")\n",
    "    print(f\"üèôÔ∏è Unique locations: {analysis['unique_locations']}\")\n",
    "    \n",
    "    print(f\"\\nüè† Property types:\")\n",
    "    for prop_type, count in analysis['property_types'].items():\n",
    "        print(f\"   {prop_type}: {count}\")\n",
    "    \n",
    "    print(f\"\\nüí∞ Currency distribution:\")\n",
    "    for currency, count in analysis['currency_distribution'].items():\n",
    "        if currency:  # Skip None values\n",
    "            print(f\"   {currency}: {count}\")\n",
    "    \n",
    "    if 'USD' in analysis['price_stats']:\n",
    "        usd_stats = analysis['price_stats']['USD']\n",
    "        print(f\"\\nüíµ USD Price statistics:\")\n",
    "        print(f\"   Count: {usd_stats['count']}\")\n",
    "        print(f\"   Average: ${usd_stats['mean']:,.2f}\")\n",
    "        print(f\"   Median: ${usd_stats['median']:,.2f}\")\n",
    "        print(f\"   Range: ${usd_stats['min']:,.0f} - ${usd_stats['max']:,.0f}\")\n",
    "    \n",
    "    if analysis['area_stats']:\n",
    "        area_stats = analysis['area_stats']\n",
    "        print(f\"\\nüìê Area statistics (sqm):\")\n",
    "        print(f\"   Count: {area_stats['count']}\")\n",
    "        print(f\"   Average: {area_stats['mean']:.1f} sqm\")\n",
    "        print(f\"   Median: {area_stats['median']:.1f} sqm\")\n",
    "        print(f\"   Range: {area_stats['min']} - {area_stats['max']} sqm\")\n",
    "    \n",
    "    print(f\"\\nüó∫Ô∏è Top locations:\")\n",
    "    for location, count in list(analysis['location_stats'].items())[:5]:\n",
    "        if location:  # Skip None values\n",
    "            print(f\"   {location}: {count}\")\n",
    "    \n",
    "    # Save data to CSV\n",
    "    df = pd.DataFrame(listings)\n",
    "    filename = f'listam_listings_{pd.Timestamp.now().strftime(\"%Y%m%d_%H%M%S\")}.csv'\n",
    "    df.to_csv(filename, index=False, encoding='utf-8')\n",
    "    print(f\"\\nüíæ Data saved to: {filename}\")\n",
    "    \n",
    "    # Save analysis to JSON\n",
    "    analysis_filename = f'listam_analysis_{pd.Timestamp.now().strftime(\"%Y%m%d_%H%M%S\")}.json'\n",
    "    with open(analysis_filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(analysis, f, ensure_ascii=False, indent=2, default=str)\n",
    "    print(f\"üìä Analysis saved to: {analysis_filename}\")\n",
    "else:\n",
    "    print(\"‚ùå No data to analyze\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced List.am Scraping Techniques\n",
    "\n",
    "def scrape_detailed_listing(listing_url, headers=None):\n",
    "    \"\"\"\n",
    "    Scrape detailed information from a single listing page\n",
    "    \n",
    "    Args:\n",
    "        listing_url (str): URL of the specific listing\n",
    "        headers (dict): HTTP headers to use\n",
    "    \n",
    "    Returns:\n",
    "        dict: Detailed listing information\n",
    "    \"\"\"\n",
    "    \n",
    "    if headers is None:\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "        }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(listing_url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Extract detailed information (adjust selectors based on actual page structure)\n",
    "        details = {\n",
    "            'url': listing_url,\n",
    "            'title': None,\n",
    "            'price': None,\n",
    "            'description': None,\n",
    "            'contact_info': None,\n",
    "            'images': [],\n",
    "            'features': [],\n",
    "            'posted_date': None\n",
    "        }\n",
    "        \n",
    "        # Extract title\n",
    "        title_element = soup.find('h1') or soup.find('title')\n",
    "        if title_element:\n",
    "            details['title'] = title_element.get_text(strip=True)\n",
    "        \n",
    "        # Extract description\n",
    "        desc_selectors = [\n",
    "            'div.description', \n",
    "            'div.content', \n",
    "            '.item-description',\n",
    "            'p'\n",
    "        ]\n",
    "        \n",
    "        for selector in desc_selectors:\n",
    "            desc_element = soup.select_one(selector)\n",
    "            if desc_element and len(desc_element.get_text(strip=True)) > 50:\n",
    "                details['description'] = desc_element.get_text(strip=True)\n",
    "                break\n",
    "        \n",
    "        # Extract images\n",
    "        img_elements = soup.find_all('img', src=True)\n",
    "        for img in img_elements:\n",
    "            src = img.get('src')\n",
    "            if src and ('jpg' in src or 'jpeg' in src or 'png' in src):\n",
    "                full_img_url = urljoin(listing_url, src)\n",
    "                details['images'].append(full_img_url)\n",
    "        \n",
    "        # Extract contact information (phone numbers)\n",
    "        text_content = soup.get_text()\n",
    "        phone_patterns = [\n",
    "            r'\\+374\\s?\\d{2}\\s?\\d{3}\\s?\\d{3}',  # Armenian format\n",
    "            r'0\\d{2}\\s?\\d{3}\\s?\\d{3}',        # Local format\n",
    "            r'\\d{2}-\\d{2}-\\d{2}'              # Alternative format\n",
    "        ]\n",
    "        \n",
    "        for pattern in phone_patterns:\n",
    "            phones = re.findall(pattern, text_content)\n",
    "            if phones:\n",
    "                details['contact_info'] = phones[0]\n",
    "                break\n",
    "        \n",
    "        return details\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error scraping detailed listing {listing_url}: {e}\")\n",
    "        return {}\n",
    "\n",
    "def create_price_monitor(target_criteria, check_interval=3600):\n",
    "    \"\"\"\n",
    "    Create a price monitoring system for specific criteria\n",
    "    \n",
    "    Args:\n",
    "        target_criteria (dict): Criteria to monitor (location, max_price, min_area, etc.)\n",
    "        check_interval (int): Check interval in seconds\n",
    "    \n",
    "    Returns:\n",
    "        function: Monitoring function\n",
    "    \"\"\"\n",
    "    \n",
    "    def monitor():\n",
    "        print(f\"üîç Monitoring for: {target_criteria}\")\n",
    "        \n",
    "        # Get current listings\n",
    "        current_listings = scrape_listam_listings(max_pages=1, delay=2)\n",
    "        \n",
    "        matching_listings = []\n",
    "        \n",
    "        for listing in current_listings:\n",
    "            matches = True\n",
    "            \n",
    "            # Check location\n",
    "            if 'location' in target_criteria:\n",
    "                if listing['location'] != target_criteria['location']:\n",
    "                    matches = False\n",
    "            \n",
    "            # Check max price\n",
    "            if 'max_price_usd' in target_criteria and listing['price'] and listing['price_currency'] == 'USD':\n",
    "                try:\n",
    "                    price = float(listing['price'].replace(',', ''))\n",
    "                    if price > target_criteria['max_price_usd']:\n",
    "                        matches = False\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            # Check minimum area\n",
    "            if 'min_area' in target_criteria and listing['area_sqm']:\n",
    "                try:\n",
    "                    area = int(listing['area_sqm'])\n",
    "                    if area < target_criteria['min_area']:\n",
    "                        matches = False\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            # Check property type\n",
    "            if 'property_type' in target_criteria:\n",
    "                if listing['property_type'] != target_criteria['property_type']:\n",
    "                    matches = False\n",
    "            \n",
    "            if matches:\n",
    "                matching_listings.append(listing)\n",
    "        \n",
    "        if matching_listings:\n",
    "            print(f\"üéØ Found {len(matching_listings)} matching listings:\")\n",
    "            for listing in matching_listings:\n",
    "                print(f\"   - {listing['property_type']} in {listing['location']}: {listing['price']} {listing['price_currency']}\")\n",
    "                print(f\"     URL: {listing['url']}\")\n",
    "        else:\n",
    "            print(\"‚ùå No matching listings found\")\n",
    "        \n",
    "        return matching_listings\n",
    "    \n",
    "    return monitor\n",
    "\n",
    "# Example: Monitor for apartments in Kentron under $200,000\n",
    "print(\"\\nüéØ Setting up price monitoring example...\")\n",
    "monitor_criteria = {\n",
    "    'location': '‘ø’•’∂’ø÷Ä’∏’∂',\n",
    "    'max_price_usd': 200000,\n",
    "    'min_area': 50,\n",
    "    'property_type': 'Apartment'\n",
    "}\n",
    "\n",
    "price_monitor = create_price_monitor(monitor_criteria)\n",
    "\n",
    "print(\"\\nüí° Price monitor created! You can run price_monitor() to check for matching listings.\")\n",
    "print(\"üîÑ In a real application, you would schedule this to run periodically.\")\n",
    "\n",
    "# Example of running the monitor once\n",
    "print(\"\\nüèÉ‚Äç‚ôÇÔ∏è Running price monitor once as example...\")\n",
    "# matching = price_monitor()  # Uncomment to run the monitor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ Advanced Beautiful Soup Techniques\n",
    "\n",
    "#### 1. Different Parsing Methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Beautiful Soup techniques\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "sample_html = \"\"\"\n",
    "<div class=\"container\">\n",
    "    <div class=\"product\" data-price=\"29.99\" data-category=\"electronics\">\n",
    "        <h3>Smartphone</h3>\n",
    "        <p class=\"description\">Latest smartphone with amazing features</p>\n",
    "        <span class=\"price\">$29.99</span>\n",
    "        <div class=\"reviews\">\n",
    "            <span class=\"rating\">4.5</span>\n",
    "            <span class=\"review-count\">(150 reviews)</span>\n",
    "        </div>\n",
    "    </div>\n",
    "    \n",
    "    <div class=\"product\" data-price=\"599.99\" data-category=\"electronics\">\n",
    "        <h3>Laptop</h3>\n",
    "        <p class=\"description\">High-performance laptop for professionals</p>\n",
    "        <span class=\"price\">$599.99</span>\n",
    "        <div class=\"reviews\">\n",
    "            <span class=\"rating\">4.8</span>\n",
    "            <span class=\"review-count\">(89 reviews)</span>\n",
    "        </div>\n",
    "    </div>\n",
    "    \n",
    "    <article class=\"blog-post\">\n",
    "        <h2>Tech News</h2>\n",
    "        <p>Latest technology trends and updates...</p>\n",
    "        <time datetime=\"2025-01-15\">January 15, 2025</time>\n",
    "    </article>\n",
    "</div>\n",
    "\"\"\"\n",
    "\n",
    "soup = BeautifulSoup(sample_html, 'html.parser')\n",
    "\n",
    "print(\"üîß Advanced Beautiful Soup Techniques:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. Find with attributes\n",
    "print(\"\\n1Ô∏è‚É£ Finding by attributes:\")\n",
    "expensive_products = soup.find_all('div', {'data-price': lambda x: x and float(x) > 100})\n",
    "for product in expensive_products:\n",
    "    name = product.h3.text\n",
    "    price = product.get('data-price')\n",
    "    print(f\"   {name}: ${price}\")\n",
    "\n",
    "# 2. Using regular expressions\n",
    "print(\"\\n2Ô∏è‚É£ Using regex patterns:\")\n",
    "price_spans = soup.find_all('span', string=re.compile(r'\\$\\d+\\.\\d+'))\n",
    "for span in price_spans:\n",
    "    print(f\"   Found price: {span.text}\")\n",
    "\n",
    "# 3. CSS selectors advanced\n",
    "print(\"\\n3Ô∏è‚É£ Advanced CSS selectors:\")\n",
    "# Products with rating above 4.5\n",
    "high_rated = soup.select('div.product:has(.rating)')\n",
    "for product in high_rated:\n",
    "    name = product.h3.text\n",
    "    rating = product.select_one('.rating').text\n",
    "    if float(rating) > 4.5:\n",
    "        print(f\"   High-rated: {name} ({rating}‚≠ê)\")\n",
    "\n",
    "# 4. Parent and sibling navigation\n",
    "print(\"\\n4Ô∏è‚É£ Navigation between elements:\")\n",
    "rating_element = soup.find('span', class_='rating')\n",
    "if rating_element:\n",
    "    # Get parent\n",
    "    reviews_div = rating_element.parent\n",
    "    print(f\"   Parent element: {reviews_div.name}\")\n",
    "    \n",
    "    # Get sibling\n",
    "    review_count = rating_element.find_next_sibling('span')\n",
    "    print(f\"   Review count: {review_count.text}\")\n",
    "\n",
    "# 5. Extracting numbers from text\n",
    "print(\"\\n5Ô∏è‚É£ Extracting numbers from text:\")\n",
    "review_texts = soup.find_all('span', class_='review-count')\n",
    "for review in review_texts:\n",
    "    # Extract number using regex\n",
    "    numbers = re.findall(r'\\d+', review.text)\n",
    "    if numbers:\n",
    "        print(f\"   Reviews: {numbers[0]}\")\n",
    "\n",
    "# 6. Custom filters\n",
    "print(\"\\n6Ô∏è‚É£ Custom filters:\")\n",
    "def has_class_and_data_price(tag):\n",
    "    return tag.has_attr('class') and tag.has_attr('data-price')\n",
    "\n",
    "products_with_price = soup.find_all(has_class_and_data_price)\n",
    "for product in products_with_price:\n",
    "    print(f\"   Product: {product.h3.text}, Price: ${product['data-price']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöó Selenium - For Dynamic and JavaScript-Heavy Websites\n",
    "\n",
    "### ü§î When Do You Need Selenium?\n",
    "\n",
    "**Beautiful Soup + Requests** works great for static HTML, but many modern websites use JavaScript to load content dynamically. This is where Selenium comes in.\n",
    "\n",
    "#### Signs You Need Selenium:\n",
    "- Content loads after the page loads (AJAX)\n",
    "- You need to click buttons or fill forms\n",
    "- The data you want appears only after user interaction\n",
    "- The website is a Single Page Application (SPA)\n",
    "- You see \"Loading...\" messages or spinners\n",
    "\n",
    "#### What Selenium Does:\n",
    "- **Controls a real browser** (Chrome, Firefox, Safari)\n",
    "- **Executes JavaScript** like a human user\n",
    "- **Waits for content** to load dynamically\n",
    "- **Simulates user actions** (clicks, typing, scrolling)\n",
    "\n",
    "### ‚ö° Selenium vs Beautiful Soup Comparison:\n",
    "\n",
    "| Feature | Beautiful Soup | Selenium |\n",
    "|---------|----------------|----------|\n",
    "| **Speed** | ‚ö° Very fast | üêå Slower (launches browser) |\n",
    "| **JavaScript** | ‚ùå No support | ‚úÖ Full support |\n",
    "| **User Interaction** | ‚ùå Cannot click/type | ‚úÖ Can simulate user actions |\n",
    "| **Memory Usage** | üíö Low | üî¥ High (browser overhead) |\n",
    "| **Complexity** | üíö Simple | üü° More complex setup |\n",
    "| **Best For** | Static websites | Dynamic/interactive websites |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üõ†Ô∏è Selenium Installation & Setup\n",
    "\n",
    "#### Step 1: Install Selenium\n",
    "```bash\n",
    "pip install selenium webdriver-manager\n",
    "```\n",
    "\n",
    "#### Step 2: Understanding WebDrivers\n",
    "Selenium needs a **WebDriver** to control browsers:\n",
    "- **ChromeDriver** - For Google Chrome\n",
    "- **GeckoDriver** - For Firefox  \n",
    "- **EdgeDriver** - For Microsoft Edge\n",
    "\n",
    "**Good News**: `webdriver-manager` automatically downloads the correct driver!\n",
    "\n",
    "#### Step 3: Basic Setup Options\n",
    "\n",
    "```python\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# Option 1: Visible browser (for development/debugging)\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "\n",
    "# Option 2: Headless browser (for production)\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"--headless\")\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "```\n",
    "\n",
    "#### Step 4: Common Chrome Options\n",
    "```python\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"--headless\")          # Run without GUI\n",
    "options.add_argument(\"--no-sandbox\")        # Required for some environments\n",
    "options.add_argument(\"--disable-dev-shm-usage\")  # Overcome limited resource problems\n",
    "options.add_argument(\"--window-size=1920,1080\")  # Set window size\n",
    "options.add_argument(\"--user-agent=Custom User Agent\")  # Custom user agent\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Selenium and WebDriver\n",
    "!pip install selenium webdriver-manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selenium Basic Example - Part 1: Setup and Navigation\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "def selenium_basic_demo():\n",
    "    \"\"\"Demonstrate Selenium basic usage\"\"\"\n",
    "    \n",
    "    print(\"üöó Selenium Basic Demo - Part 1: Setup\")\n",
    "    print(\"=\" * 45)\n",
    "    \n",
    "    # Setup Chrome options for demo\n",
    "    options = Options()\n",
    "    options.add_argument(\"--headless\")  # Run without GUI for demo\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    \n",
    "    try:\n",
    "        print(\"\\nüìã Step 1: Initialize WebDriver\")\n",
    "        # This automatically downloads ChromeDriver if needed\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        print(\"   ‚úÖ Chrome WebDriver initialized successfully\")\n",
    "        \n",
    "        print(\"\\nüåê Step 2: Navigate to website\")\n",
    "        url = \"https://quotes.toscrape.com/js/\"  # JavaScript version\n",
    "        driver.get(url)\n",
    "        print(f\"   üìç Navigated to: {url}\")\n",
    "        print(f\"   üìÑ Page title: {driver.title}\")\n",
    "        \n",
    "        print(\"\\n‚è≥ Step 3: Wait for content to load\")\n",
    "        # Wait up to 10 seconds for quotes to appear\n",
    "        wait = WebDriverWait(driver, 10)\n",
    "        quotes = wait.until(EC.presence_of_all_elements_located((By.CLASS_NAME, \"quote\")))\n",
    "        print(f\"   ‚úÖ Found {len(quotes)} quotes after waiting for JavaScript\")\n",
    "        \n",
    "        print(f\"\\nüìä Page Information:\")\n",
    "        print(f\"   Current URL: {driver.current_url}\")\n",
    "        print(f\"   Page source length: {len(driver.page_source):,} characters\")\n",
    "        \n",
    "        return driver, quotes\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error in Selenium demo: {e}\")\n",
    "        return None, []\n",
    "\n",
    "# Note: This demo shows setup - actual scraping in next cell\n",
    "print(\"\udca1 Note: This example shows Selenium setup and navigation.\")\n",
    "print(\"\udd04 For full functionality, Chrome browser and ChromeDriver are required.\")\n",
    "print(\"\udcdd In Colab/Jupyter environments, additional setup might be needed.\")\n",
    "\n",
    "# Uncomment the line below to run the demo (if Chrome is available)\n",
    "# driver, quotes = selenium_basic_demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selenium Basic Example - Part 2: Data Extraction and Interaction\n",
    "# This continues from Part 1\n",
    "\n",
    "def selenium_scraping_demo():\n",
    "    \"\"\"Demonstrate Selenium data extraction and interaction\"\"\"\n",
    "    \n",
    "    print(\"üöó Selenium Basic Demo - Part 2: Data Extraction\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    print(\"\\nüìù Common Selenium Element Location Methods:\")\n",
    "    print(\"   By.CLASS_NAME    ‚Üí find_element(By.CLASS_NAME, 'quote')\")\n",
    "    print(\"   By.ID            ‚Üí find_element(By.ID, 'main-content')\")\n",
    "    print(\"   By.TAG_NAME      ‚Üí find_element(By.TAG_NAME, 'h1')\")\n",
    "    print(\"   By.CSS_SELECTOR  ‚Üí find_element(By.CSS_SELECTOR, '.quote .text')\")\n",
    "    print(\"   By.XPATH         ‚Üí find_element(By.XPATH, '//div[@class=\\\"quote\\\"]')\")\n",
    "    \n",
    "    # Simulated data extraction (would work with real driver)\n",
    "    simulated_quotes = [\n",
    "        {\n",
    "            'text': '\"The world as we have created it is a process of our thinking.\"',\n",
    "            'author': 'Albert Einstein',\n",
    "            'tags': ['change', 'deep-thoughts', 'thinking', 'world']\n",
    "        },\n",
    "        {\n",
    "            'text': '\"It is our choices, Harry, that show what we truly are.\"',\n",
    "            'author': 'J.K. Rowling',\n",
    "            'tags': ['abilities', 'choices']\n",
    "        },\n",
    "        {\n",
    "            'text': '\"There are only two ways to live your life.\"',\n",
    "            'author': 'Albert Einstein',\n",
    "            'tags': ['inspirational', 'life', 'live', 'miracle', 'miracles']\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    print(f\"\\nüîç Extracting Data with Selenium:\")\n",
    "    print(\"   (Simulated - shows the process)\")\n",
    "    \n",
    "    for i, quote_data in enumerate(simulated_quotes, 1):\n",
    "        print(f\"\\nüí¨ Quote {i}:\")\n",
    "        print(f\"   Text: {quote_data['text']}\")\n",
    "        print(f\"   Author: {quote_data['author']}\")\n",
    "        print(f\"   Tags: {', '.join(quote_data['tags'])}\")\n",
    "    \n",
    "    print(f\"\\nüéØ Real Selenium Code Pattern:\")\n",
    "    selenium_code = '''\n",
    "# Real Selenium extraction code:\n",
    "quotes = driver.find_elements(By.CLASS_NAME, \"quote\")\n",
    "\n",
    "for quote in quotes:\n",
    "    text = quote.find_element(By.CLASS_NAME, \"text\").text\n",
    "    author = quote.find_element(By.CLASS_NAME, \"author\").text\n",
    "    tags = [tag.text for tag in quote.find_elements(By.CLASS_NAME, \"tag\")]\n",
    "    \n",
    "    quote_data = {\n",
    "        'text': text,\n",
    "        'author': author,\n",
    "        'tags': tags\n",
    "    }\n",
    "'''\n",
    "    \n",
    "    print(selenium_code)\n",
    "    \n",
    "    print(f\"\\nüñ±Ô∏è Selenium Interaction Examples:\")\n",
    "    interaction_code = '''\n",
    "# Click elements\n",
    "button = driver.find_element(By.ID, \"load-more-btn\")\n",
    "button.click()\n",
    "\n",
    "# Fill forms\n",
    "search_box = driver.find_element(By.NAME, \"search\")\n",
    "search_box.send_keys(\"python\")\n",
    "search_box.submit()\n",
    "\n",
    "# Scroll page\n",
    "driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "# Wait for specific conditions\n",
    "wait = WebDriverWait(driver, 10)\n",
    "element = wait.until(EC.element_to_be_clickable((By.ID, \"submit-btn\")))\n",
    "'''\n",
    "    \n",
    "    print(interaction_code)\n",
    "    \n",
    "    print(f\"\\n‚ö†Ô∏è Important Selenium Concepts:\")\n",
    "    print(\"   üïê Explicit Waits: Wait for specific conditions\")\n",
    "    print(\"   üïë Implicit Waits: Global wait time for all elements\")\n",
    "    print(\"   üé≠ Headless Mode: Run without visible browser\")\n",
    "    print(\"   üîí Always Close: driver.quit() to free resources\")\n",
    "\n",
    "# Run the demo\n",
    "selenium_scraping_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Parallel Web Scraping & Multiprocessing\n",
    "\n",
    "When scraping large amounts of data, performance becomes crucial. Python's multiprocessing and libraries like `joblib` allow us to speed up scraping by processing multiple URLs simultaneously.\n",
    "\n",
    "## üß† Why Use Parallel Processing?\n",
    "\n",
    "**Sequential Processing:**\n",
    "- Scrapes one URL at a time\n",
    "- Total time = (number of URLs) √ó (average time per URL)\n",
    "- CPU cores remain underutilized\n",
    "\n",
    "**Parallel Processing:**\n",
    "- Scrapes multiple URLs simultaneously\n",
    "- Total time ‚âà (number of URLs) √∑ (number of workers) √ó (average time per URL)\n",
    "- Better resource utilization\n",
    "\n",
    "‚ö†Ô∏è **Important**: Always respect websites' rate limits and robots.txt when using parallel processing!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Basic Multiprocessing Concepts\n",
    "\n",
    "Before applying multiprocessing to web scraping, let's understand the basics with simple examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import multiprocessing as mp\n",
    "from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor\n",
    "import requests\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# Example 1: CPU-intensive task (Sequential vs Parallel)\n",
    "def square_number(n):\n",
    "    \"\"\"Simulate CPU-intensive work\"\"\"\n",
    "    time.sleep(0.1)  # Simulate computation time\n",
    "    return n ** 2\n",
    "\n",
    "def demonstrate_multiprocessing():\n",
    "    numbers = list(range(1, 21))  # 1 to 20\n",
    "    \n",
    "    # Sequential processing\n",
    "    print(\"üêå Sequential Processing:\")\n",
    "    start_time = time.time()\n",
    "    sequential_results = [square_number(n) for n in numbers]\n",
    "    sequential_time = time.time() - start_time\n",
    "    print(f\"   Time taken: {sequential_time:.2f} seconds\")\n",
    "    print(f\"   Results: {sequential_results[:5]}... (showing first 5)\")\n",
    "    \n",
    "    # Parallel processing with multiprocessing\n",
    "    print(\"\\n‚ö° Parallel Processing (multiprocessing):\")\n",
    "    start_time = time.time()\n",
    "    with ProcessPoolExecutor(max_workers=4) as executor:\n",
    "        parallel_results = list(executor.map(square_number, numbers))\n",
    "    parallel_time = time.time() - start_time\n",
    "    print(f\"   Time taken: {parallel_time:.2f} seconds\")\n",
    "    print(f\"   Results: {parallel_results[:5]}... (showing first 5)\")\n",
    "    print(f\"   Speedup: {sequential_time/parallel_time:.2f}x faster\")\n",
    "\n",
    "# Run the demonstration\n",
    "demonstrate_multiprocessing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Introduction to Joblib\n",
    "\n",
    "`joblib` is a powerful library that makes parallel computing easy and efficient. It's particularly great for:\n",
    "- CPU-bound tasks\n",
    "- Machine learning workloads\n",
    "- Data processing pipelines\n",
    "\n",
    "**Key advantages:**\n",
    "- Simple API: `Parallel(n_jobs=-1)(delayed(function)(args) for args in data)`\n",
    "- Automatic memory optimization\n",
    "- Built-in progress tracking\n",
    "- Works well with NumPy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install joblib if not already installed\n",
    "# !pip install joblib\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "import numpy as np\n",
    "\n",
    "def process_data(x):\n",
    "    \"\"\"Simulate data processing\"\"\"\n",
    "    time.sleep(0.05)\n",
    "    return x ** 3 + 2 * x ** 2 + x + 1\n",
    "\n",
    "def demonstrate_joblib():\n",
    "    data = list(range(1, 51))  # 1 to 50\n",
    "    \n",
    "    print(\"üîß Joblib Examples:\")\n",
    "    \n",
    "    # Sequential processing\n",
    "    print(\"\\nüêå Sequential Processing:\")\n",
    "    start_time = time.time()\n",
    "    sequential_results = [process_data(x) for x in data]\n",
    "    sequential_time = time.time() - start_time\n",
    "    print(f\"   Time taken: {sequential_time:.2f} seconds\")\n",
    "    \n",
    "    # Parallel processing with joblib (all CPU cores)\n",
    "    print(\"\\n‚ö° Joblib Parallel (all cores):\")\n",
    "    start_time = time.time()\n",
    "    parallel_results = Parallel(n_jobs=-1)(delayed(process_data)(x) for x in data)\n",
    "    parallel_time = time.time() - start_time\n",
    "    print(f\"   Time taken: {parallel_time:.2f} seconds\")\n",
    "    print(f\"   Speedup: {sequential_time/parallel_time:.2f}x faster\")\n",
    "    \n",
    "    # Parallel processing with specific number of workers\n",
    "    print(\"\\n‚ö° Joblib Parallel (4 workers):\")\n",
    "    start_time = time.time()\n",
    "    parallel_results_4 = Parallel(n_jobs=4)(delayed(process_data)(x) for x in data)\n",
    "    parallel_time_4 = time.time() - start_time\n",
    "    print(f\"   Time taken: {parallel_time_4:.2f} seconds\")\n",
    "    \n",
    "    # With verbose progress tracking\n",
    "    print(\"\\nüìä Joblib with Progress Tracking:\")\n",
    "    start_time = time.time()\n",
    "    parallel_results_verbose = Parallel(n_jobs=4, verbose=1)(\n",
    "        delayed(process_data)(x) for x in data\n",
    "    )\n",
    "    verbose_time = time.time() - start_time\n",
    "    print(f\"   Time taken: {verbose_time:.2f} seconds\")\n",
    "    \n",
    "    # Verify results are the same\n",
    "    print(f\"\\n‚úÖ Results match: {sequential_results == parallel_results}\")\n",
    "\n",
    "# Run joblib demonstration\n",
    "demonstrate_joblib()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üåê Parallel Web Scraping Examples\n",
    "\n",
    "Now let's apply these concepts to web scraping. We'll compare sequential vs parallel approaches for scraping multiple URLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "import time\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "def scrape_single_url(url, timeout=10):\n",
    "    \"\"\"Scrape a single URL and extract basic information\"\"\"\n",
    "    try:\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        }\n",
    "        \n",
    "        response = requests.get(url, headers=headers, timeout=timeout)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Extract basic information\n",
    "        title = soup.find('title')\n",
    "        title_text = title.get_text(strip=True) if title else \"No title\"\n",
    "        \n",
    "        # Count paragraphs\n",
    "        paragraphs = soup.find_all('p')\n",
    "        paragraph_count = len(paragraphs)\n",
    "        \n",
    "        # Count links\n",
    "        links = soup.find_all('a', href=True)\n",
    "        link_count = len(links)\n",
    "        \n",
    "        # Get first paragraph text (if available)\n",
    "        first_paragraph = \"\"\n",
    "        if paragraphs:\n",
    "            first_paragraph = paragraphs[0].get_text(strip=True)[:200] + \"...\"\n",
    "        \n",
    "        return {\n",
    "            'url': url,\n",
    "            'title': title_text[:100],  # Limit title length\n",
    "            'status': 'success',\n",
    "            'paragraph_count': paragraph_count,\n",
    "            'link_count': link_count,\n",
    "            'first_paragraph': first_paragraph,\n",
    "            'response_time': response.elapsed.total_seconds()\n",
    "        }\n",
    "        \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return {\n",
    "            'url': url,\n",
    "            'title': None,\n",
    "            'status': 'error',\n",
    "            'error': str(e),\n",
    "            'paragraph_count': 0,\n",
    "            'link_count': 0,\n",
    "            'first_paragraph': '',\n",
    "            'response_time': None\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'url': url,\n",
    "            'title': None,\n",
    "            'status': 'error',\n",
    "            'error': f\"Parsing error: {str(e)}\",\n",
    "            'paragraph_count': 0,\n",
    "            'link_count': 0,\n",
    "            'first_paragraph': '',\n",
    "            'response_time': None\n",
    "        }\n",
    "\n",
    "def scrape_urls_sequential(urls):\n",
    "    \"\"\"Scrape URLs one by one (sequential)\"\"\"\n",
    "    print(\"üêå Sequential scraping...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    results = []\n",
    "    for i, url in enumerate(urls, 1):\n",
    "        print(f\"   Scraping {i}/{len(urls)}: {url[:50]}...\")\n",
    "        result = scrape_single_url(url)\n",
    "        results.append(result)\n",
    "        time.sleep(1)  # Be respectful - add delay\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"   Sequential time: {total_time:.2f} seconds\")\n",
    "    return results, total_time\n",
    "\n",
    "def scrape_urls_parallel_joblib(urls, n_jobs=4):\n",
    "    \"\"\"Scrape URLs in parallel using joblib\"\"\"\n",
    "    print(f\"‚ö° Parallel scraping with joblib ({n_jobs} workers)...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Add delays in parallel execution too (but spread out)\n",
    "    def scrape_with_delay(url, delay_factor):\n",
    "        time.sleep(delay_factor * 0.5)  # Staggered delays\n",
    "        return scrape_single_url(url)\n",
    "    \n",
    "    # Create delay factors for staggered requests\n",
    "    delay_factors = [i % 4 for i in range(len(urls))]\n",
    "    \n",
    "    results = Parallel(n_jobs=n_jobs, verbose=1)(\n",
    "        delayed(scrape_with_delay)(url, delay) \n",
    "        for url, delay in zip(urls, delay_factors)\n",
    "    )\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"   Parallel time: {total_time:.2f} seconds\")\n",
    "    return results, total_time\n",
    "\n",
    "# Test URLs (using public APIs and websites that allow scraping)\n",
    "test_urls = [\n",
    "    'https://httpbin.org/html',\n",
    "    'https://httpbin.org/json',\n",
    "    'https://jsonplaceholder.typicode.com/posts/1',\n",
    "    'https://jsonplaceholder.typicode.com/posts/2',\n",
    "    'https://httpbin.org/xml',\n",
    "    'https://httpbin.org/robots.txt',\n",
    "    'https://jsonplaceholder.typicode.com/users/1',\n",
    "    'https://jsonplaceholder.typicode.com/users/2'\n",
    "]\n",
    "\n",
    "print(\"üåê Web Scraping Performance Comparison\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Sequential scraping\n",
    "sequential_results, seq_time = scrape_urls_sequential(test_urls)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "\n",
    "# Parallel scraping\n",
    "parallel_results, par_time = scrape_urls_parallel_joblib(test_urls, n_jobs=4)\n",
    "\n",
    "# Compare results\n",
    "print(f\"\\nüìä Performance Summary:\")\n",
    "print(f\"   URLs scraped: {len(test_urls)}\")\n",
    "print(f\"   Sequential time: {seq_time:.2f} seconds\")\n",
    "print(f\"   Parallel time: {par_time:.2f} seconds\")\n",
    "print(f\"   Speedup: {seq_time/par_time:.2f}x faster\")\n",
    "\n",
    "# Show success rates\n",
    "seq_success = sum(1 for r in sequential_results if r['status'] == 'success')\n",
    "par_success = sum(1 for r in parallel_results if r['status'] == 'success')\n",
    "\n",
    "print(f\"\\n‚úÖ Success Rates:\")\n",
    "print(f\"   Sequential: {seq_success}/{len(test_urls)} ({seq_success/len(test_urls)*100:.1f}%)\")\n",
    "print(f\"   Parallel: {par_success}/{len(test_urls)} ({par_success/len(test_urls)*100:.1f}%)\")\n",
    "\n",
    "# Show sample results\n",
    "print(f\"\\nüìÑ Sample Results (first 3):\")\n",
    "for i, result in enumerate(parallel_results[:3]):\n",
    "    print(f\"   {i+1}. {result['url']}\")\n",
    "    print(f\"      Title: {result['title']}\")\n",
    "    print(f\"      Status: {result['status']}\")\n",
    "    if result['status'] == 'success':\n",
    "        print(f\"      Paragraphs: {result['paragraph_count']}, Links: {result['link_count']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ°Ô∏è Advanced Parallel Scraping with Rate Limiting\n",
    "\n",
    "When scraping real websites, we need to be more careful about rate limiting, error handling, and respecting server resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from threading import Lock\n",
    "import threading\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "class RateLimitedScraper:\n",
    "    \"\"\"A rate-limited web scraper with parallel processing capabilities\"\"\"\n",
    "    \n",
    "    def __init__(self, requests_per_second=2, max_retries=3):\n",
    "        self.requests_per_second = requests_per_second\n",
    "        self.max_retries = max_retries\n",
    "        self.last_request_time = {}\n",
    "        self.lock = Lock()\n",
    "        \n",
    "    def wait_if_needed(self, domain):\n",
    "        \"\"\"Implement rate limiting per domain\"\"\"\n",
    "        with self.lock:\n",
    "            now = datetime.now()\n",
    "            if domain in self.last_request_time:\n",
    "                time_since_last = (now - self.last_request_time[domain]).total_seconds()\n",
    "                min_interval = 1.0 / self.requests_per_second\n",
    "                \n",
    "                if time_since_last < min_interval:\n",
    "                    sleep_time = min_interval - time_since_last\n",
    "                    time.sleep(sleep_time)\n",
    "            \n",
    "            self.last_request_time[domain] = datetime.now()\n",
    "    \n",
    "    def extract_domain(self, url):\n",
    "        \"\"\"Extract domain from URL\"\"\"\n",
    "        from urllib.parse import urlparse\n",
    "        return urlparse(url).netloc\n",
    "    \n",
    "    def scrape_with_retries(self, url):\n",
    "        \"\"\"Scrape URL with retry logic and rate limiting\"\"\"\n",
    "        domain = self.extract_domain(url)\n",
    "        \n",
    "        for attempt in range(self.max_retries):\n",
    "            try:\n",
    "                # Implement rate limiting\n",
    "                self.wait_if_needed(domain)\n",
    "                \n",
    "                headers = {\n",
    "                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "                }\n",
    "                \n",
    "                response = requests.get(url, headers=headers, timeout=15)\n",
    "                response.raise_for_status()\n",
    "                \n",
    "                soup = BeautifulSoup(response.content, 'html.parser')\n",
    "                \n",
    "                # Extract comprehensive data\n",
    "                result = {\n",
    "                    'url': url,\n",
    "                    'status': 'success',\n",
    "                    'attempt': attempt + 1,\n",
    "                    'timestamp': datetime.now().isoformat(),\n",
    "                    'response_code': response.status_code,\n",
    "                    'content_length': len(response.content),\n",
    "                    'title': '',\n",
    "                    'meta_description': '',\n",
    "                    'headings': {},\n",
    "                    'link_count': 0,\n",
    "                    'image_count': 0,\n",
    "                    'form_count': 0,\n",
    "                    'text_content_length': 0\n",
    "                }\n",
    "                \n",
    "                # Extract title\n",
    "                title_tag = soup.find('title')\n",
    "                if title_tag:\n",
    "                    result['title'] = title_tag.get_text(strip=True)\n",
    "                \n",
    "                # Extract meta description\n",
    "                meta_desc = soup.find('meta', attrs={'name': 'description'})\n",
    "                if meta_desc:\n",
    "                    result['meta_description'] = meta_desc.get('content', '')\n",
    "                \n",
    "                # Count different elements\n",
    "                result['link_count'] = len(soup.find_all('a', href=True))\n",
    "                result['image_count'] = len(soup.find_all('img'))\n",
    "                result['form_count'] = len(soup.find_all('form'))\n",
    "                \n",
    "                # Count headings\n",
    "                for i in range(1, 7):\n",
    "                    headings = soup.find_all(f'h{i}')\n",
    "                    if headings:\n",
    "                        result['headings'][f'h{i}'] = len(headings)\n",
    "                \n",
    "                # Get text content length\n",
    "                text_content = soup.get_text(strip=True)\n",
    "                result['text_content_length'] = len(text_content)\n",
    "                \n",
    "                return result\n",
    "                \n",
    "            except requests.exceptions.RequestException as e:\n",
    "                if attempt == self.max_retries - 1:  # Last attempt\n",
    "                    return {\n",
    "                        'url': url,\n",
    "                        'status': 'error',\n",
    "                        'error': str(e),\n",
    "                        'attempt': attempt + 1,\n",
    "                        'timestamp': datetime.now().isoformat()\n",
    "                    }\n",
    "                else:\n",
    "                    # Wait before retry (exponential backoff)\n",
    "                    wait_time = (2 ** attempt) + random.uniform(0, 1)\n",
    "                    time.sleep(wait_time)\n",
    "            \n",
    "            except Exception as e:\n",
    "                return {\n",
    "                    'url': url,\n",
    "                    'status': 'error',\n",
    "                    'error': f\"Unexpected error: {str(e)}\",\n",
    "                    'attempt': attempt + 1,\n",
    "                    'timestamp': datetime.now().isoformat()\n",
    "                }\n",
    "\n",
    "def parallel_scrape_with_rate_limiting(urls, n_jobs=3, requests_per_second=2):\n",
    "    \"\"\"Scrape URLs in parallel with rate limiting\"\"\"\n",
    "    scraper = RateLimitedScraper(requests_per_second=requests_per_second)\n",
    "    \n",
    "    print(f\"üöÄ Advanced Parallel Scraping:\")\n",
    "    print(f\"   URLs: {len(urls)}\")\n",
    "    print(f\"   Workers: {n_jobs}\")\n",
    "    print(f\"   Rate limit: {requests_per_second} requests/second per domain\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    results = Parallel(n_jobs=n_jobs, verbose=1)(\n",
    "        delayed(scraper.scrape_with_retries)(url) for url in urls\n",
    "    )\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    \n",
    "    # Analyze results\n",
    "    successful = [r for r in results if r['status'] == 'success']\n",
    "    failed = [r for r in results if r['status'] == 'error']\n",
    "    \n",
    "    print(f\"\\nüìä Scraping Summary:\")\n",
    "    print(f\"   Total time: {total_time:.2f} seconds\")\n",
    "    print(f\"   Average time per URL: {total_time/len(urls):.2f} seconds\")\n",
    "    print(f\"   Successful: {len(successful)}/{len(urls)} ({len(successful)/len(urls)*100:.1f}%)\")\n",
    "    print(f\"   Failed: {len(failed)}/{len(urls)} ({len(failed)/len(urls)*100:.1f}%)\")\n",
    "    \n",
    "    if successful:\n",
    "        avg_content_length = sum(r['content_length'] for r in successful) / len(successful)\n",
    "        total_links = sum(r['link_count'] for r in successful)\n",
    "        total_images = sum(r['image_count'] for r in successful)\n",
    "        \n",
    "        print(f\"\\nüìÑ Content Analysis:\")\n",
    "        print(f\"   Average content length: {avg_content_length:.0f} bytes\")\n",
    "        print(f\"   Total links found: {total_links}\")\n",
    "        print(f\"   Total images found: {total_images}\")\n",
    "    \n",
    "    if failed:\n",
    "        print(f\"\\n‚ùå Failed URLs:\")\n",
    "        for fail in failed[:3]:  # Show first 3 failures\n",
    "            print(f\"   {fail['url']}: {fail.get('error', 'Unknown error')}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example with mixed domains (rate limiting will be applied per domain)\n",
    "mixed_urls = [\n",
    "    'https://httpbin.org/html',\n",
    "    'https://httpbin.org/json',\n",
    "    'https://httpbin.org/xml',\n",
    "    'https://jsonplaceholder.typicode.com/posts/1',\n",
    "    'https://jsonplaceholder.typicode.com/posts/2',\n",
    "    'https://jsonplaceholder.typicode.com/users/1',\n",
    "    'https://httpbin.org/robots.txt',\n",
    "    'https://httpbin.org/user-agent',\n",
    "    'https://jsonplaceholder.typicode.com/comments/1',\n",
    "    'https://httpbin.org/headers'\n",
    "]\n",
    "\n",
    "# Run advanced parallel scraping\n",
    "results = parallel_scrape_with_rate_limiting(\n",
    "    mixed_urls, \n",
    "    n_jobs=3, \n",
    "    requests_per_second=2\n",
    ")\n",
    "\n",
    "# Show detailed results for successful scrapes\n",
    "print(f\"\\nüìã Detailed Results (first 3 successful):\")\n",
    "successful_results = [r for r in results if r['status'] == 'success']\n",
    "for i, result in enumerate(successful_results[:3]):\n",
    "    print(f\"\\n{i+1}. {result['url']}\")\n",
    "    print(f\"   Title: {result['title'][:60]}...\")\n",
    "    print(f\"   Response Code: {result['response_code']}\")\n",
    "    print(f\"   Content Length: {result['content_length']:,} bytes\")\n",
    "    print(f\"   Links: {result['link_count']}, Images: {result['image_count']}\")\n",
    "    if result['headings']:\n",
    "        print(f\"   Headings: {result['headings']}\")\n",
    "    print(f\"   Attempt: {result['attempt']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõí Real-World Example: Parallel E-commerce Data Scraping\n",
    "\n",
    "Let's create a practical example that simulates scraping product data from multiple pages, using parallel processing to handle large datasets efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "class EcommerceScraper:\n",
    "    \"\"\"Simulate e-commerce product scraping with parallel processing\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "        })\n",
    "    \n",
    "    def simulate_product_page(self, product_id):\n",
    "        \"\"\"Simulate scraping a product page\"\"\"\n",
    "        # In real scraping, this would fetch from actual URLs\n",
    "        # For demo purposes, we'll simulate data\n",
    "        \n",
    "        time.sleep(random.uniform(0.5, 2.0))  # Simulate network delay\n",
    "        \n",
    "        # Simulate occasional failures\n",
    "        if random.random() < 0.1:  # 10% failure rate\n",
    "            raise requests.exceptions.RequestException(f\"Failed to load product {product_id}\")\n",
    "        \n",
    "        # Generate simulated product data\n",
    "        categories = ['Electronics', 'Clothing', 'Books', 'Home & Garden', 'Sports']\n",
    "        brands = ['BrandA', 'BrandB', 'BrandC', 'BrandD', 'BrandE']\n",
    "        \n",
    "        product = {\n",
    "            'product_id': product_id,\n",
    "            'name': f'Product {product_id}',\n",
    "            'price': round(random.uniform(10, 500), 2),\n",
    "            'category': random.choice(categories),\n",
    "            'brand': random.choice(brands),\n",
    "            'rating': round(random.uniform(1, 5), 1),\n",
    "            'review_count': random.randint(0, 1000),\n",
    "            'in_stock': random.choice([True, False]),\n",
    "            'description_length': random.randint(100, 1000),\n",
    "            'image_count': random.randint(1, 10),\n",
    "            'scrape_timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        return product\n",
    "    \n",
    "    def scrape_product_batch(self, product_ids):\n",
    "        \"\"\"Scrape a batch of product IDs\"\"\"\n",
    "        results = []\n",
    "        batch_start = time.time()\n",
    "        \n",
    "        for product_id in product_ids:\n",
    "            try:\n",
    "                product = self.simulate_product_page(product_id)\n",
    "                product['status'] = 'success'\n",
    "                results.append(product)\n",
    "            except Exception as e:\n",
    "                results.append({\n",
    "                    'product_id': product_id,\n",
    "                    'status': 'error',\n",
    "                    'error': str(e),\n",
    "                    'scrape_timestamp': datetime.now().isoformat()\n",
    "                })\n",
    "        \n",
    "        batch_time = time.time() - batch_start\n",
    "        return results, batch_time\n",
    "\n",
    "def parallel_ecommerce_scraping(product_ids, batch_size=50, n_jobs=4):\n",
    "    \"\"\"Scrape e-commerce products in parallel batches\"\"\"\n",
    "    \n",
    "    # Split product IDs into batches\n",
    "    batches = [product_ids[i:i + batch_size] for i in range(0, len(product_ids), batch_size)]\n",
    "    \n",
    "    print(f\"üõí E-commerce Parallel Scraping:\")\n",
    "    print(f\"   Total products: {len(product_ids)}\")\n",
    "    print(f\"   Batch size: {batch_size}\")\n",
    "    print(f\"   Number of batches: {len(batches)}\")\n",
    "    print(f\"   Parallel workers: {n_jobs}\")\n",
    "    \n",
    "    scraper = EcommerceScraper()\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Process batches in parallel\n",
    "    batch_results = Parallel(n_jobs=n_jobs, verbose=1)(\n",
    "        delayed(scraper.scrape_product_batch)(batch) for batch in batches\n",
    "    )\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    \n",
    "    # Flatten results\n",
    "    all_products = []\n",
    "    total_batch_time = 0\n",
    "    \n",
    "    for results, batch_time in batch_results:\n",
    "        all_products.extend(results)\n",
    "        total_batch_time += batch_time\n",
    "    \n",
    "    # Analyze results\n",
    "    successful_products = [p for p in all_products if p['status'] == 'success']\n",
    "    failed_products = [p for p in all_products if p['status'] == 'error']\n",
    "    \n",
    "    print(f\"\\nüìä Scraping Results:\")\n",
    "    print(f\"   Total time: {total_time:.2f} seconds\")\n",
    "    print(f\"   Products/second: {len(product_ids)/total_time:.2f}\")\n",
    "    print(f\"   Successful: {len(successful_products)}/{len(product_ids)} ({len(successful_products)/len(product_ids)*100:.1f}%)\")\n",
    "    print(f\"   Failed: {len(failed_products)}/{len(product_ids)} ({len(failed_products)/len(product_ids)*100:.1f}%)\")\n",
    "    \n",
    "    return successful_products, failed_products\n",
    "\n",
    "def analyze_scraped_products(products):\n",
    "    \"\"\"Analyze the scraped product data\"\"\"\n",
    "    if not products:\n",
    "        print(\"‚ùå No products to analyze\")\n",
    "        return\n",
    "    \n",
    "    df = pd.DataFrame(products)\n",
    "    \n",
    "    print(f\"\\nüìà Product Data Analysis:\")\n",
    "    print(f\"   Dataset shape: {df.shape}\")\n",
    "    \n",
    "    # Price analysis\n",
    "    if 'price' in df.columns:\n",
    "        print(f\"\\nüí∞ Price Statistics:\")\n",
    "        print(f\"   Average price: ${df['price'].mean():.2f}\")\n",
    "        print(f\"   Median price: ${df['price'].median():.2f}\")\n",
    "        print(f\"   Price range: ${df['price'].min():.2f} - ${df['price'].max():.2f}\")\n",
    "        \n",
    "    # Category distribution\n",
    "    if 'category' in df.columns:\n",
    "        print(f\"\\nüìÇ Category Distribution:\")\n",
    "        category_counts = df['category'].value_counts()\n",
    "        for category, count in category_counts.items():\n",
    "            print(f\"   {category}: {count} products ({count/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    # Brand analysis\n",
    "    if 'brand' in df.columns:\n",
    "        print(f\"\\nüè∑Ô∏è Top Brands:\")\n",
    "        brand_counts = df['brand'].value_counts().head(5)\n",
    "        for brand, count in brand_counts.items():\n",
    "            print(f\"   {brand}: {count} products\")\n",
    "    \n",
    "    # Stock status\n",
    "    if 'in_stock' in df.columns:\n",
    "        in_stock_count = df['in_stock'].sum()\n",
    "        print(f\"\\nüì¶ Stock Status:\")\n",
    "        print(f\"   In stock: {in_stock_count}/{len(df)} ({in_stock_count/len(df)*100:.1f}%)\")\n",
    "        print(f\"   Out of stock: {len(df)-in_stock_count}/{len(df)} ({(len(df)-in_stock_count)/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    # Rating analysis\n",
    "    if 'rating' in df.columns:\n",
    "        print(f\"\\n‚≠ê Rating Statistics:\")\n",
    "        print(f\"   Average rating: {df['rating'].mean():.2f}/5.0\")\n",
    "        print(f\"   Ratings >= 4.0: {(df['rating'] >= 4.0).sum()}/{len(df)} ({(df['rating'] >= 4.0).sum()/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Generate sample product IDs (simulating large dataset)\n",
    "product_ids = [f\"PROD_{i:06d}\" for i in range(1, 501)]  # 500 products\n",
    "\n",
    "print(\"üöÄ Starting E-commerce Parallel Scraping Demo...\")\n",
    "\n",
    "# Run parallel scraping\n",
    "successful_products, failed_products = parallel_ecommerce_scraping(\n",
    "    product_ids, \n",
    "    batch_size=50, \n",
    "    n_jobs=4\n",
    ")\n",
    "\n",
    "# Analyze the results\n",
    "df_products = analyze_scraped_products(successful_products)\n",
    "\n",
    "# Save results\n",
    "if successful_products:\n",
    "    # Save to JSON\n",
    "    output_file = 'scraped_products.json'\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(successful_products, f, indent=2)\n",
    "    \n",
    "    # Save to CSV\n",
    "    csv_file = 'scraped_products.csv'\n",
    "    df_products.to_csv(csv_file, index=False)\n",
    "    \n",
    "    print(f\"\\nüíæ Data saved:\")\n",
    "    print(f\"   JSON: {output_file}\")\n",
    "    print(f\"   CSV: {csv_file}\")\n",
    "\n",
    "# Show sample products\n",
    "if successful_products:\n",
    "    print(f\"\\nüõçÔ∏è Sample Products:\")\n",
    "    for i, product in enumerate(successful_products[:3]):\n",
    "        print(f\"\\n{i+1}. {product['name']} (ID: {product['product_id']})\")\n",
    "        print(f\"   Price: ${product['price']}\")\n",
    "        print(f\"   Category: {product['category']}\")\n",
    "        print(f\"   Brand: {product['brand']}\")\n",
    "        print(f\"   Rating: {product['rating']}/5.0 ({product['review_count']} reviews)\")\n",
    "        print(f\"   In Stock: {'‚úÖ' if product['in_stock'] else '‚ùå'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.ysu.am/robots.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ö° Performance Optimization & Best Practices\n",
    "\n",
    "### üéØ Choosing the Right Approach\n",
    "\n",
    "| Method | Best For | Pros | Cons |\n",
    "|--------|----------|------|------|\n",
    "| **Sequential** | Small datasets, strict rate limits | Simple, predictable | Slow for large datasets |\n",
    "| **Threading** | I/O-bound tasks, many small requests | Good for network-bound tasks | GIL limitations in Python |\n",
    "| **Multiprocessing** | CPU-intensive parsing | True parallelism | Higher memory usage |\n",
    "| **Joblib** | Balanced approach, data science tasks | Easy to use, optimized | Extra dependency |\n",
    "\n",
    "### üõ°Ô∏è Rate Limiting Strategies\n",
    "\n",
    "```python\n",
    "# 1. Fixed delay between requests\n",
    "time.sleep(1)\n",
    "\n",
    "# 2. Random delay (more human-like)\n",
    "time.sleep(random.uniform(0.5, 2.0))\n",
    "\n",
    "# 3. Exponential backoff on errors\n",
    "wait_time = (2 ** attempt) + random.uniform(0, 1)\n",
    "\n",
    "# 4. Domain-specific rate limiting\n",
    "# Different limits for different websites\n",
    "```\n",
    "\n",
    "### üìä Monitoring & Logging\n",
    "\n",
    "```python\n",
    "# Track success rates, response times, errors\n",
    "# Use logging instead of print for production\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "logger.info(f\"Scraped {url} successfully\")\n",
    "logger.error(f\"Failed to scrape {url}: {error}\")\n",
    "```\n",
    "\n",
    "### üîß Performance Tips\n",
    "\n",
    "1. **Use connection pooling** with `requests.Session()`\n",
    "2. **Implement caching** to avoid re-scraping\n",
    "3. **Batch processing** for large datasets\n",
    "4. **Memory management** - process in chunks\n",
    "5. **Error handling** - implement retries and fallbacks\n",
    "6. **Respect robots.txt** and rate limits\n",
    "7. **Use appropriate timeouts**\n",
    "8. **Monitor resource usage** (CPU, memory, network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import threading\n",
    "from collections import defaultdict\n",
    "\n",
    "def compare_parallel_approaches(urls, max_workers=4):\n",
    "    \"\"\"Compare different parallel processing approaches\"\"\"\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # 1. Sequential baseline\n",
    "    print(\"üêå Sequential Processing:\")\n",
    "    start_time = time.time()\n",
    "    sequential_results = [scrape_single_url(url) for url in urls]\n",
    "    sequential_time = time.time() - start_time\n",
    "    results['Sequential'] = {\n",
    "        'time': sequential_time,\n",
    "        'results': sequential_results\n",
    "    }\n",
    "    print(f\"   Time: {sequential_time:.2f}s\")\n",
    "    \n",
    "    # 2. ThreadPoolExecutor\n",
    "    print(\"\\nüßµ ThreadPoolExecutor:\")\n",
    "    start_time = time.time()\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        thread_results = list(executor.map(scrape_single_url, urls))\n",
    "    thread_time = time.time() - start_time\n",
    "    results['ThreadPool'] = {\n",
    "        'time': thread_time,\n",
    "        'results': thread_results\n",
    "    }\n",
    "    print(f\"   Time: {thread_time:.2f}s\")\n",
    "    print(f\"   Speedup: {sequential_time/thread_time:.2f}x\")\n",
    "    \n",
    "    # 3. ProcessPoolExecutor\n",
    "    print(\"\\n‚öôÔ∏è ProcessPoolExecutor:\")\n",
    "    start_time = time.time()\n",
    "    with concurrent.futures.ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
    "        process_results = list(executor.map(scrape_single_url, urls))\n",
    "    process_time = time.time() - start_time\n",
    "    results['ProcessPool'] = {\n",
    "        'time': process_time,\n",
    "        'results': process_results\n",
    "    }\n",
    "    print(f\"   Time: {process_time:.2f}s\")\n",
    "    print(f\"   Speedup: {sequential_time/process_time:.2f}x\")\n",
    "    \n",
    "    # 4. Joblib\n",
    "    print(\"\\nüì¶ Joblib Parallel:\")\n",
    "    start_time = time.time()\n",
    "    joblib_results = Parallel(n_jobs=max_workers)(\n",
    "        delayed(scrape_single_url)(url) for url in urls\n",
    "    )\n",
    "    joblib_time = time.time() - start_time\n",
    "    results['Joblib'] = {\n",
    "        'time': joblib_time,\n",
    "        'results': joblib_results\n",
    "    }\n",
    "    print(f\"   Time: {joblib_time:.2f}s\")\n",
    "    print(f\"   Speedup: {sequential_time/joblib_time:.2f}x\")\n",
    "    \n",
    "    # Summary comparison\n",
    "    print(f\"\\nüìä Performance Summary:\")\n",
    "    print(f\"{'Method':<15} {'Time (s)':<10} {'Speedup':<10} {'Success Rate'}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for method, data in results.items():\n",
    "        success_count = sum(1 for r in data['results'] if r['status'] == 'success')\n",
    "        success_rate = success_count / len(urls) * 100\n",
    "        speedup = sequential_time / data['time'] if data['time'] > 0 else 0\n",
    "        \n",
    "        print(f\"{method:<15} {data['time']:<10.2f} {speedup:<10.2f} {success_rate:.1f}%\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test with a smaller set for comparison\n",
    "test_urls_small = [\n",
    "    'https://httpbin.org/delay/1',  # 1 second delay\n",
    "    'https://httpbin.org/delay/1',\n",
    "    'https://httpbin.org/delay/1',\n",
    "    'https://httpbin.org/delay/1',\n",
    "    'https://httpbin.org/json',\n",
    "    'https://httpbin.org/html',\n",
    "    'https://httpbin.org/xml',\n",
    "    'https://httpbin.org/user-agent'\n",
    "]\n",
    "\n",
    "print(\"üî¨ Comparing Parallel Processing Approaches\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "comparison_results = compare_parallel_approaches(test_urls_small, max_workers=4)\n",
    "\n",
    "# Memory usage comparison (simplified)\n",
    "print(f\"\\nüíæ Memory Usage Notes:\")\n",
    "print(\"   Sequential: Low memory, single process\")\n",
    "print(\"   ThreadPool: Medium memory, shared memory space\")\n",
    "print(\"   ProcessPool: High memory, separate processes\")\n",
    "print(\"   Joblib: Optimized memory usage, especially for NumPy arrays\")\n",
    "\n",
    "print(f\"\\nüéØ Recommendations:\")\n",
    "print(\"   ‚Ä¢ Use ThreadPool for I/O-bound web scraping\")\n",
    "print(\"   ‚Ä¢ Use ProcessPool for CPU-intensive data processing\")\n",
    "print(\"   ‚Ä¢ Use Joblib for data science and ML workloads\")\n",
    "print(\"   ‚Ä¢ Always implement rate limiting and error handling\")\n",
    "print(\"   ‚Ä¢ Monitor resource usage in production\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Key Takeaways: Parallel Web Scraping\n",
    "\n",
    "### ‚úÖ What We Learned\n",
    "\n",
    "1. **Multiprocessing Basics**: Understanding CPU cores and parallel execution\n",
    "2. **Joblib Library**: Simple and efficient parallel processing with `Parallel()` and `delayed()`\n",
    "3. **Rate Limiting**: Implementing respectful scraping with proper delays\n",
    "4. **Error Handling**: Robust retry mechanisms and failure recovery\n",
    "5. **Performance Comparison**: Different approaches for different use cases\n",
    "6. **Real-world Application**: E-commerce data scraping with batch processing\n",
    "\n",
    "### üöÄ When to Use Parallel Scraping\n",
    "\n",
    "**‚úÖ Good candidates:**\n",
    "- Large datasets (100s-1000s of URLs)\n",
    "- I/O-bound operations (network requests)\n",
    "- Independent scraping tasks\n",
    "- Time-sensitive data collection\n",
    "\n",
    "**‚ùå Avoid when:**\n",
    "- Small datasets (< 50 URLs)\n",
    "- Strict rate limits (< 1 req/sec)\n",
    "- Complex interdependent scraping\n",
    "- Server explicitly prohibits parallel access\n",
    "\n",
    "### üìã Production Checklist\n",
    "\n",
    "- [ ] Implement proper rate limiting\n",
    "- [ ] Add comprehensive error handling\n",
    "- [ ] Monitor resource usage (CPU, memory, network)\n",
    "- [ ] Respect robots.txt and terms of service\n",
    "- [ ] Implement logging and monitoring\n",
    "- [ ] Test with small datasets first\n",
    "- [ ] Use appropriate number of workers\n",
    "- [ ] Handle failures gracefully\n",
    "\n",
    "### üîó Next Steps\n",
    "\n",
    "1. Practice with the provided examples\n",
    "2. Implement rate limiting in your projects\n",
    "3. Experiment with different worker counts\n",
    "4. Monitor performance and optimize\n",
    "5. Always prioritize ethical scraping practices\n",
    "\n",
    "Remember: **With great power comes great responsibility!** Use parallel scraping responsibly and always respect website terms of service."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üï∏Ô∏è Scrapy Framework - Industrial-Strength Web Scraping\n",
    "\n",
    "Scrapy is not just a library - it's a **complete framework** for building web scrapers. Think of it as the difference between a hammer (Beautiful Soup) and a complete construction toolkit (Scrapy).\n",
    "\n",
    "### ü§î When to Choose Scrapy vs Beautiful Soup?\n",
    "\n",
    "| Use Case | Beautiful Soup | Scrapy |\n",
    "|----------|----------------|---------|\n",
    "| **Simple, one-time scraping** | ‚úÖ Perfect | ‚ùå Overkill |\n",
    "| **Large-scale projects** | ‚ùå Limited | ‚úÖ Excellent |\n",
    "| **Multiple websites** | ‚ùå Manual work | ‚úÖ Built-in support |\n",
    "| **Following links automatically** | ‚ùå Manual coding | ‚úÖ Built-in |\n",
    "| **Data export (CSV, JSON)** | ‚ùå Manual coding | ‚úÖ Built-in |\n",
    "| **Handling cookies/sessions** | ‚ùå Manual coding | ‚úÖ Automatic |\n",
    "| **Concurrent requests** | ‚ùå Manual threading | ‚úÖ Built-in |\n",
    "| **Respecting robots.txt** | ‚ùå Manual checking | ‚úÖ Automatic |\n",
    "\n",
    "### üèóÔ∏è Scrapy Architecture\n",
    "\n",
    "Scrapy follows a **component-based architecture**:\n",
    "\n",
    "1. **Engine** - Controls data flow between components\n",
    "2. **Scheduler** - Manages which URLs to scrape next\n",
    "3. **Downloader** - Fetches web pages\n",
    "4. **Spiders** - Your custom logic for extracting data\n",
    "5. **Item Pipeline** - Processes extracted data\n",
    "6. **Middlewares** - Hooks for customizing requests/responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üöÄ Getting Started with Scrapy\n",
    "\n",
    "#### Installation:\n",
    "```bash\n",
    "pip install scrapy\n",
    "```\n",
    "\n",
    "#### Creating a Scrapy Project:\n",
    "```bash\n",
    "# Create new project\n",
    "scrapy startproject myproject\n",
    "\n",
    "# Project structure created:\n",
    "myproject/\n",
    "    scrapy.cfg            # deploy configuration file\n",
    "    myproject/            # project's Python module\n",
    "        __init__.py\n",
    "        items.py          # project items definition file\n",
    "        middlewares.py    # project middlewares file\n",
    "        pipelines.py      # project pipelines file\n",
    "        settings.py       # project settings file\n",
    "        spiders/          # directory for spiders\n",
    "            __init__.py\n",
    "```\n",
    "\n",
    "#### Key Files Explained:\n",
    "- **spiders/** - Where you write your scraping logic\n",
    "- **items.py** - Define what data you want to extract\n",
    "- **pipelines.py** - Process the extracted data\n",
    "- **settings.py** - Configure how Scrapy behaves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üï∑Ô∏è Understanding Scrapy Spiders\n",
    "\n",
    "A **Spider** is a class that defines how to scrape a website. Every spider must:\n",
    "1. **Have a unique name**\n",
    "2. **Define starting URLs**\n",
    "3. **Implement a parse method**\n",
    "\n",
    "#### Basic Spider Structure:\n",
    "\n",
    "```python\n",
    "import scrapy\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = 'quotes'                    # Unique identifier\n",
    "    allowed_domains = ['quotes.toscrape.com']  # Optional: restrict domains\n",
    "    start_urls = ['http://quotes.toscrape.com/']  # Starting URLs\n",
    "    \n",
    "    def parse(self, response):\n",
    "        # This method is called for each start_url\n",
    "        # Extract data and/or follow links\n",
    "        pass\n",
    "```\n",
    "\n",
    "#### The `response` Object:\n",
    "- `response.css()` - Use CSS selectors\n",
    "- `response.xpath()` - Use XPath selectors  \n",
    "- `response.url` - Current URL\n",
    "- `response.status` - HTTP status code\n",
    "- `response.follow()` - Follow links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete Scrapy Spider Example (Simulated)\n",
    "# Note: This is how a Scrapy spider looks - normally it runs in Scrapy framework\n",
    "\n",
    "class QuotesSpider:\n",
    "    \"\"\"\n",
    "    Example Scrapy Spider for quotes.toscrape.com\n",
    "    This shows the structure and logic of a real Scrapy spider\n",
    "    \"\"\"\n",
    "    \n",
    "    name = 'quotes'\n",
    "    allowed_domains = ['quotes.toscrape.com']\n",
    "    start_urls = ['http://quotes.toscrape.com/']\n",
    "    \n",
    "    def parse(self, response):\n",
    "        \"\"\"\n",
    "        Main parsing method - called for each response\n",
    "        \n",
    "        Args:\n",
    "            response: Scrapy response object with methods:\n",
    "                - response.css('selector') - CSS selectors\n",
    "                - response.xpath('xpath') - XPath selectors  \n",
    "                - response.follow(link) - Follow links\n",
    "        \"\"\"\n",
    "        \n",
    "        # Extract all quotes on the current page\n",
    "        quotes = response.css('div.quote')\n",
    "        \n",
    "        for quote in quotes:\n",
    "            # Extract individual fields using CSS selectors\n",
    "            yield {\n",
    "                'text': quote.css('span.text::text').get(),\n",
    "                'author': quote.css('small.author::text').get(),\n",
    "                'tags': quote.css('div.tags a.tag::text').getall(),\n",
    "            }\n",
    "        \n",
    "        # Follow the \"Next\" page link automatically\n",
    "        next_page = response.css('li.next a::attr(href)').get()\n",
    "        if next_page is not None:\n",
    "            # This tells Scrapy to follow the link and call parse() again\n",
    "            yield response.follow(next_page, self.parse)\n",
    "\n",
    "# Let's simulate what Scrapy does behind the scenes\n",
    "print(\"üï∑Ô∏è Scrapy Spider Analysis:\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "print(\"\\n1Ô∏è‚É£ Spider Attributes:\")\n",
    "spider = QuotesSpider()\n",
    "print(f\"   Name: {spider.name}\")\n",
    "print(f\"   Allowed domains: {spider.allowed_domains}\")\n",
    "print(f\"   Starting URLs: {spider.start_urls}\")\n",
    "\n",
    "print(\"\\n2Ô∏è‚É£ How Scrapy Works:\")\n",
    "print(\"   Step 1: Scrapy sends requests to start_urls\")\n",
    "print(\"   Step 2: Calls parse() method with each response\")\n",
    "print(\"   Step 3: Spider yields data items and/or new requests\")\n",
    "print(\"   Step 4: Scrapy schedules new requests and processes items\")\n",
    "print(\"   Step 5: Repeats until no more requests\")\n",
    "\n",
    "print(\"\\n3Ô∏è‚É£ Key Scrapy Concepts:\")\n",
    "print(\"   üì• yield items ‚Üí Data extraction\")\n",
    "print(\"   üì§ yield requests ‚Üí Following links\")\n",
    "print(\"   üîÑ response.follow() ‚Üí Automatic link following\")\n",
    "print(\"   üéØ CSS/XPath selectors ‚Üí Element selection\")\n",
    "\n",
    "print(\"\\n4Ô∏è‚É£ Scrapy Selectors:\")\n",
    "print(\"   .get() ‚Üí Get first match (like select_one)\")\n",
    "print(\"   .getall() ‚Üí Get all matches (like select)\")\n",
    "print(\"   ::text ‚Üí Extract text content\")\n",
    "print(\"   ::attr(name) ‚Üí Extract attribute value\")\n",
    "\n",
    "print(\"\\n5Ô∏è‚É£ Running the Spider:\")\n",
    "print(\"   Command: scrapy crawl quotes -o quotes.json\")\n",
    "print(\"   Output: Saves all extracted data to quotes.json\")\n",
    "print(\"   Automatically: Handles requests, follows links, exports data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üì¶ Scrapy Items - Structured Data Definition\n",
    "\n",
    "**Items** define the structure of data you want to extract. Think of them as data containers with validation.\n",
    "\n",
    "```python\n",
    "# items.py\n",
    "import scrapy\n",
    "\n",
    "class QuoteItem(scrapy.Item):\n",
    "    text = scrapy.Field()\n",
    "    author = scrapy.Field()\n",
    "    tags = scrapy.Field()\n",
    "    url = scrapy.Field()\n",
    "    scraped_at = scrapy.Field()\n",
    "```\n",
    "\n",
    "**Benefits of Items:**\n",
    "- **Data validation** - Ensure consistent data structure\n",
    "- **IDE support** - Better autocomplete and error checking\n",
    "- **Documentation** - Clear data schema\n",
    "- **Pipeline compatibility** - Works seamlessly with pipelines\n",
    "\n",
    "### üîß Scrapy Pipelines - Data Processing\n",
    "\n",
    "**Pipelines** process items after extraction. Common uses:\n",
    "- Cleaning and validation\n",
    "- Duplicate removal\n",
    "- Database storage\n",
    "- File export\n",
    "\n",
    "```python\n",
    "# pipelines.py\n",
    "class QuotesPipeline:\n",
    "    def process_item(self, item, spider):\n",
    "        # Clean the quote text\n",
    "        item['text'] = item['text'].strip('\"')\n",
    "        \n",
    "        # Add timestamp\n",
    "        from datetime import datetime\n",
    "        item['scraped_at'] = datetime.now()\n",
    "        \n",
    "        return item\n",
    "\n",
    "class DuplicatesPipeline:\n",
    "    def __init__(self):\n",
    "        self.seen_quotes = set()\n",
    "    \n",
    "    def process_item(self, item, spider):\n",
    "        if item['text'] in self.seen_quotes:\n",
    "            raise DropItem(f\"Duplicate quote: {item['text']}\")\n",
    "        \n",
    "        self.seen_quotes.add(item['text'])\n",
    "        return item\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚ö° Scrapy Advanced Features\n",
    "\n",
    "#### üîß Built-in Settings (settings.py):\n",
    "```python\n",
    "# Respect robots.txt\n",
    "ROBOTSTXT_OBEY = True\n",
    "\n",
    "# Configure delays between requests\n",
    "DOWNLOAD_DELAY = 1  # 1 second delay\n",
    "RANDOMIZE_DOWNLOAD_DELAY = 0.5  # Random delay up to 50%\n",
    "\n",
    "# Configure concurrent requests\n",
    "CONCURRENT_REQUESTS = 16\n",
    "CONCURRENT_REQUESTS_PER_DOMAIN = 8\n",
    "\n",
    "# User agent\n",
    "USER_AGENT = 'myproject (+http://www.yourdomain.com)'\n",
    "\n",
    "# Enable pipelines\n",
    "ITEM_PIPELINES = {\n",
    "    'myproject.pipelines.DuplicatesPipeline': 300,\n",
    "    'myproject.pipelines.QuotesPipeline': 800,\n",
    "}\n",
    "```\n",
    "\n",
    "#### üéØ Scrapy Shell - Interactive Testing:\n",
    "```bash\n",
    "# Start interactive shell for testing selectors\n",
    "scrapy shell \"http://quotes.toscrape.com/\"\n",
    "\n",
    "# In shell:\n",
    ">>> response.css('div.quote').getall()\n",
    ">>> response.css('span.text::text').get()\n",
    ">>> view(response)  # Opens in browser\n",
    "```\n",
    "\n",
    "#### üöÄ Running Scrapy Spiders:\n",
    "```bash\n",
    "# Basic run\n",
    "scrapy crawl quotes\n",
    "\n",
    "# Export to different formats\n",
    "scrapy crawl quotes -o quotes.json\n",
    "scrapy crawl quotes -o quotes.csv\n",
    "scrapy crawl quotes -o quotes.xml\n",
    "\n",
    "# Custom settings\n",
    "scrapy crawl quotes -s USER_AGENT='Custom Bot'\n",
    "scrapy crawl quotes -s DOWNLOAD_DELAY=2\n",
    "\n",
    "# Multiple settings\n",
    "scrapy crawl quotes -s DOWNLOAD_DELAY=1 -s CONCURRENT_REQUESTS=1\n",
    "```\n",
    "\n",
    "#### üîç Monitoring and Debugging:\n",
    "```bash\n",
    "# Verbose output\n",
    "scrapy crawl quotes -L INFO\n",
    "\n",
    "# Very detailed debugging\n",
    "scrapy crawl quotes -L DEBUG\n",
    "\n",
    "# Save log to file\n",
    "scrapy crawl quotes --logfile=scrapy.log\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Resources & Documentation - Organized by Library\n",
    "\n",
    "### ü•Ñ Beautiful Soup Resources\n",
    "\n",
    "#### Official Documentation:\n",
    "- **[Beautiful Soup Documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)** - Complete official documentation\n",
    "- **[Beautiful Soup Quick Start](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#quick-start)** - Getting started guide\n",
    "- **[CSS Selectors Reference](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#css-selectors)** - CSS selector syntax\n",
    "\n",
    "#### Video Tutorials:\n",
    "- **[Beautiful Soup Tutorial](https://www.youtube.com/watch?v=87Gx3U0BDlo)** - Corey Schafer\n",
    "- **[Web Scraping with Beautiful Soup](https://www.youtube.com/watch?v=ng2o98k983k)** - Tech With Tim\n",
    "- **[Beautiful Soup Complete Guide](https://www.youtube.com/watch?v=XVv6mJpFOb0)** - freeCodeCamp\n",
    "\n",
    "#### Articles & Tutorials:\n",
    "- **[Real Python - Beautiful Soup Guide](https://realpython.com/beautiful-soup-web-scraper-python/)** - Comprehensive tutorial\n",
    "- **[GeeksforGeeks - Beautiful Soup](https://www.geeksforgeeks.org/implementing-web-scraping-python-beautiful-soup/)** - Step-by-step examples\n",
    "\n",
    "### üåê Requests Library Resources\n",
    "\n",
    "#### Official Documentation:\n",
    "- **[Requests Documentation](https://requests.readthedocs.io/)** - HTTP library documentation\n",
    "- **[Requests Quickstart](https://requests.readthedocs.io/en/latest/user/quickstart/)** - Basic usage examples\n",
    "- **[Advanced Usage](https://requests.readthedocs.io/en/latest/user/advanced/)** - Sessions, cookies, SSL\n",
    "\n",
    "#### Video Tutorials:\n",
    "- **[Requests Library Tutorial](https://www.youtube.com/watch?v=tb8gHvYlCFs)** - Corey Schafer\n",
    "- **[HTTP Requests in Python](https://www.youtube.com/watch?v=9N6a-VLBa2I)** - Programming with Mosh\n",
    "\n",
    "#### Articles:\n",
    "- **[Requests vs urllib](https://realpython.com/python-requests/)** - When to use what\n",
    "- **[Session Objects in Requests](https://requests.readthedocs.io/en/latest/user/advanced/#session-objects)** - Persistent sessions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üï∏Ô∏è Scrapy Framework Resources\n",
    "\n",
    "#### Official Documentation:\n",
    "- **[Scrapy Documentation](https://docs.scrapy.org/)** - Complete framework guide\n",
    "- **[Scrapy Tutorial](https://docs.scrapy.org/en/latest/intro/tutorial.html)** - Step-by-step tutorial\n",
    "- **[Scrapy Best Practices](https://docs.scrapy.org/en/latest/topics/practices.html)** - Production tips\n",
    "- **[Scrapy Shell](https://docs.scrapy.org/en/latest/topics/shell.html)** - Interactive testing\n",
    "\n",
    "#### Video Tutorials:\n",
    "- **[Scrapy Framework Tutorial](https://www.youtube.com/watch?v=s4jtkzHhLzY)** - Traversy Media\n",
    "- **[Complete Scrapy Course](https://www.youtube.com/watch?v=mBoX_JCKZTE)** - Coding Entrepreneurs\n",
    "- **[Scrapy vs Beautiful Soup](https://www.youtube.com/watch?v=52rxmBEmeKQ)** - When to use what\n",
    "\n",
    "#### Books & Courses:\n",
    "- **\"Learning Scrapy\"** by Dimitris Kouzis-Loukas - Packt\n",
    "- **\"Web Scraping with Python and Scrapy\"** - Udemy courses\n",
    "- **[Scrapy GitHub Examples](https://github.com/scrapy/scrapy/tree/master/docs/topics/examples)** - Official examples\n",
    "\n",
    "### üöó Selenium Resources\n",
    "\n",
    "#### Official Documentation:\n",
    "- **[Selenium Documentation](https://selenium-python.readthedocs.io/)** - Official Python bindings\n",
    "- **[WebDriver API](https://selenium-python.readthedocs.io/api.html)** - Complete API reference\n",
    "- **[Waits in Selenium](https://selenium-python.readthedocs.io/waits.html)** - Handling dynamic content\n",
    "- **[Selenium Grid](https://selenium.dev/documentation/grid/)** - Distributed testing\n",
    "\n",
    "#### Video Tutorials:\n",
    "- **[Selenium WebDriver with Python](https://www.youtube.com/watch?v=Xjv1sY630Uc)** - Programming with Mosh\n",
    "- **[Selenium Complete Course](https://www.youtube.com/watch?v=j7VZsCCnptM)** - Edureka\n",
    "- **[Selenium with Python Tutorial](https://www.youtube.com/watch?v=zjo9yFHoUl8)** - Telusko\n",
    "\n",
    "#### Articles & Guides:\n",
    "- **[Real Python - Selenium Guide](https://realpython.com/modern-web-automation-with-python-and-selenium/)** - Modern web automation\n",
    "- **[Selenium Best Practices](https://selenium.dev/documentation/test_practices/)** - Official best practices\n",
    "- **[Handling Dynamic Content](https://selenium-python.readthedocs.io/waits.html)** - WebDriverWait examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚ö° Parallel Processing & Advanced Topics\n",
    "\n",
    "#### Joblib Resources:\n",
    "- **[Joblib Documentation](https://joblib.readthedocs.io/)** - Official documentation\n",
    "- **[Parallel Computing with Joblib](https://joblib.readthedocs.io/en/latest/parallel.html)** - Parallel processing guide\n",
    "- **[Joblib vs Multiprocessing](https://stackoverflow.com/questions/20776189/joblib-vs-multiprocessing)** - When to use what\n",
    "\n",
    "#### Multiprocessing & Threading:\n",
    "- **[Python Multiprocessing](https://docs.python.org/3/library/multiprocessing.html)** - Official documentation\n",
    "- **[Concurrent.futures](https://docs.python.org/3/library/concurrent.futures.html)** - High-level interface\n",
    "- **[Threading vs Multiprocessing](https://realpython.com/python-concurrency/)** - Real Python guide\n",
    "\n",
    "#### Performance & Optimization:\n",
    "- **[Web Scraping Performance](https://blog.apify.com/web-scraping-performance-optimization/)** - Apify blog\n",
    "- **[Async Web Scraping](https://www.scrapehero.com/async-web-scraping-with-aiohttp/)** - ScrapeHero guide\n",
    "\n",
    "### üõ†Ô∏è General Web Scraping Resources\n",
    "\n",
    "#### Practice Websites:\n",
    "- **[Quotes to Scrape](http://quotes.toscrape.com/)** - Perfect for beginners\n",
    "- **[Books to Scrape](http://books.toscrape.com/)** - E-commerce practice\n",
    "- **[Scrape This Site](https://scrapethissite.com/)** - Various challenges\n",
    "- **[HTTP Bin](https://httpbin.org/)** - HTTP testing service\n",
    "\n",
    "#### Alternative Libraries:\n",
    "- **[requests-html](https://github.com/psf/requests-html)** - JavaScript support for requests\n",
    "- **[playwright-python](https://playwright.dev/python/)** - Modern browser automation\n",
    "- **[httpx](https://www.python-httpx.org/)** - Next-generation HTTP client\n",
    "- **[pyppeteer](https://github.com/pyppeteer/pyppeteer)** - Puppeteer port for Python\n",
    "\n",
    "#### Data Processing:\n",
    "- **[Pandas Documentation](https://pandas.pydata.org/docs/)** - Data manipulation and analysis\n",
    "- **[NumPy User Guide](https://numpy.org/doc/stable/user/)** - Numerical computing\n",
    "- **[Matplotlib Tutorials](https://matplotlib.org/stable/tutorials/index.html)** - Data visualization\n",
    "\n",
    "### üìñ Books & Comprehensive Courses\n",
    "\n",
    "#### Recommended Books:\n",
    "- **\"Web Scraping with Python\"** by Ryan Mitchell - O'Reilly Media (Classic)\n",
    "- **\"Python Web Scraping Cookbook\"** by Michael Heydt - Packt\n",
    "- **\"Mastering Python Web Scraping\"** - Advanced techniques\n",
    "\n",
    "#### Online Courses:\n",
    "- **[freeCodeCamp Web Scraping](https://www.youtube.com/watch?v=XVv6mJpFOb0)** - 3+ hour complete course\n",
    "- **[Udemy Web Scraping Courses](https://www.udemy.com/topic/web-scraping/)** - Various paid courses\n",
    "- **[Coursera Web Scraping](https://www.coursera.org/search?query=web%20scraping)** - University courses\n",
    "\n",
    "### üéØ Learning Path Recommendations\n",
    "\n",
    "#### Beginner (1-2 weeks):\n",
    "1. HTML/CSS basics\n",
    "2. Beautiful Soup fundamentals\n",
    "3. Simple scraping projects\n",
    "4. Practice websites\n",
    "\n",
    "#### Intermediate (2-4 weeks):\n",
    "5. Selenium for dynamic content\n",
    "6. Error handling and robustness\n",
    "7. Data processing with pandas\n",
    "8. Multiple page scraping\n",
    "\n",
    "#### Advanced (4+ weeks):\n",
    "9. Scrapy framework\n",
    "10. Parallel processing\n",
    "11. Large-scale projects\n",
    "12. Production deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üõ†Ô∏è ‘≥’∏÷Ä’Æ’∂’°’Ø’°’∂\n",
    "\n",
    "Let's put our knowledge into practice with hands-on exercises!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Exercise 1: News Headlines Scraper\n",
    "\n",
    "Create a scraper that extracts news headlines from a news website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: News Headlines Scraper\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "def scrape_news_headlines():\n",
    "    \"\"\"\n",
    "    Scrape news headlines from a sample news site\n",
    "    Note: In real projects, always check robots.txt and terms of service\n",
    "    \"\"\"\n",
    "    \n",
    "    # Using BBC RSS feed as an example (more reliable than scraping HTML)\n",
    "    url = \"http://feeds.bbci.co.uk/news/rss.xml\"\n",
    "    \n",
    "    try:\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "        }\n",
    "        \n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Parse XML (RSS feeds are XML)\n",
    "        soup = BeautifulSoup(response.content, 'xml')\n",
    "        \n",
    "        # Find all items (news articles)\n",
    "        items = soup.find_all('item')\n",
    "        \n",
    "        news_data = []\n",
    "        \n",
    "        for item in items[:10]:  # Get first 10 articles\n",
    "            title = item.find('title')\n",
    "            link = item.find('link')\n",
    "            description = item.find('description')\n",
    "            pub_date = item.find('pubDate')\n",
    "            \n",
    "            news_data.append({\n",
    "                'title': title.text if title else 'N/A',\n",
    "                'link': link.text if link else 'N/A',\n",
    "                'description': description.text if description else 'N/A',\n",
    "                'published': pub_date.text if pub_date else 'N/A',\n",
    "                'scraped_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            })\n",
    "        \n",
    "        return news_data\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping news: {e}\")\n",
    "        return []\n",
    "\n",
    "# Run the scraper\n",
    "print(\"üì∞ Scraping BBC News Headlines...\")\n",
    "news_headlines = scrape_news_headlines()\n",
    "\n",
    "if news_headlines:\n",
    "    print(f\"‚úÖ Successfully scraped {len(news_headlines)} headlines\")\n",
    "    \n",
    "    # Display first 3 headlines\n",
    "    for i, article in enumerate(news_headlines[:3], 1):\n",
    "        print(f\"\\n{i}. {article['title']}\")\n",
    "        print(f\"   Published: {article['published']}\")\n",
    "        print(f\"   Description: {article['description'][:100]}...\")\n",
    "    \n",
    "    # Save to CSV\n",
    "    df = pd.DataFrame(news_headlines)\n",
    "    df.to_csv('news_headlines.csv', index=False, encoding='utf-8')\n",
    "    print(f\"\\nüíæ Data saved to 'news_headlines.csv'\")\n",
    "    \n",
    "    # Show basic statistics\n",
    "    print(f\"\\nüìä Statistics:\")\n",
    "    print(f\"   Total articles: {len(news_headlines)}\")\n",
    "    print(f\"   Average title length: {df['title'].str.len().mean():.1f} characters\")\n",
    "else:\n",
    "    print(\"‚ùå No headlines found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Exercise 2: Table Data Scraper\n",
    "\n",
    "Extract tabular data from websites and convert it to pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Table Data Scraper\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Create sample HTML table for demonstration\n",
    "sample_table_html = \"\"\"\n",
    "<html>\n",
    "<body>\n",
    "    <h2>Cryptocurrency Prices</h2>\n",
    "    <table id=\"crypto-table\" class=\"data-table\">\n",
    "        <thead>\n",
    "            <tr>\n",
    "                <th>Rank</th>\n",
    "                <th>Name</th>\n",
    "                <th>Symbol</th>\n",
    "                <th>Price (USD)</th>\n",
    "                <th>24h Change</th>\n",
    "                <th>Market Cap</th>\n",
    "            </tr>\n",
    "        </thead>\n",
    "        <tbody>\n",
    "            <tr>\n",
    "                <td>1</td>\n",
    "                <td>Bitcoin</td>\n",
    "                <td>BTC</td>\n",
    "                <td>$43,250.00</td>\n",
    "                <td class=\"positive\">+2.34%</td>\n",
    "                <td>$847.5B</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td>2</td>\n",
    "                <td>Ethereum</td>\n",
    "                <td>ETH</td>\n",
    "                <td>$2,580.50</td>\n",
    "                <td class=\"negative\">-1.25%</td>\n",
    "                <td>$310.2B</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td>3</td>\n",
    "                <td>Cardano</td>\n",
    "                <td>ADA</td>\n",
    "                <td>$0.45</td>\n",
    "                <td class=\"positive\">+5.67%</td>\n",
    "                <td>$15.2B</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td>4</td>\n",
    "                <td>Solana</td>\n",
    "                <td>SOL</td>\n",
    "                <td>$98.75</td>\n",
    "                <td class=\"positive\">+3.21%</td>\n",
    "                <td>$42.8B</td>\n",
    "            </tr>\n",
    "        </tbody>\n",
    "    </table>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "def scrape_table_data(html_content):\n",
    "    \"\"\"Extract table data and convert to pandas DataFrame\"\"\"\n",
    "    \n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    # Find the table\n",
    "    table = soup.find('table', {'id': 'crypto-table'})\n",
    "    \n",
    "    if not table:\n",
    "        print(\"‚ùå Table not found\")\n",
    "        return None\n",
    "    \n",
    "    # Extract headers\n",
    "    headers = []\n",
    "    header_row = table.find('thead').find('tr')\n",
    "    for th in header_row.find_all('th'):\n",
    "        headers.append(th.text.strip())\n",
    "    \n",
    "    print(f\"üìã Table headers: {headers}\")\n",
    "    \n",
    "    # Extract data rows\n",
    "    data_rows = []\n",
    "    tbody = table.find('tbody')\n",
    "    \n",
    "    for row in tbody.find_all('tr'):\n",
    "        row_data = []\n",
    "        for td in row.find_all('td'):\n",
    "            # Clean the text (remove extra whitespace, currency symbols, etc.)\n",
    "            cell_text = td.text.strip()\n",
    "            row_data.append(cell_text)\n",
    "        data_rows.append(row_data)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(data_rows, columns=headers)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def clean_financial_data(df):\n",
    "    \"\"\"Clean and process financial data\"\"\"\n",
    "    \n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # Clean price column (remove $ and convert to float)\n",
    "    if 'Price (USD)' in df_clean.columns:\n",
    "        df_clean['Price_Numeric'] = df_clean['Price (USD)'].str.replace('$', '').str.replace(',', '').astype(float)\n",
    "    \n",
    "    # Clean percentage change (remove % and convert to float)\n",
    "    if '24h Change' in df_clean.columns:\n",
    "        df_clean['Change_Numeric'] = df_clean['24h Change'].str.replace('%', '').str.replace('+', '').astype(float)\n",
    "    \n",
    "    # Clean market cap (convert to billions)\n",
    "    if 'Market Cap' in df_clean.columns:\n",
    "        def parse_market_cap(cap_str):\n",
    "            cap_str = cap_str.replace('$', '').replace(',', '')\n",
    "            if 'B' in cap_str:\n",
    "                return float(cap_str.replace('B', '')) * 1e9\n",
    "            elif 'M' in cap_str:\n",
    "                return float(cap_str.replace('M', '')) * 1e6\n",
    "            return float(cap_str)\n",
    "        \n",
    "        df_clean['Market_Cap_Numeric'] = df_clean['Market Cap'].apply(parse_market_cap)\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "# Scrape the table\n",
    "print(\"üìä Scraping table data...\")\n",
    "crypto_df = scrape_table_data(sample_table_html)\n",
    "\n",
    "if crypto_df is not None:\n",
    "    print(\"\\n‚úÖ Raw table data:\")\n",
    "    print(crypto_df.to_string(index=False))\n",
    "    \n",
    "    # Clean the data\n",
    "    crypto_clean = clean_financial_data(crypto_df)\n",
    "    \n",
    "    print(\"\\nüßπ Cleaned data with numeric columns:\")\n",
    "    print(crypto_clean[['Name', 'Symbol', 'Price_Numeric', 'Change_Numeric']].to_string(index=False))\n",
    "    \n",
    "    # Basic analysis\n",
    "    print(\"\\nüìà Quick Analysis:\")\n",
    "    print(f\"   Average price: ${crypto_clean['Price_Numeric'].mean():,.2f}\")\n",
    "    print(f\"   Highest price: {crypto_clean.loc[crypto_clean['Price_Numeric'].idxmax(), 'Name']} (${crypto_clean['Price_Numeric'].max():,.2f})\")\n",
    "    print(f\"   Best performer: {crypto_clean.loc[crypto_clean['Change_Numeric'].idxmax(), 'Name']} ({crypto_clean['Change_Numeric'].max()}%)\")\n",
    "    print(f\"   Worst performer: {crypto_clean.loc[crypto_clean['Change_Numeric'].idxmin(), 'Name']} ({crypto_clean['Change_Numeric'].min()}%)\")\n",
    "    \n",
    "    # Save to CSV\n",
    "    crypto_clean.to_csv('crypto_data.csv', index=False)\n",
    "    print(\"\\nüíæ Data saved to 'crypto_data.csv'\")\n",
    "else:\n",
    "    print(\"‚ùå Failed to scrape table data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \udfaf Summary & Next Steps\n",
    "\n",
    "## üìö What We've Learned\n",
    "\n",
    "### üåê HTML & CSS Fundamentals\n",
    "- **HTML structure**: Tags, attributes, and document hierarchy\n",
    "- **CSS selectors**: The foundation of all web scraping\n",
    "- **Key concepts**: Classes, IDs, and element relationships\n",
    "\n",
    "### ü•Ñ Beautiful Soup Mastery\n",
    "- **Parsing HTML**: Converting text to navigable objects\n",
    "- **Finding elements**: Multiple methods for element selection\n",
    "- **Data extraction**: Getting text, attributes, and structured data\n",
    "- **Navigation**: Moving between parents, children, and siblings\n",
    "\n",
    "### üï∏Ô∏è Scrapy Framework\n",
    "- **When to use**: Large-scale, production scraping projects\n",
    "- **Architecture**: Spiders, Items, Pipelines, and Settings\n",
    "- **Advanced features**: Automatic link following, data export, rate limiting\n",
    "- **Commands**: Creating projects and running spiders\n",
    "\n",
    "### üöó Selenium for Dynamic Content\n",
    "- **JavaScript handling**: Scraping interactive websites\n",
    "- **WebDriver control**: Automating browser actions\n",
    "- **Waiting strategies**: Handling dynamic content loading\n",
    "- **User simulation**: Clicks, form filling, and scrolling\n",
    "\n",
    "### ‚ö° Parallel Processing\n",
    "- **Performance optimization**: Multiprocessing and joblib\n",
    "- **Rate limiting**: Respectful scraping practices\n",
    "- **Error handling**: Robust scraping systems\n",
    "- **Real-world applications**: E-commerce and large datasets\n",
    "\n",
    "## üöÄ Your Learning Path Forward\n",
    "\n",
    "### üìñ Practice Projects (Beginner)\n",
    "1. **News Headlines Scraper**: Start with RSS feeds or simple news sites\n",
    "2. **Product Price Monitor**: Track prices on e-commerce sites\n",
    "3. **Weather Data Collector**: Scrape weather information\n",
    "4. **Social Media Posts**: Extract public posts (check terms of service!)\n",
    "\n",
    "### üîß Intermediate Projects\n",
    "5. **Multi-page Scraper**: Follow pagination automatically\n",
    "6. **Data Analysis Pipeline**: Scrape ‚Üí Clean ‚Üí Analyze ‚Üí Visualize\n",
    "7. **API vs Scraping**: Compare when to use APIs vs scraping\n",
    "8. **Error Recovery System**: Build robust scrapers with retry logic\n",
    "\n",
    "### üèÜ Advanced Projects\n",
    "9. **Distributed Scraping**: Use Scrapy with multiple machines\n",
    "10. **Anti-bot Evasion**: Handle CAPTCHAs and bot detection\n",
    "11. **Real-time Monitoring**: Build alerting systems\n",
    "12. **Machine Learning Integration**: Use scraped data for ML projects\n",
    "\n",
    "## üí° Best Practices to Remember\n",
    "\n",
    "### ‚úÖ Do:\n",
    "- Start with simple static websites\n",
    "- Always check robots.txt\n",
    "- Add delays between requests\n",
    "- Handle errors gracefully\n",
    "- Save data in structured formats\n",
    "- Test selectors thoroughly\n",
    "- Use version control for your scrapers\n",
    "\n",
    "### ‚ùå Don't:\n",
    "- Scrape faster than necessary\n",
    "- Ignore HTTP status codes\n",
    "- Scrape without permission for commercial use\n",
    "- Store sensitive data insecurely\n",
    "- Forget to close browser instances (Selenium)\n",
    "- Hardcode values that might change\n",
    "\n",
    "## üîó Keep Learning\n",
    "\n",
    "- **Join communities**: Reddit r/webscraping, Stack Overflow\n",
    "- **Read documentation**: Stay updated with library changes\n",
    "- **Build projects**: Apply what you've learned\n",
    "- **Share knowledge**: Help others learn web scraping\n",
    "- **Stay ethical**: Always respect website terms and rate limits\n",
    "\n",
    "### üéì Recommended Next Topics\n",
    "- **Data Analysis**: pandas, matplotlib, seaborn\n",
    "- **Databases**: SQLite, PostgreSQL, MongoDB\n",
    "- **APIs**: requests, authentication, REST APIs\n",
    "- **Deployment**: Docker, cloud services, scheduling\n",
    "- **Monitoring**: Logging, error tracking, performance metrics\n",
    "\n",
    "Remember: **The best way to learn web scraping is by doing!** Start with simple projects and gradually increase complexity as you gain confidence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üé≤ 00\n",
    "- ‚ñ∂Ô∏è[Video]()\n",
    "- üîó[Random link]()\n",
    "- üá¶üá≤üé∂[]()\n",
    "- üåêüé∂[]()\n",
    "- ü§å[‘ø’°÷Ä’£’´’∂]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TfbUegKqXVyH"
   },
   "source": [
    "\n",
    "<a href=\"http://s01.flagcounter.com/more/1oO\"><img src=\"https://s01.flagcounter.com/count2/1oO/bg_FFFFFF/txt_000000/border_CCCCCC/columns_2/maxflags_10/viewers_0/labels_0/pageviews_1/flags_0/percent_0/\" alt=\"Flag Counter\"></a>\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOx7X+DxNeKu1zUVVCmsSHJ",
   "provenance": [
    {
     "file_id": "1_9UtYmPVVGmnWIKdBzPYkbtTlTbd0clo",
     "timestamp": 1735604987843
    },
    {
     "file_id": "15x56uwwONMo_ilzUJgt6UcX1d552xH6X",
     "timestamp": 1708441161475
    },
    {
     "file_id": "1LbG88IWtk30WlIoINzG4_vXHoJAvoDaP",
     "timestamp": 1683614319950
    }
   ]
  },
  "kernelspec": {
   "display_name": "lectures",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
