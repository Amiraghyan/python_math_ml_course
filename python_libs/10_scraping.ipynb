{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"10 Scraping\"\n",
    "description: \"Beautiful Soup, Scrapy, and Selenium\"\n",
    "lightbox: true\n",
    "format: \n",
    "  html:\n",
    "    code-fold: false\n",
    "number-offset: 1\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "![image.png](../background_photos/)\n",
    "[’¨’∏÷Ç’Ω’°’∂’Ø’°÷Ä’´ ’∞’≤’∏÷Ç’¥’®](https://unsplash.com/photos/a-large-mountain-with-a-very-tall-cliff-UiP9KfVe3aQ), ’Ä’•’≤’´’∂’°’Ø’ù []()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "<a href=\"ToDo\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a> (ToDo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "> Song reference - ToDo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# üìå ’Ü’Ø’°÷Ä’°’£’´÷Ä\n",
    "\n",
    "[üìö ‘±’¥’¢’∏’≤’ª’°’Ø’°’∂ ’∂’µ’∏÷Ç’©’®]()\n",
    "\n",
    "#### üì∫ ’è’•’Ω’°’∂’µ’∏÷Ç’©’•÷Ä\n",
    "#### üè° ’è’∂’°’µ’´’∂"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# üìö ’Ü’µ’∏÷Ç’©’®"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üåê HTML & CSS Basics\n",
    "\n",
    "Before diving into web scraping, it's essential to understand the structure of web pages. HTML (HyperText Markup Language) provides the structure, while CSS (Cascading Style Sheets) handles the styling.\n",
    "\n",
    "### HTML Structure\n",
    "\n",
    "HTML uses **tags** to define elements. Tags are enclosed in angle brackets `< >` and usually come in pairs:\n",
    "\n",
    "```html\n",
    "<tagname>Content goes here</tagname>\n",
    "```\n",
    "\n",
    "#### Common HTML Tags:\n",
    "\n",
    "- `<html>` - Root element\n",
    "- `<head>` - Contains metadata\n",
    "- `<title>` - Page title\n",
    "- `<body>` - Visible page content\n",
    "- `<h1>`, `<h2>`, `<h3>` - Headers\n",
    "- `<p>` - Paragraphs\n",
    "- `<div>` - Generic container\n",
    "- `<span>` - Inline container\n",
    "- `<a>` - Links\n",
    "- `<img>` - Images\n",
    "- `<ul>`, `<ol>`, `<li>` - Lists\n",
    "- `<table>`, `<tr>`, `<td>` - Tables\n",
    "\n",
    "#### HTML Attributes:\n",
    "\n",
    "Attributes provide additional information about elements:\n",
    "\n",
    "```html\n",
    "<div id=\"content\" class=\"main-section\">\n",
    "<a href=\"https://example.com\" target=\"_blank\">Link</a>\n",
    "<img src=\"image.jpg\" alt=\"Description\">\n",
    "```\n",
    "\n",
    "**Important attributes for scraping:**\n",
    "- `id` - Unique identifier\n",
    "- `class` - CSS class name(s)\n",
    "- `href` - Link destination\n",
    "- `src` - Source for images/scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSS Selectors\n",
    "\n",
    "CSS selectors are crucial for web scraping as they help us target specific elements:\n",
    "\n",
    "#### Basic Selectors:\n",
    "- **Element**: `p` (selects all `<p>` elements)\n",
    "- **Class**: `.classname` (selects elements with `class=\"classname\"`)\n",
    "- **ID**: `#idname` (selects element with `id=\"idname\"`)\n",
    "- **Attribute**: `[attribute=\"value\"]`\n",
    "\n",
    "#### Combination Selectors:\n",
    "- **Descendant**: `div p` (all `<p>` inside `<div>`)\n",
    "- **Child**: `div > p` (direct `<p>` children of `<div>`)\n",
    "- **Adjacent**: `h1 + p` (first `<p>` after `<h1>`)\n",
    "\n",
    "### Sample HTML Document:\n",
    "\n",
    "```html\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "    <title>Sample Page</title>\n",
    "</head>\n",
    "<body>\n",
    "    <header id=\"main-header\">\n",
    "        <h1>Welcome to My Site</h1>\n",
    "        <nav class=\"navigation\">\n",
    "            <ul>\n",
    "                <li><a href=\"#home\">Home</a></li>\n",
    "                <li><a href=\"#about\">About</a></li>\n",
    "                <li><a href=\"#contact\">Contact</a></li>\n",
    "            </ul>\n",
    "        </nav>\n",
    "    </header>\n",
    "    \n",
    "    <main class=\"content\">\n",
    "        <article class=\"post\" data-id=\"123\">\n",
    "            <h2 class=\"post-title\">Article Title</h2>\n",
    "            <p class=\"post-content\">This is the article content...</p>\n",
    "            <div class=\"post-meta\">\n",
    "                <span class=\"author\">John Doe</span>\n",
    "                <span class=\"date\">2025-01-15</span>\n",
    "            </div>\n",
    "        </article>\n",
    "        \n",
    "        <article class=\"post\" data-id=\"124\">\n",
    "            <h2 class=\"post-title\">Another Article</h2>\n",
    "            <p class=\"post-content\">More content here...</p>\n",
    "            <div class=\"post-meta\">\n",
    "                <span class=\"author\">Jane Smith</span>\n",
    "                <span class=\"date\">2025-01-16</span>\n",
    "            </div>\n",
    "        </article>\n",
    "    </main>\n",
    "    \n",
    "    <footer>\n",
    "        <p>&copy; 2025 My Website</p>\n",
    "    </footer>\n",
    "</body>\n",
    "</html>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üï∑Ô∏è Web Scraping Fundamentals\n",
    "\n",
    "Web scraping is the process of extracting data from websites programmatically. Python offers several powerful libraries for this purpose:\n",
    "\n",
    "1. **Beautiful Soup** - For parsing HTML/XML\n",
    "2. **Requests** - For making HTTP requests\n",
    "3. **Scrapy** - Full-featured scraping framework\n",
    "4. **Selenium** - For JavaScript-heavy sites\n",
    "\n",
    "### ü•Ñ Beautiful Soup\n",
    "\n",
    "Beautiful Soup is perfect for beginners and handles most scraping tasks effectively.\n",
    "\n",
    "#### Installation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install beautifulsoup4 requests lxml html5lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Beautiful Soup example\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# Sample HTML for demonstration\n",
    "html_content = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "    <title>Sample Page</title>\n",
    "</head>\n",
    "<body>\n",
    "    <header id=\"main-header\">\n",
    "        <h1>Welcome to My Site</h1>\n",
    "        <nav class=\"navigation\">\n",
    "            <ul>\n",
    "                <li><a href=\"#home\">Home</a></li>\n",
    "                <li><a href=\"#about\">About</a></li>\n",
    "                <li><a href=\"#contact\">Contact</a></li>\n",
    "            </ul>\n",
    "        </nav>\n",
    "    </header>\n",
    "    \n",
    "    <main class=\"content\">\n",
    "        <article class=\"post\" data-id=\"123\">\n",
    "            <h2 class=\"post-title\">Article Title</h2>\n",
    "            <p class=\"post-content\">This is the article content...</p>\n",
    "            <div class=\"post-meta\">\n",
    "                <span class=\"author\">John Doe</span>\n",
    "                <span class=\"date\">2025-01-15</span>\n",
    "            </div>\n",
    "        </article>\n",
    "        \n",
    "        <article class=\"post\" data-id=\"124\">\n",
    "            <h2 class=\"post-title\">Another Article</h2>\n",
    "            <p class=\"post-content\">More content here...</p>\n",
    "            <div class=\"post-meta\">\n",
    "                <span class=\"author\">Jane Smith</span>\n",
    "                <span class=\"date\">2025-01-16</span>\n",
    "            </div>\n",
    "        </article>\n",
    "    </main>\n",
    "    \n",
    "    <footer>\n",
    "        <p>&copy; 2025 My Website</p>\n",
    "    </footer>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "# Create BeautifulSoup object\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "print(\"üîç Basic Beautiful Soup Operations:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. Find by tag\n",
    "title = soup.find('title')\n",
    "print(f\"Page title: {title.text}\")\n",
    "\n",
    "# 2. Find by class\n",
    "articles = soup.find_all('article', class_='post')\n",
    "print(f\"Number of articles: {len(articles)}\")\n",
    "\n",
    "# 3. Find by ID\n",
    "header = soup.find('header', id='main-header')\n",
    "print(f\"Header text: {header.h1.text}\")\n",
    "\n",
    "# 4. CSS selectors\n",
    "nav_links = soup.select('nav.navigation a')\n",
    "print(f\"Navigation links: {[link.text for link in nav_links]}\")\n",
    "\n",
    "# 5. Extract data from each article\n",
    "print(\"\\nüì∞ Article Information:\")\n",
    "for i, article in enumerate(articles, 1):\n",
    "    title = article.find('h2', class_='post-title').text\n",
    "    content = article.find('p', class_='post-content').text\n",
    "    author = article.find('span', class_='author').text\n",
    "    date = article.find('span', class_='date').text\n",
    "    data_id = article.get('data-id')\n",
    "    \n",
    "    print(f\"\\nArticle {i}:\")\n",
    "    print(f\"  ID: {data_id}\")\n",
    "    print(f\"  Title: {title}\")\n",
    "    print(f\"  Author: {author}\")\n",
    "    print(f\"  Date: {date}\")\n",
    "    print(f\"  Content: {content[:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üåê Real Website Scraping Example\n",
    "\n",
    "Let's scrape some real data from a website. We'll use `httpbin.org` which provides testing endpoints:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import required libraries for web scraping\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import json\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Define the scraping function\n",
    "def scrape_quotes():\n",
    "    \"\"\"\n",
    "    Scrape quotes from quotes.toscrape.com\n",
    "    Returns a list of dictionaries containing quote data\n",
    "    \"\"\"\n",
    "    url = \"http://quotes.toscrape.com/\"\n",
    "    \n",
    "    try:\n",
    "        print(f\"üåê Sending request to: {url}\")\n",
    "        \n",
    "        # Send GET request to the website\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raise exception for bad status codes\n",
    "        \n",
    "        print(f\"‚úÖ Request successful! Status code: {response.status_code}\")\n",
    "        \n",
    "        # Parse HTML content with Beautiful Soup\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Find all quote containers\n",
    "        quotes = soup.find_all('div', class_='quote')\n",
    "        print(f\"üìä Found {len(quotes)} quotes on the page\")\n",
    "        \n",
    "        scraped_data = []\n",
    "        \n",
    "        # Extract data from each quote\n",
    "        for i, quote in enumerate(quotes, 1):\n",
    "            # Extract quote text (remove quotes and whitespace)\n",
    "            text = quote.find('span', class_='text').text\n",
    "            \n",
    "            # Extract author name\n",
    "            author = quote.find('small', class_='author').text\n",
    "            \n",
    "            # Extract tags (multiple tags per quote)\n",
    "            tags = [tag.text for tag in quote.find_all('a', class_='tag')]\n",
    "            \n",
    "            # Store data in dictionary\n",
    "            quote_data = {\n",
    "                'text': text,\n",
    "                'author': author,\n",
    "                'tags': tags\n",
    "            }\n",
    "            \n",
    "            scraped_data.append(quote_data)\n",
    "            print(f\"  üìù Processed quote {i}: {author}\")\n",
    "        \n",
    "        return scraped_data\n",
    "    \n",
    "    except requests.RequestException as e:\n",
    "        print(f\"‚ùå Error fetching the webpage: {e}\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error processing data: {e}\")\n",
    "        return []\n",
    "\n",
    "print(\"‚úÖ Function defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Run the scraper and collect data\n",
    "print(\"üï∑Ô∏è Starting the scraping process...\")\n",
    "quotes_data = scrape_quotes()\n",
    "\n",
    "print(f\"\\nüìä Scraping completed!\")\n",
    "print(f\"Total quotes collected: {len(quotes_data)}\")\n",
    "\n",
    "if quotes_data:\n",
    "    print(\"\\nüéØ Sample of scraped data:\")\n",
    "    print(\"=\" * 60)\n",
    "else:\n",
    "    print(\"‚ùå No quotes were scraped. Check your internet connection.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Display the first few quotes to see our results\n",
    "if quotes_data:\n",
    "    print(\"üìù First 3 quotes from our scraping:\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for i, quote in enumerate(quotes_data[:3], 1):\n",
    "        print(f\"\\nüí¨ Quote {i}:\")\n",
    "        print(f\"   Text: {quote['text']}\")\n",
    "        print(f\"   Author: {quote['author']}\")\n",
    "        print(f\"   Tags: {', '.join(quote['tags'])}\")\n",
    "        print(\"-\" * 60)\n",
    "    \n",
    "    # Show some statistics\n",
    "    print(f\"\\nüìà Quick Statistics:\")\n",
    "    print(f\"   Total quotes: {len(quotes_data)}\")\n",
    "    \n",
    "    # Find unique authors\n",
    "    authors = set(quote['author'] for quote in quotes_data)\n",
    "    print(f\"   Unique authors: {len(authors)}\")\n",
    "    \n",
    "    # Find all unique tags\n",
    "    all_tags = set()\n",
    "    for quote in quotes_data:\n",
    "        all_tags.update(quote['tags'])\n",
    "    print(f\"   Unique tags: {len(all_tags)}\")\n",
    "    print(f\"   Some tags: {', '.join(list(all_tags)[:5])}...\")\n",
    "else:\n",
    "    print(\"‚ùå No data to display\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Save the scraped data to a file\n",
    "if quotes_data:\n",
    "    # Save to JSON file with proper formatting\n",
    "    filename = 'quotes_scraped.json'\n",
    "    \n",
    "    try:\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(quotes_data, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        print(f\"üíæ Data saved successfully to '{filename}'\")\n",
    "        print(f\"üìÅ File contains {len(quotes_data)} quotes\")\n",
    "        \n",
    "        # Show file size\n",
    "        import os\n",
    "        file_size = os.path.getsize(filename)\n",
    "        print(f\"üìä File size: {file_size:,} bytes\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error saving file: {e}\")\n",
    "        \n",
    "    # Also demonstrate saving specific data\n",
    "    print(f\"\\nüîç Example of accessing specific quote data:\")\n",
    "    if len(quotes_data) > 0:\n",
    "        first_quote = quotes_data[0]\n",
    "        print(f\"   First quote text: {first_quote['text'][:50]}...\")\n",
    "        print(f\"   First quote author: {first_quote['author']}\")\n",
    "        print(f\"   First quote tags: {first_quote['tags']}\")\n",
    "else:\n",
    "    print(\"‚ùå No data to save\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üéì What We Just Did - Step by Step:\n",
    "\n",
    "1. **üì¶ Imported Libraries**: We imported the essential tools:\n",
    "   - `requests` for making HTTP requests\n",
    "   - `BeautifulSoup` for parsing HTML\n",
    "   - `json` for saving data\n",
    "   - `time` for adding delays (good practice)\n",
    "\n",
    "2. **üîß Created Function**: We defined `scrape_quotes()` that:\n",
    "   - Sends a GET request to the website\n",
    "   - Handles errors gracefully\n",
    "   - Parses HTML with Beautiful Soup\n",
    "   - Extracts specific data using CSS selectors\n",
    "\n",
    "3. **üöÄ Executed Scraper**: We ran the function and collected data\n",
    "\n",
    "4. **üëÄ Viewed Results**: We displayed the scraped quotes to verify success\n",
    "\n",
    "5. **üíæ Saved Data**: We saved the results to a JSON file for future use\n",
    "\n",
    "**Key Learning Points:**\n",
    "- Always check `response.status_code` to ensure successful requests\n",
    "- Use `.find()` for single elements and `.find_all()` for multiple elements\n",
    "- Handle exceptions to make your scraper robust\n",
    "- Save data in structured formats like JSON or CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üè† Real Estate Scraping: List.am Example\n",
    "\n",
    "List.am is Armenia's popular classifieds website. Let's create a comprehensive scraper for real estate listings. This example demonstrates scraping a real Armenian website with proper error handling and data processing.\n",
    "\n",
    "**‚ö†Ô∏è Important Notes:**\n",
    "- Always check robots.txt: https://www.list.am/robots.txt\n",
    "- Respect the website's terms of service\n",
    "- Add appropriate delays between requests\n",
    "- This is for educational purposes only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List.am Real Estate Scraper\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import json\n",
    "\n",
    "def scrape_listam_listings(base_url=\"https://www.list.am/category/62\", max_pages=2, delay=2):\n",
    "    \"\"\"\n",
    "    Scrape real estate listings from list.am\n",
    "    \n",
    "    Args:\n",
    "        base_url (str): Base URL for the category\n",
    "        max_pages (int): Maximum number of pages to scrape\n",
    "        delay (int): Delay between requests in seconds\n",
    "    \n",
    "    Returns:\n",
    "        list: List of dictionaries containing listing data\n",
    "    \"\"\"\n",
    "    \n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "        'Accept-Language': 'en-US,en;q=0.5',\n",
    "        'Accept-Encoding': 'gzip, deflate',\n",
    "        'Connection': 'keep-alive',\n",
    "        'Upgrade-Insecure-Requests': '1',\n",
    "    }\n",
    "    \n",
    "    all_listings = []\n",
    "    \n",
    "    for page in range(1, max_pages + 1):\n",
    "        try:\n",
    "            # Construct page URL\n",
    "            if page == 1:\n",
    "                page_url = base_url\n",
    "            else:\n",
    "                page_url = f\"{base_url}/{page}\"\n",
    "            \n",
    "            print(f\"üîç Scraping page {page}: {page_url}\")\n",
    "            \n",
    "            # Add delay to be respectful\n",
    "            if page > 1:\n",
    "                time.sleep(delay)\n",
    "            \n",
    "            # Send request\n",
    "            response = requests.get(page_url, headers=headers, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            # Parse HTML\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            # Find listing containers (adjust selectors based on actual HTML structure)\n",
    "            listings = soup.find_all('a', href=True)\n",
    "            \n",
    "            page_listings = []\n",
    "            \n",
    "            for listing in listings:\n",
    "                href = listing.get('href', '')\n",
    "                \n",
    "                # Filter for item links\n",
    "                if '/item/' in href and href.startswith('/item/'):\n",
    "                    # Extract item ID\n",
    "                    item_match = re.search(r'/item/(\\d+)', href)\n",
    "                    if not item_match:\n",
    "                        continue\n",
    "                    \n",
    "                    item_id = item_match.group(1)\n",
    "                    full_url = urljoin(base_url, href)\n",
    "                    \n",
    "                    # Extract text content from the link\n",
    "                    text_content = listing.get_text(strip=True)\n",
    "                    \n",
    "                    # Parse listing information from text\n",
    "                    listing_data = parse_listing_text(text_content, item_id, full_url)\n",
    "                    \n",
    "                    if listing_data:\n",
    "                        page_listings.append(listing_data)\n",
    "            \n",
    "            print(f\"   ‚úÖ Found {len(page_listings)} listings on page {page}\")\n",
    "            all_listings.extend(page_listings)\n",
    "            \n",
    "            # Check if there's a next page\n",
    "            next_link = soup.find('a', string='’Ä’°’ª’∏÷Ä’§’® >')\n",
    "            if not next_link and page == max_pages:\n",
    "                print(\"üìÑ Reached last page or max pages limit\")\n",
    "                break\n",
    "                \n",
    "        except requests.RequestException as e:\n",
    "            print(f\"‚ùå Error fetching page {page}: {e}\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error parsing page {page}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return all_listings\n",
    "\n",
    "def parse_listing_text(text, item_id, url):\n",
    "    \"\"\"\n",
    "    Parse listing information from text content\n",
    "    \n",
    "    Args:\n",
    "        text (str): Text content of the listing\n",
    "        item_id (str): Item ID\n",
    "        url (str): Full URL to the listing\n",
    "    \n",
    "    Returns:\n",
    "        dict: Parsed listing data\n",
    "    \"\"\"\n",
    "    \n",
    "    if not text or len(text.strip()) < 10:\n",
    "        return None\n",
    "    \n",
    "    # Initialize listing data\n",
    "    listing = {\n",
    "        'id': item_id,\n",
    "        'url': url,\n",
    "        'raw_text': text.strip(),\n",
    "        'price': None,\n",
    "        'price_currency': None,\n",
    "        'location': None,\n",
    "        'property_type': None,\n",
    "        'area_sqm': None,\n",
    "        'rooms': None,\n",
    "        'floor': None,\n",
    "        'description': None\n",
    "    }\n",
    "    \n",
    "    # Extract price (handles both USD and AMD)\n",
    "    price_usd_match = re.search(r'\\$([0-9,]+(?:\\.[0-9]+)?)', text)\n",
    "    price_amd_match = re.search(r'([0-9,]+(?:\\.[0-9]+)?)\\s*÷è', text)\n",
    "    \n",
    "    if price_usd_match:\n",
    "        listing['price'] = price_usd_match.group(1).replace(',', '')\n",
    "        listing['price_currency'] = 'USD'\n",
    "    elif price_amd_match:\n",
    "        listing['price'] = price_amd_match.group(1).replace(',', '')\n",
    "        listing['price_currency'] = 'AMD'\n",
    "    \n",
    "    # Extract area (square meters)\n",
    "    area_match = re.search(r'(\\d+)\\s*÷Ñ’¥', text)\n",
    "    if area_match:\n",
    "        listing['area_sqm'] = area_match.group(1)\n",
    "    \n",
    "    # Extract number of rooms\n",
    "    rooms_match = re.search(r'(\\d+)\\s*’Ω’•’∂', text)\n",
    "    if rooms_match:\n",
    "        listing['rooms'] = rooms_match.group(1)\n",
    "    \n",
    "    # Extract floor information\n",
    "    floor_match = re.search(r'(\\d+)/(\\d+)\\s*’∞’°÷Ä’Ø', text)\n",
    "    if floor_match:\n",
    "        listing['floor'] = f\"{floor_match.group(1)}/{floor_match.group(2)}\"\n",
    "    \n",
    "    # Extract location (common locations in Yerevan)\n",
    "    locations = [\n",
    "        '‘ø’•’∂’ø÷Ä’∏’∂', '‘±÷Ä’°’¢’Ø’´÷Ä', '‘¥’°’æ’©’°’∑’•’∂', '’Ñ’°’¨’°’©’´’°-’ç’•’¢’°’Ω’ø’´’°', \n",
    "        '’á’•’∂’£’°’æ’´’©', '’Ü’∏÷Ä ’Ü’∏÷Ä÷Ñ', '‘±’ª’°÷É’∂’µ’°’Ø', '‘±’æ’°’∂', '‘∑÷Ä’•’¢’∏÷Ç’∂’´',\n",
    "        '‘≥’µ’∏÷Ç’¥÷Ä’´', '’é’°’∂’°’±’∏÷Ä', '‘±’¢’∏’æ’µ’°’∂', '‘±÷Ä’ø’°’∑’°’ø', '‘≥÷á’°÷Ä÷Ñ',\n",
    "        '‘æ’°’≤’Ø’°’±’∏÷Ä', '‘¥’´’¨’´’ª’°’∂', '‘ª’ª÷á’°’∂', '‘≥’∏÷Ä’´’Ω', '‘ø’°’∫’°’∂'\n",
    "    ]\n",
    "    \n",
    "    for location in locations:\n",
    "        if location in text:\n",
    "            listing['location'] = location\n",
    "            break\n",
    "    \n",
    "    # Determine property type based on keywords\n",
    "    if '’¢’∂’°’Ø’°÷Ä’°’∂' in text:\n",
    "        listing['property_type'] = 'Apartment'\n",
    "    elif '’ø’∏÷Ç’∂' in text or '’©’°’∏÷Ç’∂’∞’°’∏÷Ç’¶' in text:\n",
    "        listing['property_type'] = 'House'\n",
    "    elif '’∞’∏’≤’°’ø’°÷Ä’°’Æ÷Ñ' in text:\n",
    "        listing['property_type'] = 'Land'\n",
    "    elif '’°’æ’ø’∏’ø’∂’°’Ø' in text:\n",
    "        listing['property_type'] = 'Garage'\n",
    "    elif '’£÷Ä’°’Ω’•’∂’µ’°’Ø' in text:\n",
    "        listing['property_type'] = 'Office'\n",
    "    else:\n",
    "        listing['property_type'] = 'Other'\n",
    "    \n",
    "    # Clean description (remove price and location)\n",
    "    description = text\n",
    "    if listing['price'] and listing['price_currency']:\n",
    "        price_pattern = rf\"\\${listing['price']}|{listing['price']}\\s*÷è\"\n",
    "        description = re.sub(price_pattern, '', description)\n",
    "    \n",
    "    if listing['location']:\n",
    "        description = description.replace(listing['location'], '')\n",
    "    \n",
    "    listing['description'] = description.strip()\n",
    "    \n",
    "    return listing\n",
    "\n",
    "def analyze_listings(listings):\n",
    "    \"\"\"\n",
    "    Analyze scraped listings and provide statistics\n",
    "    \n",
    "    Args:\n",
    "        listings (list): List of listing dictionaries\n",
    "    \n",
    "    Returns:\n",
    "        dict: Analysis results\n",
    "    \"\"\"\n",
    "    \n",
    "    if not listings:\n",
    "        return {}\n",
    "    \n",
    "    df = pd.DataFrame(listings)\n",
    "    \n",
    "    # Convert price to numeric for analysis\n",
    "    df['price_numeric'] = pd.to_numeric(df['price'].str.replace(',', ''), errors='coerce')\n",
    "    df['area_numeric'] = pd.to_numeric(df['area_sqm'], errors='coerce')\n",
    "    df['rooms_numeric'] = pd.to_numeric(df['rooms'], errors='coerce')\n",
    "    \n",
    "    analysis = {\n",
    "        'total_listings': len(listings),\n",
    "        'unique_locations': df['location'].nunique(),\n",
    "        'property_types': df['property_type'].value_counts().to_dict(),\n",
    "        'currency_distribution': df['price_currency'].value_counts().to_dict(),\n",
    "        'price_stats': {},\n",
    "        'area_stats': {},\n",
    "        'location_stats': df['location'].value_counts().head(10).to_dict()\n",
    "    }\n",
    "    \n",
    "    # Price statistics (for USD listings)\n",
    "    usd_prices = df[df['price_currency'] == 'USD']['price_numeric'].dropna()\n",
    "    if len(usd_prices) > 0:\n",
    "        analysis['price_stats']['USD'] = {\n",
    "            'count': len(usd_prices),\n",
    "            'mean': round(usd_prices.mean(), 2),\n",
    "            'median': round(usd_prices.median(), 2),\n",
    "            'min': usd_prices.min(),\n",
    "            'max': usd_prices.max()\n",
    "        }\n",
    "    \n",
    "    # Area statistics\n",
    "    areas = df['area_numeric'].dropna()\n",
    "    if len(areas) > 0:\n",
    "        analysis['area_stats'] = {\n",
    "            'count': len(areas),\n",
    "            'mean': round(areas.mean(), 2),\n",
    "            'median': round(areas.median(), 2),\n",
    "            'min': areas.min(),\n",
    "            'max': areas.max()\n",
    "        }\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "# Example usage\n",
    "print(\"üè† Starting List.am Real Estate Scraper...\")\n",
    "print(\"‚ö†Ô∏è  Remember: This is for educational purposes only!\")\n",
    "print(\"üïê Adding delays between requests to be respectful...\")\n",
    "\n",
    "# Scrape listings (limiting to 2 pages for demo)\n",
    "listings = scrape_listam_listings(max_pages=2, delay=3)\n",
    "\n",
    "print(f\"\\nüìä Scraping completed! Total listings found: {len(listings)}\")\n",
    "\n",
    "if listings:\n",
    "    print(\"\\nüè† Sample listings:\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for i, listing in enumerate(listings[:5], 1):\n",
    "        print(f\"\\n{i}. ID: {listing['id']}\")\n",
    "        print(f\"   Type: {listing['property_type']}\")\n",
    "        print(f\"   Price: {listing['price']} {listing['price_currency'] or 'N/A'}\")\n",
    "        print(f\"   Location: {listing['location'] or 'N/A'}\")\n",
    "        print(f\"   Area: {listing['area_sqm']} sqm\" if listing['area_sqm'] else \"   Area: N/A\")\n",
    "        print(f\"   Rooms: {listing['rooms']}\" if listing['rooms'] else \"   Rooms: N/A\")\n",
    "        print(f\"   Description: {listing['description'][:60]}...\")\n",
    "        print(f\"   URL: {listing['url']}\")\n",
    "else:\n",
    "    print(\"‚ùå No listings found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Analysis and Visualization\n",
    "if listings:\n",
    "    print(\"\\nüìà Analyzing scraped data...\")\n",
    "    \n",
    "    # Perform analysis\n",
    "    analysis = analyze_listings(listings)\n",
    "    \n",
    "    print(f\"\\nüìä Analysis Results:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"üìã Total listings: {analysis['total_listings']}\")\n",
    "    print(f\"üèôÔ∏è Unique locations: {analysis['unique_locations']}\")\n",
    "    \n",
    "    print(f\"\\nüè† Property types:\")\n",
    "    for prop_type, count in analysis['property_types'].items():\n",
    "        print(f\"   {prop_type}: {count}\")\n",
    "    \n",
    "    print(f\"\\nüí∞ Currency distribution:\")\n",
    "    for currency, count in analysis['currency_distribution'].items():\n",
    "        if currency:  # Skip None values\n",
    "            print(f\"   {currency}: {count}\")\n",
    "    \n",
    "    if 'USD' in analysis['price_stats']:\n",
    "        usd_stats = analysis['price_stats']['USD']\n",
    "        print(f\"\\nüíµ USD Price statistics:\")\n",
    "        print(f\"   Count: {usd_stats['count']}\")\n",
    "        print(f\"   Average: ${usd_stats['mean']:,.2f}\")\n",
    "        print(f\"   Median: ${usd_stats['median']:,.2f}\")\n",
    "        print(f\"   Range: ${usd_stats['min']:,.0f} - ${usd_stats['max']:,.0f}\")\n",
    "    \n",
    "    if analysis['area_stats']:\n",
    "        area_stats = analysis['area_stats']\n",
    "        print(f\"\\nüìê Area statistics (sqm):\")\n",
    "        print(f\"   Count: {area_stats['count']}\")\n",
    "        print(f\"   Average: {area_stats['mean']:.1f} sqm\")\n",
    "        print(f\"   Median: {area_stats['median']:.1f} sqm\")\n",
    "        print(f\"   Range: {area_stats['min']} - {area_stats['max']} sqm\")\n",
    "    \n",
    "    print(f\"\\nüó∫Ô∏è Top locations:\")\n",
    "    for location, count in list(analysis['location_stats'].items())[:5]:\n",
    "        if location:  # Skip None values\n",
    "            print(f\"   {location}: {count}\")\n",
    "    \n",
    "    # Save data to CSV\n",
    "    df = pd.DataFrame(listings)\n",
    "    filename = f'listam_listings_{pd.Timestamp.now().strftime(\"%Y%m%d_%H%M%S\")}.csv'\n",
    "    df.to_csv(filename, index=False, encoding='utf-8')\n",
    "    print(f\"\\nüíæ Data saved to: {filename}\")\n",
    "    \n",
    "    # Save analysis to JSON\n",
    "    analysis_filename = f'listam_analysis_{pd.Timestamp.now().strftime(\"%Y%m%d_%H%M%S\")}.json'\n",
    "    with open(analysis_filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(analysis, f, ensure_ascii=False, indent=2, default=str)\n",
    "    print(f\"üìä Analysis saved to: {analysis_filename}\")\n",
    "else:\n",
    "    print(\"‚ùå No data to analyze\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced List.am Scraping Techniques\n",
    "\n",
    "def scrape_detailed_listing(listing_url, headers=None):\n",
    "    \"\"\"\n",
    "    Scrape detailed information from a single listing page\n",
    "    \n",
    "    Args:\n",
    "        listing_url (str): URL of the specific listing\n",
    "        headers (dict): HTTP headers to use\n",
    "    \n",
    "    Returns:\n",
    "        dict: Detailed listing information\n",
    "    \"\"\"\n",
    "    \n",
    "    if headers is None:\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "        }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(listing_url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Extract detailed information (adjust selectors based on actual page structure)\n",
    "        details = {\n",
    "            'url': listing_url,\n",
    "            'title': None,\n",
    "            'price': None,\n",
    "            'description': None,\n",
    "            'contact_info': None,\n",
    "            'images': [],\n",
    "            'features': [],\n",
    "            'posted_date': None\n",
    "        }\n",
    "        \n",
    "        # Extract title\n",
    "        title_element = soup.find('h1') or soup.find('title')\n",
    "        if title_element:\n",
    "            details['title'] = title_element.get_text(strip=True)\n",
    "        \n",
    "        # Extract description\n",
    "        desc_selectors = [\n",
    "            'div.description', \n",
    "            'div.content', \n",
    "            '.item-description',\n",
    "            'p'\n",
    "        ]\n",
    "        \n",
    "        for selector in desc_selectors:\n",
    "            desc_element = soup.select_one(selector)\n",
    "            if desc_element and len(desc_element.get_text(strip=True)) > 50:\n",
    "                details['description'] = desc_element.get_text(strip=True)\n",
    "                break\n",
    "        \n",
    "        # Extract images\n",
    "        img_elements = soup.find_all('img', src=True)\n",
    "        for img in img_elements:\n",
    "            src = img.get('src')\n",
    "            if src and ('jpg' in src or 'jpeg' in src or 'png' in src):\n",
    "                full_img_url = urljoin(listing_url, src)\n",
    "                details['images'].append(full_img_url)\n",
    "        \n",
    "        # Extract contact information (phone numbers)\n",
    "        text_content = soup.get_text()\n",
    "        phone_patterns = [\n",
    "            r'\\+374\\s?\\d{2}\\s?\\d{3}\\s?\\d{3}',  # Armenian format\n",
    "            r'0\\d{2}\\s?\\d{3}\\s?\\d{3}',        # Local format\n",
    "            r'\\d{2}-\\d{2}-\\d{2}'              # Alternative format\n",
    "        ]\n",
    "        \n",
    "        for pattern in phone_patterns:\n",
    "            phones = re.findall(pattern, text_content)\n",
    "            if phones:\n",
    "                details['contact_info'] = phones[0]\n",
    "                break\n",
    "        \n",
    "        return details\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error scraping detailed listing {listing_url}: {e}\")\n",
    "        return {}\n",
    "\n",
    "def create_price_monitor(target_criteria, check_interval=3600):\n",
    "    \"\"\"\n",
    "    Create a price monitoring system for specific criteria\n",
    "    \n",
    "    Args:\n",
    "        target_criteria (dict): Criteria to monitor (location, max_price, min_area, etc.)\n",
    "        check_interval (int): Check interval in seconds\n",
    "    \n",
    "    Returns:\n",
    "        function: Monitoring function\n",
    "    \"\"\"\n",
    "    \n",
    "    def monitor():\n",
    "        print(f\"üîç Monitoring for: {target_criteria}\")\n",
    "        \n",
    "        # Get current listings\n",
    "        current_listings = scrape_listam_listings(max_pages=1, delay=2)\n",
    "        \n",
    "        matching_listings = []\n",
    "        \n",
    "        for listing in current_listings:\n",
    "            matches = True\n",
    "            \n",
    "            # Check location\n",
    "            if 'location' in target_criteria:\n",
    "                if listing['location'] != target_criteria['location']:\n",
    "                    matches = False\n",
    "            \n",
    "            # Check max price\n",
    "            if 'max_price_usd' in target_criteria and listing['price'] and listing['price_currency'] == 'USD':\n",
    "                try:\n",
    "                    price = float(listing['price'].replace(',', ''))\n",
    "                    if price > target_criteria['max_price_usd']:\n",
    "                        matches = False\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            # Check minimum area\n",
    "            if 'min_area' in target_criteria and listing['area_sqm']:\n",
    "                try:\n",
    "                    area = int(listing['area_sqm'])\n",
    "                    if area < target_criteria['min_area']:\n",
    "                        matches = False\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            # Check property type\n",
    "            if 'property_type' in target_criteria:\n",
    "                if listing['property_type'] != target_criteria['property_type']:\n",
    "                    matches = False\n",
    "            \n",
    "            if matches:\n",
    "                matching_listings.append(listing)\n",
    "        \n",
    "        if matching_listings:\n",
    "            print(f\"üéØ Found {len(matching_listings)} matching listings:\")\n",
    "            for listing in matching_listings:\n",
    "                print(f\"   - {listing['property_type']} in {listing['location']}: {listing['price']} {listing['price_currency']}\")\n",
    "                print(f\"     URL: {listing['url']}\")\n",
    "        else:\n",
    "            print(\"‚ùå No matching listings found\")\n",
    "        \n",
    "        return matching_listings\n",
    "    \n",
    "    return monitor\n",
    "\n",
    "# Example: Monitor for apartments in Kentron under $200,000\n",
    "print(\"\\nüéØ Setting up price monitoring example...\")\n",
    "monitor_criteria = {\n",
    "    'location': '‘ø’•’∂’ø÷Ä’∏’∂',\n",
    "    'max_price_usd': 200000,\n",
    "    'min_area': 50,\n",
    "    'property_type': 'Apartment'\n",
    "}\n",
    "\n",
    "price_monitor = create_price_monitor(monitor_criteria)\n",
    "\n",
    "print(\"\\nüí° Price monitor created! You can run price_monitor() to check for matching listings.\")\n",
    "print(\"üîÑ In a real application, you would schedule this to run periodically.\")\n",
    "\n",
    "# Example of running the monitor once\n",
    "print(\"\\nüèÉ‚Äç‚ôÇÔ∏è Running price monitor once as example...\")\n",
    "# matching = price_monitor()  # Uncomment to run the monitor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîí List.am Scraping: Ethical Guidelines & Best Practices\n",
    "\n",
    "#### ‚öñÔ∏è Legal and Ethical Considerations for List.am:\n",
    "\n",
    "1. **robots.txt Compliance**: \n",
    "   - Check https://www.list.am/robots.txt before scraping\n",
    "   - Respect crawl delays and disallowed paths\n",
    "\n",
    "2. **Rate Limiting**: \n",
    "   - Use delays between requests (minimum 2-3 seconds)\n",
    "   - Don't overwhelm the server with concurrent requests\n",
    "   - Consider scraping during off-peak hours\n",
    "\n",
    "3. **Data Usage**:\n",
    "   - Personal data (phone numbers, emails) should be handled carefully\n",
    "   - Don't republish copyrighted content without permission\n",
    "   - Use data for analysis, not commercial republication\n",
    "\n",
    "4. **Respectful Scraping**:\n",
    "   - Monitor your requests and stop if blocked\n",
    "   - Use appropriate User-Agent headers\n",
    "   - Don't scrape more data than you need\n",
    "\n",
    "#### üõ†Ô∏è Technical Best Practices:\n",
    "\n",
    "```python\n",
    "# Good practices for List.am scraping:\n",
    "\n",
    "# 1. Session management\n",
    "session = requests.Session()\n",
    "session.headers.update({\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "})\n",
    "\n",
    "# 2. Error handling with retries\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "\n",
    "retry_strategy = Retry(\n",
    "    total=3,\n",
    "    backoff_factor=2,\n",
    "    status_forcelist=[429, 500, 502, 503, 504],\n",
    ")\n",
    "adapter = HTTPAdapter(max_retries=retry_strategy)\n",
    "session.mount(\"http://\", adapter)\n",
    "session.mount(\"https://\", adapter)\n",
    "\n",
    "# 3. Respect for robots.txt\n",
    "import urllib.robotparser\n",
    "\n",
    "def can_fetch(url):\n",
    "    rp = urllib.robotparser.RobotFileParser()\n",
    "    rp.set_url(\"https://www.list.am/robots.txt\")\n",
    "    rp.read()\n",
    "    return rp.can_fetch(\"*\", url)\n",
    "\n",
    "# 4. Caching to reduce requests\n",
    "import pickle\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def cache_get(url, cache_hours=24):\n",
    "    cache_file = f\"cache_{hash(url)}.pkl\"\n",
    "    \n",
    "    if os.path.exists(cache_file):\n",
    "        with open(cache_file, 'rb') as f:\n",
    "            cached_data, timestamp = pickle.load(f)\n",
    "        \n",
    "        if datetime.now() - timestamp < timedelta(hours=cache_hours):\n",
    "            return cached_data\n",
    "    \n",
    "    return None\n",
    "\n",
    "def cache_set(url, data):\n",
    "    cache_file = f\"cache_{hash(url)}.pkl\"\n",
    "    with open(cache_file, 'wb') as f:\n",
    "        pickle.dump((data, datetime.now()), f)\n",
    "```\n",
    "\n",
    "#### üö® Red Flags to Avoid:\n",
    "\n",
    "- **Don't** scrape faster than 1 request per second\n",
    "- **Don't** ignore HTTP status codes (429, 503, etc.)\n",
    "- **Don't** scrape personal contact information for spam\n",
    "- **Don't** bypass anti-bot measures aggressively\n",
    "- **Don't** scrape without checking terms of service\n",
    "\n",
    "#### üí° Alternative Approaches:\n",
    "\n",
    "1. **Official APIs**: Check if List.am offers an API\n",
    "2. **RSS Feeds**: Look for RSS/XML feeds for listings\n",
    "3. **Partnership**: Contact List.am for data partnership\n",
    "4. **Manual Collection**: For small datasets, manual collection might be appropriate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ Advanced Beautiful Soup Techniques\n",
    "\n",
    "#### 1. Different Parsing Methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Beautiful Soup techniques\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "sample_html = \"\"\"\n",
    "<div class=\"container\">\n",
    "    <div class=\"product\" data-price=\"29.99\" data-category=\"electronics\">\n",
    "        <h3>Smartphone</h3>\n",
    "        <p class=\"description\">Latest smartphone with amazing features</p>\n",
    "        <span class=\"price\">$29.99</span>\n",
    "        <div class=\"reviews\">\n",
    "            <span class=\"rating\">4.5</span>\n",
    "            <span class=\"review-count\">(150 reviews)</span>\n",
    "        </div>\n",
    "    </div>\n",
    "    \n",
    "    <div class=\"product\" data-price=\"599.99\" data-category=\"electronics\">\n",
    "        <h3>Laptop</h3>\n",
    "        <p class=\"description\">High-performance laptop for professionals</p>\n",
    "        <span class=\"price\">$599.99</span>\n",
    "        <div class=\"reviews\">\n",
    "            <span class=\"rating\">4.8</span>\n",
    "            <span class=\"review-count\">(89 reviews)</span>\n",
    "        </div>\n",
    "    </div>\n",
    "    \n",
    "    <article class=\"blog-post\">\n",
    "        <h2>Tech News</h2>\n",
    "        <p>Latest technology trends and updates...</p>\n",
    "        <time datetime=\"2025-01-15\">January 15, 2025</time>\n",
    "    </article>\n",
    "</div>\n",
    "\"\"\"\n",
    "\n",
    "soup = BeautifulSoup(sample_html, 'html.parser')\n",
    "\n",
    "print(\"üîß Advanced Beautiful Soup Techniques:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. Find with attributes\n",
    "print(\"\\n1Ô∏è‚É£ Finding by attributes:\")\n",
    "expensive_products = soup.find_all('div', {'data-price': lambda x: x and float(x) > 100})\n",
    "for product in expensive_products:\n",
    "    name = product.h3.text\n",
    "    price = product.get('data-price')\n",
    "    print(f\"   {name}: ${price}\")\n",
    "\n",
    "# 2. Using regular expressions\n",
    "print(\"\\n2Ô∏è‚É£ Using regex patterns:\")\n",
    "price_spans = soup.find_all('span', string=re.compile(r'\\$\\d+\\.\\d+'))\n",
    "for span in price_spans:\n",
    "    print(f\"   Found price: {span.text}\")\n",
    "\n",
    "# 3. CSS selectors advanced\n",
    "print(\"\\n3Ô∏è‚É£ Advanced CSS selectors:\")\n",
    "# Products with rating above 4.5\n",
    "high_rated = soup.select('div.product:has(.rating)')\n",
    "for product in high_rated:\n",
    "    name = product.h3.text\n",
    "    rating = product.select_one('.rating').text\n",
    "    if float(rating) > 4.5:\n",
    "        print(f\"   High-rated: {name} ({rating}‚≠ê)\")\n",
    "\n",
    "# 4. Parent and sibling navigation\n",
    "print(\"\\n4Ô∏è‚É£ Navigation between elements:\")\n",
    "rating_element = soup.find('span', class_='rating')\n",
    "if rating_element:\n",
    "    # Get parent\n",
    "    reviews_div = rating_element.parent\n",
    "    print(f\"   Parent element: {reviews_div.name}\")\n",
    "    \n",
    "    # Get sibling\n",
    "    review_count = rating_element.find_next_sibling('span')\n",
    "    print(f\"   Review count: {review_count.text}\")\n",
    "\n",
    "# 5. Extracting numbers from text\n",
    "print(\"\\n5Ô∏è‚É£ Extracting numbers from text:\")\n",
    "review_texts = soup.find_all('span', class_='review-count')\n",
    "for review in review_texts:\n",
    "    # Extract number using regex\n",
    "    numbers = re.findall(r'\\d+', review.text)\n",
    "    if numbers:\n",
    "        print(f\"   Reviews: {numbers[0]}\")\n",
    "\n",
    "# 6. Custom filters\n",
    "print(\"\\n6Ô∏è‚É£ Custom filters:\")\n",
    "def has_class_and_data_price(tag):\n",
    "    return tag.has_attr('class') and tag.has_attr('data-price')\n",
    "\n",
    "products_with_price = soup.find_all(has_class_and_data_price)\n",
    "for product in products_with_price:\n",
    "    print(f\"   Product: {product.h3.text}, Price: ${product['data-price']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöó Selenium - For Dynamic Content\n",
    "\n",
    "Selenium is essential when dealing with JavaScript-heavy websites that load content dynamically.\n",
    "\n",
    "### When to use Selenium:\n",
    "- Websites with JavaScript-rendered content\n",
    "- Sites requiring interaction (clicking, scrolling, forms)\n",
    "- Single Page Applications (SPAs)\n",
    "- Content loaded via AJAX\n",
    "\n",
    "### Installation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Selenium and WebDriver\n",
    "!pip install selenium webdriver-manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Selenium example\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import time\n",
    "\n",
    "def selenium_scraping_example():\n",
    "    \"\"\"Example of scraping with Selenium\"\"\"\n",
    "    \n",
    "    # Setup Chrome options\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")  # Run in background\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    \n",
    "    try:\n",
    "        # Setup WebDriver\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "        \n",
    "        print(\"üöó Starting Selenium WebDriver...\")\n",
    "        \n",
    "        # Navigate to website\n",
    "        url = \"https://quotes.toscrape.com/js/\"  # JavaScript version\n",
    "        driver.get(url)\n",
    "        \n",
    "        # Wait for content to load\n",
    "        wait = WebDriverWait(driver, 10)\n",
    "        quotes = wait.until(EC.presence_of_all_elements_located((By.CLASS_NAME, \"quote\")))\n",
    "        \n",
    "        print(f\"üìÑ Found {len(quotes)} quotes on the page\")\n",
    "        \n",
    "        scraped_quotes = []\n",
    "        \n",
    "        # Extract data\n",
    "        for i, quote in enumerate(quotes[:3], 1):  # First 3 quotes\n",
    "            text_element = quote.find_element(By.CLASS_NAME, \"text\")\n",
    "            author_element = quote.find_element(By.CLASS_NAME, \"author\")\n",
    "            tags_elements = quote.find_elements(By.CLASS_NAME, \"tag\")\n",
    "            \n",
    "            quote_data = {\n",
    "                'text': text_element.text,\n",
    "                'author': author_element.text,\n",
    "                'tags': [tag.text for tag in tags_elements]\n",
    "            }\n",
    "            \n",
    "            scraped_quotes.append(quote_data)\n",
    "            \n",
    "            print(f\"\\nQuote {i}:\")\n",
    "            print(f\"  Text: {quote_data['text']}\")\n",
    "            print(f\"  Author: {quote_data['author']}\")\n",
    "            print(f\"  Tags: {', '.join(quote_data['tags'])}\")\n",
    "        \n",
    "        # Try clicking \"Next\" button if exists\n",
    "        try:\n",
    "            next_button = driver.find_element(By.PARTIAL_LINK_TEXT, \"Next\")\n",
    "            if next_button:\n",
    "                print(\"\\nüîÑ 'Next' button found (not clicking in this example)\")\n",
    "        except:\n",
    "            print(\"\\n‚ùå No 'Next' button found\")\n",
    "        \n",
    "        return scraped_quotes\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "        return []\n",
    "    \n",
    "    finally:\n",
    "        # Always close the driver\n",
    "        if 'driver' in locals():\n",
    "            driver.quit()\n",
    "            print(\"\\nüîí WebDriver closed\")\n",
    "\n",
    "# Note: This example might not work in all environments due to browser setup\n",
    "# In Colab/Jupyter, you might need additional setup for Chrome/ChromeDriver\n",
    "print(\"üö® Note: This Selenium example requires proper Chrome/ChromeDriver setup\")\n",
    "print(\"üí° In production environments, make sure you have the necessary dependencies installed\")\n",
    "\n",
    "# Uncomment the line below to run the example (if Chrome is available)\n",
    "# selenium_results = selenium_scraping_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Parallel Web Scraping & Multiprocessing\n",
    "\n",
    "When scraping large amounts of data, performance becomes crucial. Python's multiprocessing and libraries like `joblib` allow us to speed up scraping by processing multiple URLs simultaneously.\n",
    "\n",
    "## üß† Why Use Parallel Processing?\n",
    "\n",
    "**Sequential Processing:**\n",
    "- Scrapes one URL at a time\n",
    "- Total time = (number of URLs) √ó (average time per URL)\n",
    "- CPU cores remain underutilized\n",
    "\n",
    "**Parallel Processing:**\n",
    "- Scrapes multiple URLs simultaneously\n",
    "- Total time ‚âà (number of URLs) √∑ (number of workers) √ó (average time per URL)\n",
    "- Better resource utilization\n",
    "\n",
    "‚ö†Ô∏è **Important**: Always respect websites' rate limits and robots.txt when using parallel processing!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Basic Multiprocessing Concepts\n",
    "\n",
    "Before applying multiprocessing to web scraping, let's understand the basics with simple examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import multiprocessing as mp\n",
    "from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor\n",
    "import requests\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# Example 1: CPU-intensive task (Sequential vs Parallel)\n",
    "def square_number(n):\n",
    "    \"\"\"Simulate CPU-intensive work\"\"\"\n",
    "    time.sleep(0.1)  # Simulate computation time\n",
    "    return n ** 2\n",
    "\n",
    "def demonstrate_multiprocessing():\n",
    "    numbers = list(range(1, 21))  # 1 to 20\n",
    "    \n",
    "    # Sequential processing\n",
    "    print(\"üêå Sequential Processing:\")\n",
    "    start_time = time.time()\n",
    "    sequential_results = [square_number(n) for n in numbers]\n",
    "    sequential_time = time.time() - start_time\n",
    "    print(f\"   Time taken: {sequential_time:.2f} seconds\")\n",
    "    print(f\"   Results: {sequential_results[:5]}... (showing first 5)\")\n",
    "    \n",
    "    # Parallel processing with multiprocessing\n",
    "    print(\"\\n‚ö° Parallel Processing (multiprocessing):\")\n",
    "    start_time = time.time()\n",
    "    with ProcessPoolExecutor(max_workers=4) as executor:\n",
    "        parallel_results = list(executor.map(square_number, numbers))\n",
    "    parallel_time = time.time() - start_time\n",
    "    print(f\"   Time taken: {parallel_time:.2f} seconds\")\n",
    "    print(f\"   Results: {parallel_results[:5]}... (showing first 5)\")\n",
    "    print(f\"   Speedup: {sequential_time/parallel_time:.2f}x faster\")\n",
    "\n",
    "# Run the demonstration\n",
    "demonstrate_multiprocessing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Introduction to Joblib\n",
    "\n",
    "`joblib` is a powerful library that makes parallel computing easy and efficient. It's particularly great for:\n",
    "- CPU-bound tasks\n",
    "- Machine learning workloads\n",
    "- Data processing pipelines\n",
    "\n",
    "**Key advantages:**\n",
    "- Simple API: `Parallel(n_jobs=-1)(delayed(function)(args) for args in data)`\n",
    "- Automatic memory optimization\n",
    "- Built-in progress tracking\n",
    "- Works well with NumPy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install joblib if not already installed\n",
    "# !pip install joblib\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "import numpy as np\n",
    "\n",
    "def process_data(x):\n",
    "    \"\"\"Simulate data processing\"\"\"\n",
    "    time.sleep(0.05)\n",
    "    return x ** 3 + 2 * x ** 2 + x + 1\n",
    "\n",
    "def demonstrate_joblib():\n",
    "    data = list(range(1, 51))  # 1 to 50\n",
    "    \n",
    "    print(\"üîß Joblib Examples:\")\n",
    "    \n",
    "    # Sequential processing\n",
    "    print(\"\\nüêå Sequential Processing:\")\n",
    "    start_time = time.time()\n",
    "    sequential_results = [process_data(x) for x in data]\n",
    "    sequential_time = time.time() - start_time\n",
    "    print(f\"   Time taken: {sequential_time:.2f} seconds\")\n",
    "    \n",
    "    # Parallel processing with joblib (all CPU cores)\n",
    "    print(\"\\n‚ö° Joblib Parallel (all cores):\")\n",
    "    start_time = time.time()\n",
    "    parallel_results = Parallel(n_jobs=-1)(delayed(process_data)(x) for x in data)\n",
    "    parallel_time = time.time() - start_time\n",
    "    print(f\"   Time taken: {parallel_time:.2f} seconds\")\n",
    "    print(f\"   Speedup: {sequential_time/parallel_time:.2f}x faster\")\n",
    "    \n",
    "    # Parallel processing with specific number of workers\n",
    "    print(\"\\n‚ö° Joblib Parallel (4 workers):\")\n",
    "    start_time = time.time()\n",
    "    parallel_results_4 = Parallel(n_jobs=4)(delayed(process_data)(x) for x in data)\n",
    "    parallel_time_4 = time.time() - start_time\n",
    "    print(f\"   Time taken: {parallel_time_4:.2f} seconds\")\n",
    "    \n",
    "    # With verbose progress tracking\n",
    "    print(\"\\nüìä Joblib with Progress Tracking:\")\n",
    "    start_time = time.time()\n",
    "    parallel_results_verbose = Parallel(n_jobs=4, verbose=1)(\n",
    "        delayed(process_data)(x) for x in data\n",
    "    )\n",
    "    verbose_time = time.time() - start_time\n",
    "    print(f\"   Time taken: {verbose_time:.2f} seconds\")\n",
    "    \n",
    "    # Verify results are the same\n",
    "    print(f\"\\n‚úÖ Results match: {sequential_results == parallel_results}\")\n",
    "\n",
    "# Run joblib demonstration\n",
    "demonstrate_joblib()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üåê Parallel Web Scraping Examples\n",
    "\n",
    "Now let's apply these concepts to web scraping. We'll compare sequential vs parallel approaches for scraping multiple URLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "import time\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "def scrape_single_url(url, timeout=10):\n",
    "    \"\"\"Scrape a single URL and extract basic information\"\"\"\n",
    "    try:\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        }\n",
    "        \n",
    "        response = requests.get(url, headers=headers, timeout=timeout)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Extract basic information\n",
    "        title = soup.find('title')\n",
    "        title_text = title.get_text(strip=True) if title else \"No title\"\n",
    "        \n",
    "        # Count paragraphs\n",
    "        paragraphs = soup.find_all('p')\n",
    "        paragraph_count = len(paragraphs)\n",
    "        \n",
    "        # Count links\n",
    "        links = soup.find_all('a', href=True)\n",
    "        link_count = len(links)\n",
    "        \n",
    "        # Get first paragraph text (if available)\n",
    "        first_paragraph = \"\"\n",
    "        if paragraphs:\n",
    "            first_paragraph = paragraphs[0].get_text(strip=True)[:200] + \"...\"\n",
    "        \n",
    "        return {\n",
    "            'url': url,\n",
    "            'title': title_text[:100],  # Limit title length\n",
    "            'status': 'success',\n",
    "            'paragraph_count': paragraph_count,\n",
    "            'link_count': link_count,\n",
    "            'first_paragraph': first_paragraph,\n",
    "            'response_time': response.elapsed.total_seconds()\n",
    "        }\n",
    "        \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return {\n",
    "            'url': url,\n",
    "            'title': None,\n",
    "            'status': 'error',\n",
    "            'error': str(e),\n",
    "            'paragraph_count': 0,\n",
    "            'link_count': 0,\n",
    "            'first_paragraph': '',\n",
    "            'response_time': None\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'url': url,\n",
    "            'title': None,\n",
    "            'status': 'error',\n",
    "            'error': f\"Parsing error: {str(e)}\",\n",
    "            'paragraph_count': 0,\n",
    "            'link_count': 0,\n",
    "            'first_paragraph': '',\n",
    "            'response_time': None\n",
    "        }\n",
    "\n",
    "def scrape_urls_sequential(urls):\n",
    "    \"\"\"Scrape URLs one by one (sequential)\"\"\"\n",
    "    print(\"üêå Sequential scraping...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    results = []\n",
    "    for i, url in enumerate(urls, 1):\n",
    "        print(f\"   Scraping {i}/{len(urls)}: {url[:50]}...\")\n",
    "        result = scrape_single_url(url)\n",
    "        results.append(result)\n",
    "        time.sleep(1)  # Be respectful - add delay\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"   Sequential time: {total_time:.2f} seconds\")\n",
    "    return results, total_time\n",
    "\n",
    "def scrape_urls_parallel_joblib(urls, n_jobs=4):\n",
    "    \"\"\"Scrape URLs in parallel using joblib\"\"\"\n",
    "    print(f\"‚ö° Parallel scraping with joblib ({n_jobs} workers)...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Add delays in parallel execution too (but spread out)\n",
    "    def scrape_with_delay(url, delay_factor):\n",
    "        time.sleep(delay_factor * 0.5)  # Staggered delays\n",
    "        return scrape_single_url(url)\n",
    "    \n",
    "    # Create delay factors for staggered requests\n",
    "    delay_factors = [i % 4 for i in range(len(urls))]\n",
    "    \n",
    "    results = Parallel(n_jobs=n_jobs, verbose=1)(\n",
    "        delayed(scrape_with_delay)(url, delay) \n",
    "        for url, delay in zip(urls, delay_factors)\n",
    "    )\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"   Parallel time: {total_time:.2f} seconds\")\n",
    "    return results, total_time\n",
    "\n",
    "# Test URLs (using public APIs and websites that allow scraping)\n",
    "test_urls = [\n",
    "    'https://httpbin.org/html',\n",
    "    'https://httpbin.org/json',\n",
    "    'https://jsonplaceholder.typicode.com/posts/1',\n",
    "    'https://jsonplaceholder.typicode.com/posts/2',\n",
    "    'https://httpbin.org/xml',\n",
    "    'https://httpbin.org/robots.txt',\n",
    "    'https://jsonplaceholder.typicode.com/users/1',\n",
    "    'https://jsonplaceholder.typicode.com/users/2'\n",
    "]\n",
    "\n",
    "print(\"üåê Web Scraping Performance Comparison\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Sequential scraping\n",
    "sequential_results, seq_time = scrape_urls_sequential(test_urls)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "\n",
    "# Parallel scraping\n",
    "parallel_results, par_time = scrape_urls_parallel_joblib(test_urls, n_jobs=4)\n",
    "\n",
    "# Compare results\n",
    "print(f\"\\nüìä Performance Summary:\")\n",
    "print(f\"   URLs scraped: {len(test_urls)}\")\n",
    "print(f\"   Sequential time: {seq_time:.2f} seconds\")\n",
    "print(f\"   Parallel time: {par_time:.2f} seconds\")\n",
    "print(f\"   Speedup: {seq_time/par_time:.2f}x faster\")\n",
    "\n",
    "# Show success rates\n",
    "seq_success = sum(1 for r in sequential_results if r['status'] == 'success')\n",
    "par_success = sum(1 for r in parallel_results if r['status'] == 'success')\n",
    "\n",
    "print(f\"\\n‚úÖ Success Rates:\")\n",
    "print(f\"   Sequential: {seq_success}/{len(test_urls)} ({seq_success/len(test_urls)*100:.1f}%)\")\n",
    "print(f\"   Parallel: {par_success}/{len(test_urls)} ({par_success/len(test_urls)*100:.1f}%)\")\n",
    "\n",
    "# Show sample results\n",
    "print(f\"\\nüìÑ Sample Results (first 3):\")\n",
    "for i, result in enumerate(parallel_results[:3]):\n",
    "    print(f\"   {i+1}. {result['url']}\")\n",
    "    print(f\"      Title: {result['title']}\")\n",
    "    print(f\"      Status: {result['status']}\")\n",
    "    if result['status'] == 'success':\n",
    "        print(f\"      Paragraphs: {result['paragraph_count']}, Links: {result['link_count']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ°Ô∏è Advanced Parallel Scraping with Rate Limiting\n",
    "\n",
    "When scraping real websites, we need to be more careful about rate limiting, error handling, and respecting server resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from threading import Lock\n",
    "import threading\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "class RateLimitedScraper:\n",
    "    \"\"\"A rate-limited web scraper with parallel processing capabilities\"\"\"\n",
    "    \n",
    "    def __init__(self, requests_per_second=2, max_retries=3):\n",
    "        self.requests_per_second = requests_per_second\n",
    "        self.max_retries = max_retries\n",
    "        self.last_request_time = {}\n",
    "        self.lock = Lock()\n",
    "        \n",
    "    def wait_if_needed(self, domain):\n",
    "        \"\"\"Implement rate limiting per domain\"\"\"\n",
    "        with self.lock:\n",
    "            now = datetime.now()\n",
    "            if domain in self.last_request_time:\n",
    "                time_since_last = (now - self.last_request_time[domain]).total_seconds()\n",
    "                min_interval = 1.0 / self.requests_per_second\n",
    "                \n",
    "                if time_since_last < min_interval:\n",
    "                    sleep_time = min_interval - time_since_last\n",
    "                    time.sleep(sleep_time)\n",
    "            \n",
    "            self.last_request_time[domain] = datetime.now()\n",
    "    \n",
    "    def extract_domain(self, url):\n",
    "        \"\"\"Extract domain from URL\"\"\"\n",
    "        from urllib.parse import urlparse\n",
    "        return urlparse(url).netloc\n",
    "    \n",
    "    def scrape_with_retries(self, url):\n",
    "        \"\"\"Scrape URL with retry logic and rate limiting\"\"\"\n",
    "        domain = self.extract_domain(url)\n",
    "        \n",
    "        for attempt in range(self.max_retries):\n",
    "            try:\n",
    "                # Implement rate limiting\n",
    "                self.wait_if_needed(domain)\n",
    "                \n",
    "                headers = {\n",
    "                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "                }\n",
    "                \n",
    "                response = requests.get(url, headers=headers, timeout=15)\n",
    "                response.raise_for_status()\n",
    "                \n",
    "                soup = BeautifulSoup(response.content, 'html.parser')\n",
    "                \n",
    "                # Extract comprehensive data\n",
    "                result = {\n",
    "                    'url': url,\n",
    "                    'status': 'success',\n",
    "                    'attempt': attempt + 1,\n",
    "                    'timestamp': datetime.now().isoformat(),\n",
    "                    'response_code': response.status_code,\n",
    "                    'content_length': len(response.content),\n",
    "                    'title': '',\n",
    "                    'meta_description': '',\n",
    "                    'headings': {},\n",
    "                    'link_count': 0,\n",
    "                    'image_count': 0,\n",
    "                    'form_count': 0,\n",
    "                    'text_content_length': 0\n",
    "                }\n",
    "                \n",
    "                # Extract title\n",
    "                title_tag = soup.find('title')\n",
    "                if title_tag:\n",
    "                    result['title'] = title_tag.get_text(strip=True)\n",
    "                \n",
    "                # Extract meta description\n",
    "                meta_desc = soup.find('meta', attrs={'name': 'description'})\n",
    "                if meta_desc:\n",
    "                    result['meta_description'] = meta_desc.get('content', '')\n",
    "                \n",
    "                # Count different elements\n",
    "                result['link_count'] = len(soup.find_all('a', href=True))\n",
    "                result['image_count'] = len(soup.find_all('img'))\n",
    "                result['form_count'] = len(soup.find_all('form'))\n",
    "                \n",
    "                # Count headings\n",
    "                for i in range(1, 7):\n",
    "                    headings = soup.find_all(f'h{i}')\n",
    "                    if headings:\n",
    "                        result['headings'][f'h{i}'] = len(headings)\n",
    "                \n",
    "                # Get text content length\n",
    "                text_content = soup.get_text(strip=True)\n",
    "                result['text_content_length'] = len(text_content)\n",
    "                \n",
    "                return result\n",
    "                \n",
    "            except requests.exceptions.RequestException as e:\n",
    "                if attempt == self.max_retries - 1:  # Last attempt\n",
    "                    return {\n",
    "                        'url': url,\n",
    "                        'status': 'error',\n",
    "                        'error': str(e),\n",
    "                        'attempt': attempt + 1,\n",
    "                        'timestamp': datetime.now().isoformat()\n",
    "                    }\n",
    "                else:\n",
    "                    # Wait before retry (exponential backoff)\n",
    "                    wait_time = (2 ** attempt) + random.uniform(0, 1)\n",
    "                    time.sleep(wait_time)\n",
    "            \n",
    "            except Exception as e:\n",
    "                return {\n",
    "                    'url': url,\n",
    "                    'status': 'error',\n",
    "                    'error': f\"Unexpected error: {str(e)}\",\n",
    "                    'attempt': attempt + 1,\n",
    "                    'timestamp': datetime.now().isoformat()\n",
    "                }\n",
    "\n",
    "def parallel_scrape_with_rate_limiting(urls, n_jobs=3, requests_per_second=2):\n",
    "    \"\"\"Scrape URLs in parallel with rate limiting\"\"\"\n",
    "    scraper = RateLimitedScraper(requests_per_second=requests_per_second)\n",
    "    \n",
    "    print(f\"üöÄ Advanced Parallel Scraping:\")\n",
    "    print(f\"   URLs: {len(urls)}\")\n",
    "    print(f\"   Workers: {n_jobs}\")\n",
    "    print(f\"   Rate limit: {requests_per_second} requests/second per domain\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    results = Parallel(n_jobs=n_jobs, verbose=1)(\n",
    "        delayed(scraper.scrape_with_retries)(url) for url in urls\n",
    "    )\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    \n",
    "    # Analyze results\n",
    "    successful = [r for r in results if r['status'] == 'success']\n",
    "    failed = [r for r in results if r['status'] == 'error']\n",
    "    \n",
    "    print(f\"\\nüìä Scraping Summary:\")\n",
    "    print(f\"   Total time: {total_time:.2f} seconds\")\n",
    "    print(f\"   Average time per URL: {total_time/len(urls):.2f} seconds\")\n",
    "    print(f\"   Successful: {len(successful)}/{len(urls)} ({len(successful)/len(urls)*100:.1f}%)\")\n",
    "    print(f\"   Failed: {len(failed)}/{len(urls)} ({len(failed)/len(urls)*100:.1f}%)\")\n",
    "    \n",
    "    if successful:\n",
    "        avg_content_length = sum(r['content_length'] for r in successful) / len(successful)\n",
    "        total_links = sum(r['link_count'] for r in successful)\n",
    "        total_images = sum(r['image_count'] for r in successful)\n",
    "        \n",
    "        print(f\"\\nüìÑ Content Analysis:\")\n",
    "        print(f\"   Average content length: {avg_content_length:.0f} bytes\")\n",
    "        print(f\"   Total links found: {total_links}\")\n",
    "        print(f\"   Total images found: {total_images}\")\n",
    "    \n",
    "    if failed:\n",
    "        print(f\"\\n‚ùå Failed URLs:\")\n",
    "        for fail in failed[:3]:  # Show first 3 failures\n",
    "            print(f\"   {fail['url']}: {fail.get('error', 'Unknown error')}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example with mixed domains (rate limiting will be applied per domain)\n",
    "mixed_urls = [\n",
    "    'https://httpbin.org/html',\n",
    "    'https://httpbin.org/json',\n",
    "    'https://httpbin.org/xml',\n",
    "    'https://jsonplaceholder.typicode.com/posts/1',\n",
    "    'https://jsonplaceholder.typicode.com/posts/2',\n",
    "    'https://jsonplaceholder.typicode.com/users/1',\n",
    "    'https://httpbin.org/robots.txt',\n",
    "    'https://httpbin.org/user-agent',\n",
    "    'https://jsonplaceholder.typicode.com/comments/1',\n",
    "    'https://httpbin.org/headers'\n",
    "]\n",
    "\n",
    "# Run advanced parallel scraping\n",
    "results = parallel_scrape_with_rate_limiting(\n",
    "    mixed_urls, \n",
    "    n_jobs=3, \n",
    "    requests_per_second=2\n",
    ")\n",
    "\n",
    "# Show detailed results for successful scrapes\n",
    "print(f\"\\nüìã Detailed Results (first 3 successful):\")\n",
    "successful_results = [r for r in results if r['status'] == 'success']\n",
    "for i, result in enumerate(successful_results[:3]):\n",
    "    print(f\"\\n{i+1}. {result['url']}\")\n",
    "    print(f\"   Title: {result['title'][:60]}...\")\n",
    "    print(f\"   Response Code: {result['response_code']}\")\n",
    "    print(f\"   Content Length: {result['content_length']:,} bytes\")\n",
    "    print(f\"   Links: {result['link_count']}, Images: {result['image_count']}\")\n",
    "    if result['headings']:\n",
    "        print(f\"   Headings: {result['headings']}\")\n",
    "    print(f\"   Attempt: {result['attempt']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõí Real-World Example: Parallel E-commerce Data Scraping\n",
    "\n",
    "Let's create a practical example that simulates scraping product data from multiple pages, using parallel processing to handle large datasets efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "class EcommerceScraper:\n",
    "    \"\"\"Simulate e-commerce product scraping with parallel processing\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "        })\n",
    "    \n",
    "    def simulate_product_page(self, product_id):\n",
    "        \"\"\"Simulate scraping a product page\"\"\"\n",
    "        # In real scraping, this would fetch from actual URLs\n",
    "        # For demo purposes, we'll simulate data\n",
    "        \n",
    "        time.sleep(random.uniform(0.5, 2.0))  # Simulate network delay\n",
    "        \n",
    "        # Simulate occasional failures\n",
    "        if random.random() < 0.1:  # 10% failure rate\n",
    "            raise requests.exceptions.RequestException(f\"Failed to load product {product_id}\")\n",
    "        \n",
    "        # Generate simulated product data\n",
    "        categories = ['Electronics', 'Clothing', 'Books', 'Home & Garden', 'Sports']\n",
    "        brands = ['BrandA', 'BrandB', 'BrandC', 'BrandD', 'BrandE']\n",
    "        \n",
    "        product = {\n",
    "            'product_id': product_id,\n",
    "            'name': f'Product {product_id}',\n",
    "            'price': round(random.uniform(10, 500), 2),\n",
    "            'category': random.choice(categories),\n",
    "            'brand': random.choice(brands),\n",
    "            'rating': round(random.uniform(1, 5), 1),\n",
    "            'review_count': random.randint(0, 1000),\n",
    "            'in_stock': random.choice([True, False]),\n",
    "            'description_length': random.randint(100, 1000),\n",
    "            'image_count': random.randint(1, 10),\n",
    "            'scrape_timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        return product\n",
    "    \n",
    "    def scrape_product_batch(self, product_ids):\n",
    "        \"\"\"Scrape a batch of product IDs\"\"\"\n",
    "        results = []\n",
    "        batch_start = time.time()\n",
    "        \n",
    "        for product_id in product_ids:\n",
    "            try:\n",
    "                product = self.simulate_product_page(product_id)\n",
    "                product['status'] = 'success'\n",
    "                results.append(product)\n",
    "            except Exception as e:\n",
    "                results.append({\n",
    "                    'product_id': product_id,\n",
    "                    'status': 'error',\n",
    "                    'error': str(e),\n",
    "                    'scrape_timestamp': datetime.now().isoformat()\n",
    "                })\n",
    "        \n",
    "        batch_time = time.time() - batch_start\n",
    "        return results, batch_time\n",
    "\n",
    "def parallel_ecommerce_scraping(product_ids, batch_size=50, n_jobs=4):\n",
    "    \"\"\"Scrape e-commerce products in parallel batches\"\"\"\n",
    "    \n",
    "    # Split product IDs into batches\n",
    "    batches = [product_ids[i:i + batch_size] for i in range(0, len(product_ids), batch_size)]\n",
    "    \n",
    "    print(f\"üõí E-commerce Parallel Scraping:\")\n",
    "    print(f\"   Total products: {len(product_ids)}\")\n",
    "    print(f\"   Batch size: {batch_size}\")\n",
    "    print(f\"   Number of batches: {len(batches)}\")\n",
    "    print(f\"   Parallel workers: {n_jobs}\")\n",
    "    \n",
    "    scraper = EcommerceScraper()\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Process batches in parallel\n",
    "    batch_results = Parallel(n_jobs=n_jobs, verbose=1)(\n",
    "        delayed(scraper.scrape_product_batch)(batch) for batch in batches\n",
    "    )\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    \n",
    "    # Flatten results\n",
    "    all_products = []\n",
    "    total_batch_time = 0\n",
    "    \n",
    "    for results, batch_time in batch_results:\n",
    "        all_products.extend(results)\n",
    "        total_batch_time += batch_time\n",
    "    \n",
    "    # Analyze results\n",
    "    successful_products = [p for p in all_products if p['status'] == 'success']\n",
    "    failed_products = [p for p in all_products if p['status'] == 'error']\n",
    "    \n",
    "    print(f\"\\nüìä Scraping Results:\")\n",
    "    print(f\"   Total time: {total_time:.2f} seconds\")\n",
    "    print(f\"   Products/second: {len(product_ids)/total_time:.2f}\")\n",
    "    print(f\"   Successful: {len(successful_products)}/{len(product_ids)} ({len(successful_products)/len(product_ids)*100:.1f}%)\")\n",
    "    print(f\"   Failed: {len(failed_products)}/{len(product_ids)} ({len(failed_products)/len(product_ids)*100:.1f}%)\")\n",
    "    \n",
    "    return successful_products, failed_products\n",
    "\n",
    "def analyze_scraped_products(products):\n",
    "    \"\"\"Analyze the scraped product data\"\"\"\n",
    "    if not products:\n",
    "        print(\"‚ùå No products to analyze\")\n",
    "        return\n",
    "    \n",
    "    df = pd.DataFrame(products)\n",
    "    \n",
    "    print(f\"\\nüìà Product Data Analysis:\")\n",
    "    print(f\"   Dataset shape: {df.shape}\")\n",
    "    \n",
    "    # Price analysis\n",
    "    if 'price' in df.columns:\n",
    "        print(f\"\\nüí∞ Price Statistics:\")\n",
    "        print(f\"   Average price: ${df['price'].mean():.2f}\")\n",
    "        print(f\"   Median price: ${df['price'].median():.2f}\")\n",
    "        print(f\"   Price range: ${df['price'].min():.2f} - ${df['price'].max():.2f}\")\n",
    "        \n",
    "    # Category distribution\n",
    "    if 'category' in df.columns:\n",
    "        print(f\"\\nüìÇ Category Distribution:\")\n",
    "        category_counts = df['category'].value_counts()\n",
    "        for category, count in category_counts.items():\n",
    "            print(f\"   {category}: {count} products ({count/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    # Brand analysis\n",
    "    if 'brand' in df.columns:\n",
    "        print(f\"\\nüè∑Ô∏è Top Brands:\")\n",
    "        brand_counts = df['brand'].value_counts().head(5)\n",
    "        for brand, count in brand_counts.items():\n",
    "            print(f\"   {brand}: {count} products\")\n",
    "    \n",
    "    # Stock status\n",
    "    if 'in_stock' in df.columns:\n",
    "        in_stock_count = df['in_stock'].sum()\n",
    "        print(f\"\\nüì¶ Stock Status:\")\n",
    "        print(f\"   In stock: {in_stock_count}/{len(df)} ({in_stock_count/len(df)*100:.1f}%)\")\n",
    "        print(f\"   Out of stock: {len(df)-in_stock_count}/{len(df)} ({(len(df)-in_stock_count)/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    # Rating analysis\n",
    "    if 'rating' in df.columns:\n",
    "        print(f\"\\n‚≠ê Rating Statistics:\")\n",
    "        print(f\"   Average rating: {df['rating'].mean():.2f}/5.0\")\n",
    "        print(f\"   Ratings >= 4.0: {(df['rating'] >= 4.0).sum()}/{len(df)} ({(df['rating'] >= 4.0).sum()/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Generate sample product IDs (simulating large dataset)\n",
    "product_ids = [f\"PROD_{i:06d}\" for i in range(1, 501)]  # 500 products\n",
    "\n",
    "print(\"üöÄ Starting E-commerce Parallel Scraping Demo...\")\n",
    "\n",
    "# Run parallel scraping\n",
    "successful_products, failed_products = parallel_ecommerce_scraping(\n",
    "    product_ids, \n",
    "    batch_size=50, \n",
    "    n_jobs=4\n",
    ")\n",
    "\n",
    "# Analyze the results\n",
    "df_products = analyze_scraped_products(successful_products)\n",
    "\n",
    "# Save results\n",
    "if successful_products:\n",
    "    # Save to JSON\n",
    "    output_file = 'scraped_products.json'\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(successful_products, f, indent=2)\n",
    "    \n",
    "    # Save to CSV\n",
    "    csv_file = 'scraped_products.csv'\n",
    "    df_products.to_csv(csv_file, index=False)\n",
    "    \n",
    "    print(f\"\\nüíæ Data saved:\")\n",
    "    print(f\"   JSON: {output_file}\")\n",
    "    print(f\"   CSV: {csv_file}\")\n",
    "\n",
    "# Show sample products\n",
    "if successful_products:\n",
    "    print(f\"\\nüõçÔ∏è Sample Products:\")\n",
    "    for i, product in enumerate(successful_products[:3]):\n",
    "        print(f\"\\n{i+1}. {product['name']} (ID: {product['product_id']})\")\n",
    "        print(f\"   Price: ${product['price']}\")\n",
    "        print(f\"   Category: {product['category']}\")\n",
    "        print(f\"   Brand: {product['brand']}\")\n",
    "        print(f\"   Rating: {product['rating']}/5.0 ({product['review_count']} reviews)\")\n",
    "        print(f\"   In Stock: {'‚úÖ' if product['in_stock'] else '‚ùå'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.ysu.am/robots.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ö° Performance Optimization & Best Practices\n",
    "\n",
    "### üéØ Choosing the Right Approach\n",
    "\n",
    "| Method | Best For | Pros | Cons |\n",
    "|--------|----------|------|------|\n",
    "| **Sequential** | Small datasets, strict rate limits | Simple, predictable | Slow for large datasets |\n",
    "| **Threading** | I/O-bound tasks, many small requests | Good for network-bound tasks | GIL limitations in Python |\n",
    "| **Multiprocessing** | CPU-intensive parsing | True parallelism | Higher memory usage |\n",
    "| **Joblib** | Balanced approach, data science tasks | Easy to use, optimized | Extra dependency |\n",
    "\n",
    "### üõ°Ô∏è Rate Limiting Strategies\n",
    "\n",
    "```python\n",
    "# 1. Fixed delay between requests\n",
    "time.sleep(1)\n",
    "\n",
    "# 2. Random delay (more human-like)\n",
    "time.sleep(random.uniform(0.5, 2.0))\n",
    "\n",
    "# 3. Exponential backoff on errors\n",
    "wait_time = (2 ** attempt) + random.uniform(0, 1)\n",
    "\n",
    "# 4. Domain-specific rate limiting\n",
    "# Different limits for different websites\n",
    "```\n",
    "\n",
    "### üìä Monitoring & Logging\n",
    "\n",
    "```python\n",
    "# Track success rates, response times, errors\n",
    "# Use logging instead of print for production\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "logger.info(f\"Scraped {url} successfully\")\n",
    "logger.error(f\"Failed to scrape {url}: {error}\")\n",
    "```\n",
    "\n",
    "### üîß Performance Tips\n",
    "\n",
    "1. **Use connection pooling** with `requests.Session()`\n",
    "2. **Implement caching** to avoid re-scraping\n",
    "3. **Batch processing** for large datasets\n",
    "4. **Memory management** - process in chunks\n",
    "5. **Error handling** - implement retries and fallbacks\n",
    "6. **Respect robots.txt** and rate limits\n",
    "7. **Use appropriate timeouts**\n",
    "8. **Monitor resource usage** (CPU, memory, network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import threading\n",
    "from collections import defaultdict\n",
    "\n",
    "def compare_parallel_approaches(urls, max_workers=4):\n",
    "    \"\"\"Compare different parallel processing approaches\"\"\"\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # 1. Sequential baseline\n",
    "    print(\"üêå Sequential Processing:\")\n",
    "    start_time = time.time()\n",
    "    sequential_results = [scrape_single_url(url) for url in urls]\n",
    "    sequential_time = time.time() - start_time\n",
    "    results['Sequential'] = {\n",
    "        'time': sequential_time,\n",
    "        'results': sequential_results\n",
    "    }\n",
    "    print(f\"   Time: {sequential_time:.2f}s\")\n",
    "    \n",
    "    # 2. ThreadPoolExecutor\n",
    "    print(\"\\nüßµ ThreadPoolExecutor:\")\n",
    "    start_time = time.time()\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        thread_results = list(executor.map(scrape_single_url, urls))\n",
    "    thread_time = time.time() - start_time\n",
    "    results['ThreadPool'] = {\n",
    "        'time': thread_time,\n",
    "        'results': thread_results\n",
    "    }\n",
    "    print(f\"   Time: {thread_time:.2f}s\")\n",
    "    print(f\"   Speedup: {sequential_time/thread_time:.2f}x\")\n",
    "    \n",
    "    # 3. ProcessPoolExecutor\n",
    "    print(\"\\n‚öôÔ∏è ProcessPoolExecutor:\")\n",
    "    start_time = time.time()\n",
    "    with concurrent.futures.ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
    "        process_results = list(executor.map(scrape_single_url, urls))\n",
    "    process_time = time.time() - start_time\n",
    "    results['ProcessPool'] = {\n",
    "        'time': process_time,\n",
    "        'results': process_results\n",
    "    }\n",
    "    print(f\"   Time: {process_time:.2f}s\")\n",
    "    print(f\"   Speedup: {sequential_time/process_time:.2f}x\")\n",
    "    \n",
    "    # 4. Joblib\n",
    "    print(\"\\nüì¶ Joblib Parallel:\")\n",
    "    start_time = time.time()\n",
    "    joblib_results = Parallel(n_jobs=max_workers)(\n",
    "        delayed(scrape_single_url)(url) for url in urls\n",
    "    )\n",
    "    joblib_time = time.time() - start_time\n",
    "    results['Joblib'] = {\n",
    "        'time': joblib_time,\n",
    "        'results': joblib_results\n",
    "    }\n",
    "    print(f\"   Time: {joblib_time:.2f}s\")\n",
    "    print(f\"   Speedup: {sequential_time/joblib_time:.2f}x\")\n",
    "    \n",
    "    # Summary comparison\n",
    "    print(f\"\\nüìä Performance Summary:\")\n",
    "    print(f\"{'Method':<15} {'Time (s)':<10} {'Speedup':<10} {'Success Rate'}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for method, data in results.items():\n",
    "        success_count = sum(1 for r in data['results'] if r['status'] == 'success')\n",
    "        success_rate = success_count / len(urls) * 100\n",
    "        speedup = sequential_time / data['time'] if data['time'] > 0 else 0\n",
    "        \n",
    "        print(f\"{method:<15} {data['time']:<10.2f} {speedup:<10.2f} {success_rate:.1f}%\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test with a smaller set for comparison\n",
    "test_urls_small = [\n",
    "    'https://httpbin.org/delay/1',  # 1 second delay\n",
    "    'https://httpbin.org/delay/1',\n",
    "    'https://httpbin.org/delay/1',\n",
    "    'https://httpbin.org/delay/1',\n",
    "    'https://httpbin.org/json',\n",
    "    'https://httpbin.org/html',\n",
    "    'https://httpbin.org/xml',\n",
    "    'https://httpbin.org/user-agent'\n",
    "]\n",
    "\n",
    "print(\"üî¨ Comparing Parallel Processing Approaches\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "comparison_results = compare_parallel_approaches(test_urls_small, max_workers=4)\n",
    "\n",
    "# Memory usage comparison (simplified)\n",
    "print(f\"\\nüíæ Memory Usage Notes:\")\n",
    "print(\"   Sequential: Low memory, single process\")\n",
    "print(\"   ThreadPool: Medium memory, shared memory space\")\n",
    "print(\"   ProcessPool: High memory, separate processes\")\n",
    "print(\"   Joblib: Optimized memory usage, especially for NumPy arrays\")\n",
    "\n",
    "print(f\"\\nüéØ Recommendations:\")\n",
    "print(\"   ‚Ä¢ Use ThreadPool for I/O-bound web scraping\")\n",
    "print(\"   ‚Ä¢ Use ProcessPool for CPU-intensive data processing\")\n",
    "print(\"   ‚Ä¢ Use Joblib for data science and ML workloads\")\n",
    "print(\"   ‚Ä¢ Always implement rate limiting and error handling\")\n",
    "print(\"   ‚Ä¢ Monitor resource usage in production\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Key Takeaways: Parallel Web Scraping\n",
    "\n",
    "### ‚úÖ What We Learned\n",
    "\n",
    "1. **Multiprocessing Basics**: Understanding CPU cores and parallel execution\n",
    "2. **Joblib Library**: Simple and efficient parallel processing with `Parallel()` and `delayed()`\n",
    "3. **Rate Limiting**: Implementing respectful scraping with proper delays\n",
    "4. **Error Handling**: Robust retry mechanisms and failure recovery\n",
    "5. **Performance Comparison**: Different approaches for different use cases\n",
    "6. **Real-world Application**: E-commerce data scraping with batch processing\n",
    "\n",
    "### üöÄ When to Use Parallel Scraping\n",
    "\n",
    "**‚úÖ Good candidates:**\n",
    "- Large datasets (100s-1000s of URLs)\n",
    "- I/O-bound operations (network requests)\n",
    "- Independent scraping tasks\n",
    "- Time-sensitive data collection\n",
    "\n",
    "**‚ùå Avoid when:**\n",
    "- Small datasets (< 50 URLs)\n",
    "- Strict rate limits (< 1 req/sec)\n",
    "- Complex interdependent scraping\n",
    "- Server explicitly prohibits parallel access\n",
    "\n",
    "### üìã Production Checklist\n",
    "\n",
    "- [ ] Implement proper rate limiting\n",
    "- [ ] Add comprehensive error handling\n",
    "- [ ] Monitor resource usage (CPU, memory, network)\n",
    "- [ ] Respect robots.txt and terms of service\n",
    "- [ ] Implement logging and monitoring\n",
    "- [ ] Test with small datasets first\n",
    "- [ ] Use appropriate number of workers\n",
    "- [ ] Handle failures gracefully\n",
    "\n",
    "### üîó Next Steps\n",
    "\n",
    "1. Practice with the provided examples\n",
    "2. Implement rate limiting in your projects\n",
    "3. Experiment with different worker counts\n",
    "4. Monitor performance and optimize\n",
    "5. Always prioritize ethical scraping practices\n",
    "\n",
    "Remember: **With great power comes great responsibility!** Use parallel scraping responsibly and always respect website terms of service."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üï∏Ô∏è Scrapy Framework\n",
    "\n",
    "Scrapy is a powerful, production-ready framework for large-scale scraping projects.\n",
    "\n",
    "### Scrapy Features:\n",
    "- Built-in support for handling requests, following links, exporting data\n",
    "- Automatic throttling and concurrent requests\n",
    "- Built-in support for handling cookies, sessions, HTTP authentication\n",
    "- Robust handling of common scraping challenges\n",
    "\n",
    "### Basic Scrapy Spider Example:\n",
    "\n",
    "```python\n",
    "import scrapy\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = 'quotes'\n",
    "    start_urls = ['http://quotes.toscrape.com/']\n",
    "    \n",
    "    def parse(self, response):\n",
    "        # Extract quotes\n",
    "        quotes = response.css('div.quote')\n",
    "        \n",
    "        for quote in quotes:\n",
    "            yield {\n",
    "                'text': quote.css('span.text::text').get(),\n",
    "                'author': quote.css('small.author::text').get(),\n",
    "                'tags': quote.css('div.tags a.tag::text').getall(),\n",
    "            }\n",
    "        \n",
    "        # Follow next page\n",
    "        next_page = response.css('li.next a::attr(href)').get()\n",
    "        if next_page is not None:\n",
    "            yield response.follow(next_page, self.parse)\n",
    "```\n",
    "\n",
    "### Running Scrapy:\n",
    "```bash\n",
    "# Create new Scrapy project\n",
    "scrapy startproject myproject\n",
    "\n",
    "# Run spider\n",
    "scrapy crawl quotes -o quotes.json\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ°Ô∏è Web Scraping Best Practices & Ethics\n",
    "\n",
    "### üìã Technical Best Practices:\n",
    "\n",
    "1. **Respect robots.txt**\n",
    "   ```python\n",
    "   # Check robots.txt before scraping\n",
    "   # Example: https://example.com/robots.txt\n",
    "   ```\n",
    "\n",
    "2. **Add delays between requests**\n",
    "   ```python\n",
    "   import time\n",
    "   time.sleep(1)  # Wait 1 second between requests\n",
    "   ```\n",
    "\n",
    "3. **Use proper headers**\n",
    "   ```python\n",
    "   headers = {\n",
    "       'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "   }\n",
    "   response = requests.get(url, headers=headers)\n",
    "   ```\n",
    "\n",
    "4. **Handle errors gracefully**\n",
    "   ```python\n",
    "   try:\n",
    "       response = requests.get(url, timeout=10)\n",
    "       response.raise_for_status()\n",
    "   except requests.RequestException as e:\n",
    "       print(f\"Error: {e}\")\n",
    "   ```\n",
    "\n",
    "5. **Implement retry logic**\n",
    "   ```python\n",
    "   from requests.adapters import HTTPAdapter\n",
    "   from urllib3.util.retry import Retry\n",
    "   \n",
    "   session = requests.Session()\n",
    "   retry_strategy = Retry(total=3, backoff_factor=1)\n",
    "   adapter = HTTPAdapter(max_retries=retry_strategy)\n",
    "   session.mount(\"http://\", adapter)\n",
    "   session.mount(\"https://\", adapter)\n",
    "   ```\n",
    "\n",
    "### ‚öñÔ∏è Legal and Ethical Considerations:\n",
    "\n",
    "1. **Check Terms of Service** - Always read the website's ToS\n",
    "2. **Respect Rate Limits** - Don't overwhelm servers\n",
    "3. **Data Privacy** - Be mindful of personal data\n",
    "4. **Copyright** - Respect intellectual property rights\n",
    "5. **Commercial Use** - Understand licensing for commercial projects\n",
    "\n",
    "### üö´ Common Challenges & Solutions:\n",
    "\n",
    "| Challenge | Solution |\n",
    "|-----------|----------|\n",
    "| **JavaScript content** | Use Selenium or requests-html |\n",
    "| **CAPTCHAs** | Slow down requests, use proxy rotation |\n",
    "| **IP blocking** | Use proxy servers, VPNs |\n",
    "| **Dynamic content** | Wait for elements, use WebDriverWait |\n",
    "| **Large datasets** | Implement pagination, use Scrapy |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Additional Resources & Documentation\n",
    "\n",
    "### üìñ Official Documentation\n",
    "\n",
    "#### Beautiful Soup\n",
    "- **[Beautiful Soup Documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)** - Complete official documentation\n",
    "- **[Beautiful Soup Quick Start](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#quick-start)** - Getting started guide\n",
    "- **[CSS Selectors Reference](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#css-selectors)** - CSS selector syntax\n",
    "\n",
    "#### Requests Library\n",
    "- **[Requests Documentation](https://requests.readthedocs.io/)** - HTTP library documentation\n",
    "- **[Requests Quickstart](https://requests.readthedocs.io/en/latest/user/quickstart/)** - Basic usage examples\n",
    "- **[Advanced Usage](https://requests.readthedocs.io/en/latest/user/advanced/)** - Sessions, cookies, SSL\n",
    "\n",
    "#### Selenium\n",
    "- **[Selenium Documentation](https://selenium-python.readthedocs.io/)** - Official Python bindings\n",
    "- **[WebDriver API](https://selenium-python.readthedocs.io/api.html)** - Complete API reference\n",
    "- **[Selenium Grid](https://selenium.dev/documentation/grid/)** - Distributed testing\n",
    "\n",
    "#### Scrapy Framework\n",
    "- **[Scrapy Documentation](https://docs.scrapy.org/)** - Complete framework guide\n",
    "- **[Scrapy Tutorial](https://docs.scrapy.org/en/latest/intro/tutorial.html)** - Step-by-step tutorial\n",
    "- **[Scrapy Best Practices](https://docs.scrapy.org/en/latest/topics/practices.html)** - Production tips\n",
    "\n",
    "### üé• YouTube Videos & Tutorials\n",
    "\n",
    "#### Beginner Friendly\n",
    "- **[Web Scraping with Python - Complete Course](https://www.youtube.com/watch?v=XVv6mJpFOb0)** - freeCodeCamp (3+ hours)\n",
    "- **[Beautiful Soup Tutorial](https://www.youtube.com/watch?v=87Gx3U0BDlo)** - Corey Schafer\n",
    "- **[Python Web Scraping Basics](https://www.youtube.com/watch?v=ng2o98k983k)** - Tech With Tim\n",
    "- **[Requests Library Tutorial](https://www.youtube.com/watch?v=tb8gHvYlCFs)** - Corey Schafer\n",
    "\n",
    "#### Advanced Topics\n",
    "- **[Selenium WebDriver with Python](https://www.youtube.com/watch?v=Xjv1sY630Uc)** - Programming with Mosh\n",
    "- **[Scrapy Framework Tutorial](https://www.youtube.com/watch?v=s4jtkzHhLzY)** - Traversy Media\n",
    "- **[Web Scraping JavaScript Sites](https://www.youtube.com/watch?v=MeBU-4Xs2RU)** - John Watson Rooney\n",
    "- **[Handling CAPTCHAs and Bot Detection](https://www.youtube.com/watch?v=HeYvNR1r6Js)** - Kalle Hallden\n",
    "\n",
    "#### Real-World Projects\n",
    "- **[Building a Price Monitor](https://www.youtube.com/watch?v=Bg9r_yLk7VY)** - Tech With Tim\n",
    "- **[News Aggregator Project](https://www.youtube.com/watch?v=R9Dc6cCLPCc)** - Sentdex\n",
    "- **[Social Media Scraper](https://www.youtube.com/watch?v=HiOtQMcI5wg)** - John Watson Rooney\n",
    "\n",
    "### üìù Excellent Articles & Blogs\n",
    "\n",
    "#### Getting Started\n",
    "- **[Real Python - Web Scraping Guide](https://realpython.com/python-web-scraping-practical-introduction/)** - Comprehensive guide\n",
    "- **[GeeksforGeeks - Web Scraping](https://www.geeksforgeeks.org/python-web-scraping-tutorial/)** - Step-by-step tutorial\n",
    "- **[Towards Data Science - Web Scraping 101](https://towardsdatascience.com/web-scraping-101-with-python-e40e233b7f5b)** - Medium article\n",
    "\n",
    "#### Advanced Techniques\n",
    "- **[Scraping JavaScript Heavy Sites](https://blog.apify.com/web-scraping-javascript-heavy-sites/)** - Apify Blog\n",
    "- **[Avoiding Bot Detection](https://www.scrapehero.com/how-to-prevent-getting-blacklisted-while-scraping/)** - ScrapeHero\n",
    "- **[Proxy Rotation Strategies](https://oxylabs.io/blog/rotating-proxies-python)** - Oxylabs Blog\n",
    "- **[Handling Dynamic Content](https://www.zenrows.com/blog/selenium-wait-for-page-to-load)** - ZenRows\n",
    "\n",
    "#### Legal & Ethical Aspects\n",
    "- **[Web Scraping Laws](https://blog.apify.com/is-web-scraping-legal/)** - Legal considerations\n",
    "- **[robots.txt Guide](https://developers.google.com/search/docs/crawling-indexing/robots/intro)** - Google Developers\n",
    "- **[Ethical Scraping Practices](https://www.scrapehero.com/web-scraping-ethics/)** - ScrapeHero\n",
    "\n",
    "### üõ†Ô∏è Tools & Browser Extensions\n",
    "\n",
    "#### Browser Developer Tools\n",
    "- **[Chrome DevTools](https://developer.chrome.com/docs/devtools/)** - Inspect elements, network requests\n",
    "- **[Firefox Developer Tools](https://firefox-source-docs.mozilla.org/devtools-user/)** - Alternative dev tools\n",
    "- **[Selector Gadget](https://selectorgadget.com/)** - Chrome extension for CSS selectors\n",
    "\n",
    "#### Testing & Debugging\n",
    "- **[Postman](https://www.postman.com/)** - API testing tool\n",
    "- **[httpbin.org](http://httpbin.org/)** - HTTP testing service\n",
    "- **[quotes.toscrape.com](http://quotes.toscrape.com/)** - Practice scraping site\n",
    "- **[scrape.world](https://scrape.world/)** - More practice sites\n",
    "\n",
    "#### Proxy & VPN Services\n",
    "- **[ProxyMesh](https://proxymesh.com/)** - Rotating proxy service\n",
    "- **[Bright Data](https://brightdata.com/)** - Professional proxy network\n",
    "- **[Oxylabs](https://oxylabs.io/)** - Residential proxies\n",
    "\n",
    "### üìä Data Processing & Storage\n",
    "\n",
    "#### Data Analysis\n",
    "- **[Pandas Documentation](https://pandas.pydata.org/docs/)** - Data manipulation\n",
    "- **[NumPy User Guide](https://numpy.org/doc/stable/user/)** - Numerical computing\n",
    "- **[Matplotlib Tutorials](https://matplotlib.org/stable/tutorials/index.html)** - Data visualization\n",
    "\n",
    "#### Database Storage\n",
    "- **[SQLite3 Tutorial](https://docs.python.org/3/library/sqlite3.html)** - Lightweight database\n",
    "- **[MongoDB with Python](https://pymongo.readthedocs.io/)** - NoSQL database\n",
    "- **[PostgreSQL with Python](https://www.psycopg.org/docs/)** - Relational database\n",
    "\n",
    "### üîß Advanced Libraries & Frameworks\n",
    "\n",
    "#### Alternative Scraping Libraries\n",
    "- **[requests-html](https://github.com/psf/requests-html)** - JavaScript support for requests\n",
    "- **[pyppeteer](https://github.com/pyppeteer/pyppeteer)** - Puppeteer port for Python\n",
    "- **[playwright-python](https://playwright.dev/python/)** - Modern browser automation\n",
    "- **[httpx](https://www.python-httpx.org/)** - Next-generation HTTP client\n",
    "\n",
    "#### Specialized Tools\n",
    "- **[Splash](https://splash.readthedocs.io/)** - JavaScript rendering service\n",
    "- **[AutoScraper](https://github.com/alirezamika/autoscraper)** - Intelligent scraping\n",
    "- **[newspaper3k](https://github.com/codelucas/newspaper)** - News article extraction\n",
    "- **[trafilatura](https://github.com/adbar/trafilatura)** - Text extraction from web pages\n",
    "\n",
    "### üéì Online Courses & Learning Platforms\n",
    "\n",
    "#### Free Courses\n",
    "- **[freeCodeCamp](https://www.freecodecamp.org/)** - Web scraping with Python\n",
    "- **[Coursera - Web Scraping](https://www.coursera.org/search?query=web%20scraping)** - University courses\n",
    "- **[edX - Data Science](https://www.edx.org/search?q=web%20scraping)** - MIT and Harvard courses\n",
    "\n",
    "#### Paid Courses\n",
    "- **[Udemy - Web Scraping Courses](https://www.udemy.com/topic/web-scraping/)** - Various instructors\n",
    "- **[Pluralsight](https://www.pluralsight.com/search?q=web%20scraping)** - Professional development\n",
    "- **[DataCamp](https://www.datacamp.com/search?q=web%20scraping)** - Interactive learning\n",
    "\n",
    "### üìö Books & E-books\n",
    "\n",
    "#### Beginner Books\n",
    "- **\"Web Scraping with Python\"** by Ryan Mitchell - O'Reilly Media\n",
    "- **\"Python Web Scraping Cookbook\"** by Michael Heydt - Packt\n",
    "- **\"Learning Scrapy\"** by Dimitris Kouzis-Loukas - Packt\n",
    "\n",
    "#### Advanced Books\n",
    "- **\"Web Scraping with Python: Data Extraction\"** by Ryan Mitchell\n",
    "- **\"Mastering Python Web Scraping\"** by Various Authors\n",
    "- **\"Python for Data Analysis\"** by Wes McKinney - Pandas creator\n",
    "\n",
    "### üèÜ Practice Websites & Challenges\n",
    "\n",
    "#### Beginner Practice\n",
    "- **[Quotes to Scrape](http://quotes.toscrape.com/)** - Basic scraping practice\n",
    "- **[Books to Scrape](http://books.toscrape.com/)** - E-commerce scraping\n",
    "- **[Scrape This Site](https://scrapethissite.com/)** - Various challenges\n",
    "\n",
    "#### Advanced Practice\n",
    "- **[HackerRank](https://www.hackerrank.com/)** - Coding challenges\n",
    "- **[Kaggle](https://www.kaggle.com/)** - Data science competitions\n",
    "- **[GitHub Scraping Projects](https://github.com/topics/web-scraping)** - Open source projects\n",
    "\n",
    "### üí° Pro Tips for Learning\n",
    "\n",
    "1. **Start Small**: Begin with static HTML sites before tackling JavaScript\n",
    "2. **Practice Regularly**: Scrape different types of websites\n",
    "3. **Read robots.txt**: Always check before scraping\n",
    "4. **Join Communities**: Reddit r/webscraping, Stack Overflow\n",
    "5. **Stay Updated**: Web technologies change frequently\n",
    "6. **Build Projects**: Create real-world applications\n",
    "7. **Learn HTML/CSS**: Understanding structure helps with scraping\n",
    "8. **Respect Websites**: Follow ethical practices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üõ†Ô∏è ‘≥’∏÷Ä’Æ’∂’°’Ø’°’∂\n",
    "\n",
    "Let's put our knowledge into practice with hands-on exercises!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Exercise 1: News Headlines Scraper\n",
    "\n",
    "Create a scraper that extracts news headlines from a news website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: News Headlines Scraper\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "def scrape_news_headlines():\n",
    "    \"\"\"\n",
    "    Scrape news headlines from a sample news site\n",
    "    Note: In real projects, always check robots.txt and terms of service\n",
    "    \"\"\"\n",
    "    \n",
    "    # Using BBC RSS feed as an example (more reliable than scraping HTML)\n",
    "    url = \"http://feeds.bbci.co.uk/news/rss.xml\"\n",
    "    \n",
    "    try:\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "        }\n",
    "        \n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Parse XML (RSS feeds are XML)\n",
    "        soup = BeautifulSoup(response.content, 'xml')\n",
    "        \n",
    "        # Find all items (news articles)\n",
    "        items = soup.find_all('item')\n",
    "        \n",
    "        news_data = []\n",
    "        \n",
    "        for item in items[:10]:  # Get first 10 articles\n",
    "            title = item.find('title')\n",
    "            link = item.find('link')\n",
    "            description = item.find('description')\n",
    "            pub_date = item.find('pubDate')\n",
    "            \n",
    "            news_data.append({\n",
    "                'title': title.text if title else 'N/A',\n",
    "                'link': link.text if link else 'N/A',\n",
    "                'description': description.text if description else 'N/A',\n",
    "                'published': pub_date.text if pub_date else 'N/A',\n",
    "                'scraped_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            })\n",
    "        \n",
    "        return news_data\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping news: {e}\")\n",
    "        return []\n",
    "\n",
    "# Run the scraper\n",
    "print(\"üì∞ Scraping BBC News Headlines...\")\n",
    "news_headlines = scrape_news_headlines()\n",
    "\n",
    "if news_headlines:\n",
    "    print(f\"‚úÖ Successfully scraped {len(news_headlines)} headlines\")\n",
    "    \n",
    "    # Display first 3 headlines\n",
    "    for i, article in enumerate(news_headlines[:3], 1):\n",
    "        print(f\"\\n{i}. {article['title']}\")\n",
    "        print(f\"   Published: {article['published']}\")\n",
    "        print(f\"   Description: {article['description'][:100]}...\")\n",
    "    \n",
    "    # Save to CSV\n",
    "    df = pd.DataFrame(news_headlines)\n",
    "    df.to_csv('news_headlines.csv', index=False, encoding='utf-8')\n",
    "    print(f\"\\nüíæ Data saved to 'news_headlines.csv'\")\n",
    "    \n",
    "    # Show basic statistics\n",
    "    print(f\"\\nüìä Statistics:\")\n",
    "    print(f\"   Total articles: {len(news_headlines)}\")\n",
    "    print(f\"   Average title length: {df['title'].str.len().mean():.1f} characters\")\n",
    "else:\n",
    "    print(\"‚ùå No headlines found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Exercise 2: Table Data Scraper\n",
    "\n",
    "Extract tabular data from websites and convert it to pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Table Data Scraper\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Create sample HTML table for demonstration\n",
    "sample_table_html = \"\"\"\n",
    "<html>\n",
    "<body>\n",
    "    <h2>Cryptocurrency Prices</h2>\n",
    "    <table id=\"crypto-table\" class=\"data-table\">\n",
    "        <thead>\n",
    "            <tr>\n",
    "                <th>Rank</th>\n",
    "                <th>Name</th>\n",
    "                <th>Symbol</th>\n",
    "                <th>Price (USD)</th>\n",
    "                <th>24h Change</th>\n",
    "                <th>Market Cap</th>\n",
    "            </tr>\n",
    "        </thead>\n",
    "        <tbody>\n",
    "            <tr>\n",
    "                <td>1</td>\n",
    "                <td>Bitcoin</td>\n",
    "                <td>BTC</td>\n",
    "                <td>$43,250.00</td>\n",
    "                <td class=\"positive\">+2.34%</td>\n",
    "                <td>$847.5B</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td>2</td>\n",
    "                <td>Ethereum</td>\n",
    "                <td>ETH</td>\n",
    "                <td>$2,580.50</td>\n",
    "                <td class=\"negative\">-1.25%</td>\n",
    "                <td>$310.2B</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td>3</td>\n",
    "                <td>Cardano</td>\n",
    "                <td>ADA</td>\n",
    "                <td>$0.45</td>\n",
    "                <td class=\"positive\">+5.67%</td>\n",
    "                <td>$15.2B</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td>4</td>\n",
    "                <td>Solana</td>\n",
    "                <td>SOL</td>\n",
    "                <td>$98.75</td>\n",
    "                <td class=\"positive\">+3.21%</td>\n",
    "                <td>$42.8B</td>\n",
    "            </tr>\n",
    "        </tbody>\n",
    "    </table>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "def scrape_table_data(html_content):\n",
    "    \"\"\"Extract table data and convert to pandas DataFrame\"\"\"\n",
    "    \n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    # Find the table\n",
    "    table = soup.find('table', {'id': 'crypto-table'})\n",
    "    \n",
    "    if not table:\n",
    "        print(\"‚ùå Table not found\")\n",
    "        return None\n",
    "    \n",
    "    # Extract headers\n",
    "    headers = []\n",
    "    header_row = table.find('thead').find('tr')\n",
    "    for th in header_row.find_all('th'):\n",
    "        headers.append(th.text.strip())\n",
    "    \n",
    "    print(f\"üìã Table headers: {headers}\")\n",
    "    \n",
    "    # Extract data rows\n",
    "    data_rows = []\n",
    "    tbody = table.find('tbody')\n",
    "    \n",
    "    for row in tbody.find_all('tr'):\n",
    "        row_data = []\n",
    "        for td in row.find_all('td'):\n",
    "            # Clean the text (remove extra whitespace, currency symbols, etc.)\n",
    "            cell_text = td.text.strip()\n",
    "            row_data.append(cell_text)\n",
    "        data_rows.append(row_data)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(data_rows, columns=headers)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def clean_financial_data(df):\n",
    "    \"\"\"Clean and process financial data\"\"\"\n",
    "    \n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # Clean price column (remove $ and convert to float)\n",
    "    if 'Price (USD)' in df_clean.columns:\n",
    "        df_clean['Price_Numeric'] = df_clean['Price (USD)'].str.replace('$', '').str.replace(',', '').astype(float)\n",
    "    \n",
    "    # Clean percentage change (remove % and convert to float)\n",
    "    if '24h Change' in df_clean.columns:\n",
    "        df_clean['Change_Numeric'] = df_clean['24h Change'].str.replace('%', '').str.replace('+', '').astype(float)\n",
    "    \n",
    "    # Clean market cap (convert to billions)\n",
    "    if 'Market Cap' in df_clean.columns:\n",
    "        def parse_market_cap(cap_str):\n",
    "            cap_str = cap_str.replace('$', '').replace(',', '')\n",
    "            if 'B' in cap_str:\n",
    "                return float(cap_str.replace('B', '')) * 1e9\n",
    "            elif 'M' in cap_str:\n",
    "                return float(cap_str.replace('M', '')) * 1e6\n",
    "            return float(cap_str)\n",
    "        \n",
    "        df_clean['Market_Cap_Numeric'] = df_clean['Market Cap'].apply(parse_market_cap)\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "# Scrape the table\n",
    "print(\"üìä Scraping table data...\")\n",
    "crypto_df = scrape_table_data(sample_table_html)\n",
    "\n",
    "if crypto_df is not None:\n",
    "    print(\"\\n‚úÖ Raw table data:\")\n",
    "    print(crypto_df.to_string(index=False))\n",
    "    \n",
    "    # Clean the data\n",
    "    crypto_clean = clean_financial_data(crypto_df)\n",
    "    \n",
    "    print(\"\\nüßπ Cleaned data with numeric columns:\")\n",
    "    print(crypto_clean[['Name', 'Symbol', 'Price_Numeric', 'Change_Numeric']].to_string(index=False))\n",
    "    \n",
    "    # Basic analysis\n",
    "    print(\"\\nüìà Quick Analysis:\")\n",
    "    print(f\"   Average price: ${crypto_clean['Price_Numeric'].mean():,.2f}\")\n",
    "    print(f\"   Highest price: {crypto_clean.loc[crypto_clean['Price_Numeric'].idxmax(), 'Name']} (${crypto_clean['Price_Numeric'].max():,.2f})\")\n",
    "    print(f\"   Best performer: {crypto_clean.loc[crypto_clean['Change_Numeric'].idxmax(), 'Name']} ({crypto_clean['Change_Numeric'].max()}%)\")\n",
    "    print(f\"   Worst performer: {crypto_clean.loc[crypto_clean['Change_Numeric'].idxmin(), 'Name']} ({crypto_clean['Change_Numeric'].min()}%)\")\n",
    "    \n",
    "    # Save to CSV\n",
    "    crypto_clean.to_csv('crypto_data.csv', index=False)\n",
    "    print(\"\\nüíæ Data saved to 'crypto_data.csv'\")\n",
    "else:\n",
    "    print(\"‚ùå Failed to scrape table data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üè°’è’∂’°’µ’´’∂\n",
    "\n",
    "Practice your web scraping skills with the examples provided in the tutorial!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üé≤ 00\n",
    "- ‚ñ∂Ô∏è[Video]()\n",
    "- üîó[Random link]()\n",
    "- üá¶üá≤üé∂[]()\n",
    "- üåêüé∂[]()\n",
    "- ü§å[‘ø’°÷Ä’£’´’∂]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TfbUegKqXVyH"
   },
   "source": [
    "\n",
    "<a href=\"http://s01.flagcounter.com/more/1oO\"><img src=\"https://s01.flagcounter.com/count2/1oO/bg_FFFFFF/txt_000000/border_CCCCCC/columns_2/maxflags_10/viewers_0/labels_0/pageviews_1/flags_0/percent_0/\" alt=\"Flag Counter\"></a>\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOx7X+DxNeKu1zUVVCmsSHJ",
   "provenance": [
    {
     "file_id": "1_9UtYmPVVGmnWIKdBzPYkbtTlTbd0clo",
     "timestamp": 1735604987843
    },
    {
     "file_id": "15x56uwwONMo_ilzUJgt6UcX1d552xH6X",
     "timestamp": 1708441161475
    },
    {
     "file_id": "1LbG88IWtk30WlIoINzG4_vXHoJAvoDaP",
     "timestamp": 1683614319950
    }
   ]
  },
  "kernelspec": {
   "display_name": "lectures",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
